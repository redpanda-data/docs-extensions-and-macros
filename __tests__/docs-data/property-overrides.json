{
  "properties": {
    "abort_index_segment_size": {
      "description": "Capacity (in number of txns) of an abort index segment.\n\nEach partition tracks the aborted transaction offset ranges to help service client requests. If the number of transactions increases beyond this threshold, they are flushed to disk to ease memory pressure. Then they're loaded on demand. This configuration controls the maximum number of aborted transactions before they are flushed to disk.",
      "config_scope": "cluster"
    },
    "admin": {
      "example": [
        ".Example",
        "[,yaml]",
        "----",
        "redpanda:",
        "  admin:",
        "    - name: <admin-api-name>",
        "      address: <external-broker-hostname>",
        "      port: <admin-api-port>",
        "----",
        "",
        "Replace the following placeholders with your values:",
        "",
        "* `<admin-api-name>`: Name for the Admin API listener (TLS configuration is handled separately in the <<admin_api_tls,`admin_api_tls`>> broker property)",
        "* `<external-broker-hostname>`: The externally accessible hostname or IP address that clients use to connect to this broker",
        "* `<admin-api-port>`: The port number for the Admin API endpoint"
      ],
      "description": "Network address for the glossterm:Admin API[] server.",
      "config_scope": "broker",
      "category": "redpanda"
    },
    "admin_api_doc_dir": {
      "config_scope": "broker",
      "category": "redpanda"
    },
    "admin_api_tls": {
      "example": [
        ".Example",
        "[,yaml]",
        "----",
        "redpanda:",
        "  admin_api_tls:",
        "    - name: <admin-api-tls-name>",
        "      enabled: true",
        "      cert_file: <path-to-cert-file>",
        "      key_file: <path-to-key-file>",
        "      truststore_file: <path-to-truststore-file>",
        "      require_client_auth: true",
        "----",
        "",
        "Replace the following placeholders with your values:",
        "",
        "* `<admin-api-tls-name>`: Name that matches your Admin API listener (defined in the <<admin, `admin`>> broker property)",
        "* `<path-to-cert-file>`: Full path to the TLS certificate file",
        "* `<path-to-key-file>`: Full path to the TLS private key file",
        "* `<path-to-truststore-file>`: Full path to the Certificate Authority file"
      ],
      "config_scope": "broker",
      "category": "redpanda"
    },
    "advertised_kafka_api": {
      "description": "Address of the Kafka API published to the clients. If not set, the <<kafka_api, `kafka_api`>> broker property is used. When behind a load balancer or in containerized environments, this should be the externally-accessible address that clients use to connect.",
      "example": [
        ".Example",
        "[,yaml]",
        "----",
        "redpanda:",
        "  advertised_kafka_api:",
        "    - name: <kafka-api-name>",
        "      address: <external-broker-hostname>",
        "      port: <kafka-port>",
        "----",
        "",
        "Replace the following placeholders with your values:",
        "",
        "* `<kafka-api-name>`: Name that matches your Kafka API listener (defined in the <<kafka_api, `kafka_api`>> broker property)",
        "* `<external-broker-hostname>`: The externally accessible hostname or IP address that clients use to connect to this broker",
        "* `<kafka-port>`: The port number for the Kafka API endpoint"
      ],
      "config_scope": "broker",
      "category": "redpanda"
    },
    "advertised_pandaproxy_api": {
      "config_scope": "broker",
      "category": "pandaproxy",
      "description": "Network address for the HTTP Proxy API server to publish to clients."
    },
    "advertised_rpc_api": {
      "description": "Address of RPC endpoint published to other cluster members. If not set, the <<rpc_server, `rpc_server`>> broker property is used. This should be the address other brokers can use to communicate with this broker.",
      "example": [
        ".Example",
        "[,yaml]",
        "----",
        "redpanda:",
        "  advertised_rpc_api:",
        "    address: <external-broker-hostname>",
        "    port: <rpc-port>",
        "----",
        "",
        "Replace the following placeholders with your values:",
        "",
        "* `<external-broker-hostname>`: The externally accessible hostname or IP address that other brokers use to communicate with this broker",
        "* `<rpc-port>`: The port number for the RPC endpoint (default is 33145)"
      ],
      "config_scope": "broker",
      "category": "redpanda"
    },
    "aggregate_metrics": {
      "description": "Enable aggregation of metrics returned by the xref:reference:internal-metrics-reference.adoc[`/metrics`] endpoint. Aggregation can simplify monitoring by providing summarized data instead of raw, per-instance metrics. Metric aggregation is performed by summing the values of samples by labels and is done when it makes sense by the shard and/or partition labels.",
      "related_topics": [
        "xref:reference:internal-metrics-reference.adoc[`/metrics`]"
      ],
      "config_scope": "cluster"
    },
    "api_doc_dir": {
      "description": "Path to the API specifications directory. This directory contains API documentation for both the HTTP Proxy API and Schema Registry API.",
      "config_scope": "broker",
      "category": "pandaproxy"
    },
    "audit_enabled": {
      "related_topics": [],
      "config_scope": "cluster"
    },
    "auto_create_topics_enabled": {
      "description": "Allow automatic topic creation.\n\nIf you produce to a topic that doesn't exist, the topic will be created with defaults if this property is enabled.",
      "config_scope": "cluster"
    },
    "broker_tls": {
      "config_scope": "broker",
      "category": "pandaproxy-client",
      "description": "TLS configuration for the Kafka API servers to which the HTTP Proxy client should connect."
    },
    "brokers": {
      "config_scope": "broker",
      "category": "pandaproxy-client",
      "description": "Network addresses of the Kafka API servers to which the HTTP Proxy client should connect."
    },
    "cleanup.policy": {
      "description": "The cleanup policy to apply for log segments of a topic.\nWhen `cleanup.policy` is set, it overrides the cluster property xref:cluster-properties.adoc#log_cleanup_policy[`log_cleanup_policy`] for the topic.",
      "related_topics": [
        "xref:cluster-properties.adoc#log_cleanup_policy[`log_cleanup_policy`]"
      ],
      "config_scope": "topic"
    },
    "client_cache_max_size": {
      "config_scope": "broker",
      "category": "pandaproxy",
      "description": "The maximum number of Kafka client connections that Redpanda can cache in the LRU (least recently used) cache. The LRU cache helps optimize resource utilization by keeping the most recently used clients in memory, facilitating quicker reconnections for frequent clients while limiting memory usage."
    },
    "client_identifier": {
      "config_scope": "broker",
      "category": "pandaproxy-client",
      "description": "Custom identifier to include in the Kafka request header for the HTTP Proxy client. This identifier can help debug or monitor client activities."
    },
    "client_keep_alive": {
      "description": "Time, in milliseconds, that an idle client connection may remain open to the HTTP Proxy API.",
      "config_scope": "broker",
      "category": "pandaproxy"
    },
    "cloud_storage_access_key": {
      "description": "AWS or GCP access key. This access key is part of the credentials that Redpanda requires to authenticate with object storage services for Tiered Storage. This access key is used with the <<cloud_storage_secret_key,`cloud_storage_secret_key`>> to form the complete credentials required for authentication.\nTo authenticate using IAM roles, see <<cloud_storage_credentials_source,`cloud_storage_credentials_source`>>."
    },
    "cloud_storage_api_endpoint": {
      "description": "Optional API endpoint. The only instance in which you must set this value is when using a custom domain with your object storage service.\n\n- AWS: If not set, this is automatically generated using <<cloud_storage_region,region>> and <<cloud_storage_bucket,bucket>>. Otherwise, this uses the value assigned.\n- GCP: If not set, this is automatically generated using `storage.googleapis.com` and <<cloud_storage_bucket,bucket>>.\n- Azure: If not set, this is automatically generated using `blob.core.windows.net` and <<cloud_storage_azure_storage_account,`cloud_storage_azure_storage_account`>>. If you have enabled hierarchical namespaces for your storage account and use a custom endpoint, use <<cloud_storage_azure_adls_endpoint,`cloud_storage_azure_adls_endpoint`>>."
    },
    "cloud_storage_azure_adls_endpoint": {
      "description": "Azure Data Lake Storage v2 endpoint override. Use when hierarchical namespaces are enabled on your storage account and you have set up a custom endpoint.\n\nIf not set, this is automatically generated using `dfs.core.windows.net` and <<cloud_storage_azure_storage_account,`cloud_storage_azure_storage_account`>>."
    },
    "cloud_storage_azure_adls_port": {
      "description": "Azure Data Lake Storage v2 port override. See also: <<cloud_storage_azure_adls_endpoint,`cloud_storage_azure_adls_endpoint`>>. Use when hierarchical namespaces are enabled on your storage account and you have set up a custom endpoint."
    },
    "cloud_storage_azure_container": {
      "description": "The name of the Azure container to use with Tiered Storage. If `null`, the property is disabled.\n\nNOTE: The container must belong to <<cloud_storage_azure_storage_account,`cloud_storage_azure_storage_account`>>."
    },
    "cloud_storage_azure_hierarchical_namespace_enabled": {
      "description": "Force Redpanda to use or not use an Azure Data Lake Storage (ADLS) Gen2 hierarchical namespace-compliant client in <<cloud_storage_azure_storage_account,`cloud_storage_azure_storage_account`>>. \n\nWhen this property is not set, <<cloud_storage_azure_shared_key,`cloud_storage_azure_shared_key`>> must be set, and each broker checks at startup if a hierarchical namespace is enabled. \n\nWhen set to `true`, this property disables the check and assumes a hierarchical namespace is enabled. \n\nWhen set to `false`, this property disables the check and assumes a hierarchical namespace is not enabled. \n\nThis setting should be used only in emergencies where Redpanda fails to detect the correct a hierarchical namespace status."
    },
    "cloud_storage_azure_managed_identity_id": {
      "description": "The managed identity ID to use for access to the Azure storage account. To use Azure managed identities, you must set <<cloud_storage_credentials_source,`cloud_storage_credentials_source`>> to `azure_vm_instance_metadata`. See xref:manage:security/iam-roles.adoc[IAM Roles] for more information on managed identities.",
      "related_topics": [
        "xref:manage:security/iam-roles.adoc[IAM Roles]"
      ]
    },
    "cloud_storage_azure_shared_key": {
      "description": "The account access key to be used for Azure Shared Key authentication with the Azure storage account configured by <<cloud_storage_azure_storage_account,`cloud_storage_azure_storage_account`>>.  If `null`, the property is disabled."
    },
    "cloud_storage_backend": {
      "description": "Optional object storage backend variant used to select API capabilities. If not supplied, this will be inferred from other configuration properties."
    },
    "cloud_storage_bucket": {
      "description": "AWS or GCP bucket that should be used to store data.\n\nWARNING: Modifying this property after writing data to a bucket could cause data loss."
    },
    "cloud_storage_cache_directory": {
      "description": "Directory for archival cache. Set when the xref:reference:properties/cluster-properties.adoc#cloud_storage_enabled[`cloud_storage_enabled`] cluster property is enabled. If not specified, Redpanda uses a default path within the data directory.",
      "example": [
        ".Example",
        "[,yaml]",
        "----",
        "redpanda:",
        "  cloud_storage_cache_directory: <cache-directory-path>",
        "----",
        "\n",
        "Replace `<cache-directory-path>` with the full path to your desired cache directory."
      ],
      "related_topics": [
        "xref:reference:properties/cluster-properties.adoc#cloud_storage_enabled[`cloud_storage_enabled`]"
      ],
      "config_scope": "broker",
      "category": "redpanda"
    },
    "cloud_storage_cache_max_objects": {
      "description": "Maximum number of objects that may be held in the Tiered Storage cache.  This applies simultaneously with <<cloud_storage_cache_size,`cloud_storage_cache_size`>>, and whichever limit is hit first will trigger trimming of the cache."
    },
    "cloud_storage_cache_size": {
      "description": "Maximum size of the object storage cache, in bytes.\n\nThis property works together with <<cloud_storage_cache_size_percent,`cloud_storage_cache_size_percent`>> to define cache behavior:\n\n- When both properties are set, Redpanda uses the smaller calculated value of the two, in bytes.\n\n- If one of these properties is set to `0`, Redpanda uses the non-zero value.\n\n- These properties cannot both be `0`.\n\n- `cloud_storage_cache_size` cannot be `0` while `cloud_storage_cache_size_percent` is `null`."
    },
    "cloud_storage_cache_size_percent": {
      "related_topics": [
        "xref:reference:cluster-properties.adoc#disk_reservation_percent[`disk_reservation_percent`]"
      ]
    },
    "cloud_storage_cache_trim_threshold_percent_objects": {
      "description": "Cache trimming is triggered when the number of objects in the cache reaches this percentage relative to its maximum object count. If unset, the default behavior is to start trimming when the cache is full.",
      "version": "24.1.10"
    },
    "cloud_storage_cache_trim_threshold_percent_size": {
      "description": "Cache trimming is triggered when the cache size reaches this percentage relative to its maximum capacity. If unset, the default behavior is to start trimming when the cache is full.",
      "version": "24.1.10"
    },
    "cloud_storage_cache_trim_walk_concurrency": {
      "description": "The maximum number of concurrent tasks launched for traversing the directory structure during cache trimming. A higher number allows cache trimming to run faster but can cause latency spikes due to increased pressure on I/O subsystem and syscall threads."
    },
    "cloud_storage_chunk_prefetch": {
      "description": "Number of chunks to prefetch ahead of every downloaded chunk. Prefetching additional chunks can enhance read performance by reducing wait times for sequential data access. A value of `0` disables prefetching, relying solely on on-demand downloads. Adjusting this property allows for tuning the balance between improved read performance and increased network and storage I/O."
    },
    "cloud_storage_client_lease_timeout_ms": {
      "description": "Maximum time to hold a cloud storage client lease (ms), after which any outstanding connection is immediately closed.",
      "config_scope": "cluster"
    },
    "cloud_storage_credentials_host": {
      "description": "The hostname to connect to for retrieving role based credentials. Derived from <<cloud_storage_credentials_source,`cloud_storage_credentials_source`>> if not set. Only required when using IAM role based access. To authenticate using access keys, see <<cloud_storage_access_key,`cloud_storage_access_key`>>."
    },
    "cloud_storage_credentials_source": {
      "description": "The source of credentials used to authenticate to object storage services.\nRequired for AWS or GCP authentication with IAM roles.\n\nTo authenticate using access keys, see <<cloud_storage_access_key,`cloud_storage_access_key`>>."
    },
    "cloud_storage_crl_file": {
      "description": "Path to certificate revocation list for <<cloud_storage_trust_file, `cloud_storage_trust_file`>>."
    },
    "cloud_storage_disable_archival_stm_rw_fence": {
      "description": "Disables the concurrency control mechanism in Tiered Storage. This safety feature keeps data organized and correct when multiple processes access it simultaneously. Disabling it can cause data consistency problems, so use this setting only for testing, never in production systems."
    },
    "cloud_storage_disable_archiver_manager": {
      "description": "Use legacy upload mode and do not start archiver_manager.",
      "config_scope": "cluster"
    },
    "cloud_storage_disable_chunk_reads": {
      "description": "Disable chunk reads and switch back to legacy mode where full segments are downloaded. When set to `true`, this option disables the more efficient chunk-based reads, causing Redpanda to download entire segments. This legacy behavior might be useful in specific scenarios where chunk-based fetching is not optimal."
    },
    "cloud_storage_disable_read_replica_loop_for_tests": {
      "description": "Begins the read replica sync loop in topic partitions with Tiered Storage enabled. The property exists to simplify testing and shouldn't be set in production."
    },
    "cloud_storage_disable_remote_labels_for_tests": {
      "description": "If `true`, Redpanda disables remote labels and falls back on the hash-based object naming scheme for new topics."
    },
    "cloud_storage_disable_upload_consistency_checks": {
      "description": "Disable all upload consistency checks to allow Redpanda to upload logs with gaps and replicate metadata with consistency violations. Do not change the default value unless requested by Redpanda Support."
    },
    "cloud_storage_disable_upload_loop_for_tests": {
      "description": "Begins the upload loop in topic partitions with Tiered Storage enabled. The property exists to simplify testing and shouldn't be set in production."
    },
    "cloud_storage_enable_compacted_topic_reupload": {
      "description": "Enable re-uploading data for compacted topics.\nWhen set to `true`, Redpanda can re-upload data for compacted topics to object storage, ensuring that the most current state of compacted topics is available in the cloud. Disabling this property (`false`) may reduce storage and network overhead but at the risk of not having the latest compacted data state in object storage."
    },
    "cloud_storage_enable_remote_allow_gaps": {
      "description": "Controls the eviction of locally stored log segments when Tiered Storage uploads are paused. Set to `false` to only evict data that has already been uploaded to object storage. If the retained data fills the local volume, Redpanda throttles producers. Set to `true` to allow the eviction of locally stored log segments, which may create gaps in offsets."
    },
    "cloud_storage_enable_remote_read": {
      "description": "Default remote read config value for new topics.\nWhen set to `true`, new topics are by default configured to allow reading data directly from object storage, facilitating access to older data that might have been offloaded as part of Tiered Storage. With the default set to `false`, remote reads must be explicitly enabled at the topic level."
    },
    "cloud_storage_enable_remote_write": {
      "description": "Default remote write value for new topics.\nWhen set to `true`, new topics are by default configured to upload data to object storage. With the default set to `false`, remote write must be explicitly enabled at the topic level."
    },
    "cloud_storage_enable_scrubbing": {
      "description": "Enable routine checks (scrubbing) of object storage partitions. The scrubber validates the integrity of data and metadata uploaded to object storage."
    },
    "cloud_storage_enable_segment_merging": {
      "related_topics": [
        "xref:manage:tiered-storage.adoc#object-storage-housekeeping[Object storage housekeeping]"
      ]
    },
    "cloud_storage_enable_segment_uploads": {
      "description": "Controls the upload of log segments to Tiered Storage. If set to `false`, this property temporarily pauses all log segment uploads from the Redpanda cluster. When the uploads are paused, the <<cloud_storage_enable_remote_allow_gaps, `cloud_storage_enable_remote_allow_gaps`>> cluster configuration and `redpanda.remote.allowgaps` topic properties control local retention behavior."
    },
    "cloud_storage_enabled": {
      "related_topics": []
    },
    "cloud_storage_full_scrub_interval_ms": {
      "description": "Interval, in milliseconds, between a final scrub and the next scrub."
    },
    "cloud_storage_garbage_collect_timeout_ms": {
      "description": "Timeout for running the cloud storage garbage collection, in milliseconds."
    },
    "cloud_storage_graceful_transfer_timeout_ms": {
      "description": "Time limit on waiting for uploads to complete before a leadership transfer.  If this is `null`, leadership transfers proceed without waiting."
    },
    "cloud_storage_housekeeping_interval_ms": {
      "description": "Interval, in milliseconds, between object storage housekeeping tasks."
    },
    "cloud_storage_hydrated_chunks_per_segment_ratio": {
      "description": "The maximum number of chunks per segment that can be hydrated at a time. Above this number, unused chunks are trimmed.\n\nA segment is divided into chunks. Chunk hydration means downloading the chunk (which is a small part of a full segment) from cloud storage and placing it in the local disk cache. Redpanda periodically removes old, unused chunks from your local disk. This process is called chunk eviction. This property  controls how many chunks can be present for a given segment in local disk at a time, before eviction is triggered, removing the oldest ones from disk. Note that this property is not used for the default eviction strategy which simply removes all unused chunks."
    },
    "cloud_storage_hydration_timeout_ms": {
      "description": "Time to wait for a hydration request to be fulfilled. If hydration is not completed within this time, the consumer is notified with a timeout error.\n\nNegative doesn't make sense, but it may not be checked-for/enforced. Large is subjective, but a huge timeout also doesn't make sense. This particular config doesn't have a min/max bounds control, but it probably should to avoid mistakes."
    },
    "cloud_storage_idle_threshold_rps": {
      "description": "The object storage request rate threshold for idle state detection. If the average request rate for the configured period is lower than this threshold, the object storage is considered idle."
    },
    "cloud_storage_idle_timeout_ms": {
      "description": "The timeout, in milliseconds, used to detect the idle state of the object storage API. If the average object storage request rate is below this threshold for a configured amount of time, the object storage is considered idle and the housekeeping jobs are started."
    },
    "cloud_storage_initial_backoff_ms": {
      "description": "Initial backoff time for exponential backoff algorithm (ms)."
    },
    "cloud_storage_inventory_hash_path_directory": {
      "description": "Directory to store inventory report hashes for use by cloud storage scrubber. If not specified, Redpanda uses a default path within the data directory.",
      "example": [
        ".Example",
        "[,yaml]",
        "----",
        "redpanda:",
        "  cloud_storage_inventory_hash_store: <inventory-hash-directory-path>",
        "----",
        "",
        "Replace `<inventory-hash-directory-path>` with the full path to your desired inventory hash storage directory."
      ],
      "config_scope": "broker"
    },
    "cloud_storage_inventory_hash_store": {
      "description": "Directory to store inventory report hashes for use by cloud storage scrubber. If not specified, Redpanda uses a default path within the data directory.",
      "example": [
        ".Example",
        "[,yaml]",
        "----",
        "redpanda:",
        "  cloud_storage_inventory_hash_store: <inventory-hash-directory-path>",
        "----"
      ],
      "config_scope": "broker",
      "category": "redpanda"
    },
    "cloud_storage_inventory_max_hash_size_during_parse": {
      "description": "Maximum bytes of hashes held in memory before writing data to disk during inventory report parsing. This affects the number of files written to disk during inventory report parsing. When this limit is reached, new files are written to disk."
    },
    "cloud_storage_manifest_cache_size": {
      "description": "Amount of memory that can be used to handle Tiered Storage metadata."
    },
    "cloud_storage_manifest_max_upload_interval_sec": {
      "description": "Minimum interval, in seconds, between partition manifest uploads. Actual time between uploads may be greater than this interval. If this is `null`, metadata is updated after each segment upload."
    },
    "cloud_storage_manifest_upload_timeout_ms": {
      "description": "Manifest upload timeout, in milliseconds."
    },
    "cloud_storage_materialized_manifest_ttl_ms": {
      "description": "The interval, in milliseconds, determines how long the materialized manifest can stay in the cache under contention. This setting is used for performance tuning. When the spillover manifest is materialized and stored in the cache, and the cache needs to evict it, it uses this value as a timeout. The cursor that uses the spillover manifest uses this value as a TTL interval, after which it stops referencing the manifest making it available for eviction. This only affects spillover manifests under contention.",
      "config_scope": "cluster"
    },
    "cloud_storage_max_concurrent_hydrations_per_shard": {
      "description": "Maximum concurrent segment hydrations of remote data per CPU core.  If unset, value of `cloud_storage_max_connections / 2` is used, which means that half of available object storage bandwidth could be used to download data from object storage. If the cloud storage cache is empty every new segment reader will require a download. This will lead to 1:1 mapping between number of partitions scanned by the fetch request and number of parallel downloads. If this value is too large the downloads can affect other workloads. In case of any problem caused by the tiered-storage reads this value can be lowered. This will only affect segment hydrations (downloads) but won't affect cached segments. If fetch request is reading from the tiered-storage cache its concurrency will only be limited by available memory."
    },
    "cloud_storage_max_connection_idle_time_ms": {
      "description": "Defines the maximum duration an HTTPS connection to object storage can stay idle, in milliseconds, before being terminated.\nThis setting reduces resource utilization by closing inactive connections. Adjust this property to balance keeping connections ready for subsequent requests and freeing resources associated with idle connections."
    },
    "cloud_storage_max_segment_readers_per_shard": {
      "description": "Maximum concurrent I/O cursors of materialized remote segments per CPU core.  If unset, the value of `topic_partitions_per_shard` is used, where one segment reader per partition is used if the shard is at its maximum partition capacity.  These readers are cached across Kafka consume requests and store a readahead buffer."
    },
    "cloud_storage_max_segments_pending_deletion_per_partition": {
      "description": "The per-partition limit for the number of segments pending deletion from the cloud. Segments can be deleted due to retention or compaction. If this limit is breached and deletion fails, then segments are orphaned in the cloud and must be removed manually."
    },
    "cloud_storage_max_throughput_per_shard": {
      "description": "Maximum bandwidth allocated to Tiered Storage operations per shard, in bytes per second.\nThis setting limits the Tiered Storage subsystem's throughput per shard, facilitating precise control over bandwidth usage in testing scenarios. In production environments, use `cloud_storage_throughput_limit_percent` for more dynamic throughput management based on actual storage capabilities."
    },
    "cloud_storage_metadata_sync_timeout_ms": {
      "description": "Timeout for xref:manage:tiered-storage.adoc[] metadata synchronization."
    },
    "cloud_storage_min_chunks_per_segment_threshold": {
      "description": "The minimum number of chunks per segment for trimming to be enabled. If the number of chunks in a segment is below this threshold, the segment is small enough that all chunks in it can be hydrated at any given time."
    },
    "cloud_storage_readreplica_manifest_sync_timeout_ms": {
      "description": "Timeout to check if new data is available for partitions in object storage for read replicas."
    },
    "cloud_storage_recovery_temporary_retention_bytes_default": {
      "description": "Retention in bytes for topics created during automated recovery."
    },
    "cloud_storage_recovery_topic_validation_depth": {
      "description": "Number of metadata segments to validate, from newest to oldest, when <<cloud_storage_recovery_topic_validation_mode,`cloud_storage_recovery_topic_validation_mode`>> is set to `check_manifest_and_segment_metadata`."
    },
    "cloud_storage_recovery_topic_validation_mode": {
      "description": "Validation performed before recovering a topic from object storage. In case of failure, the reason for the failure appears as `ERROR` lines in the Redpanda application log. For each topic, this reports errors for all partitions, but for each partition, only the first error is reported.\n\nThis property accepts the following parameters:\n\n- `no_check`: Skips the checks for topic recovery.\n- `check_manifest_existence`:  Runs an existence check on each `partition_manifest`. Fails if there are connection issues to the object storage.\n- `check_manifest_and_segment_metadata`: Downloads the manifest and runs a consistency check, comparing the metadata with the cloud storage objects. The process fails if metadata references any missing cloud storage objects.\n\nExample: Redpanda validates the topic `kafka/panda-topic-recovery-NOT-OK` and stops due to a fatal error on partition 0:\n\n```bash\nERROR 2024-04-24 21:29:08,166 [shard 1:main] cluster - [fiber11|0|299996ms recovery validation of {kafka/panda-topic-recovery-NOT-OK/0}/24] - manifest metadata check: missing segment, validation not ok\nERROR 2024-04-24 21:29:08,166 [shard 1:main] cluster - topics_frontend.cc:519 - Stopping recovery of {kafka/panda-topic-recovery-NOT-OK} due to validation error\n```\n\nEach failing partition error message has the following format:\n\n```bash\nERROR .... [... recovery validation of {<namespace/topic/partition>}...] - <failure-reason>, validation not ok\n```\n\nAt the end of the process, Redpanda outputs a final ERROR message: \n\n```bash\nERROR ... ... - Stopping recovery of {<namespace/topic>} due to validation error\n```"
    },
    "cloud_storage_roles_operation_timeout_ms": {
      "description": "Timeout for IAM role related operations (ms)."
    },
    "cloud_storage_scrubbing_interval_jitter_ms": {
      "description": "Jitter applied to the object storage scrubbing interval."
    },
    "cloud_storage_segment_max_upload_interval_sec": {
      "description": "Time that a segment can be kept locally without uploading it to the object storage, in seconds."
    },
    "cloud_storage_segment_size_min": {
      "description": "Smallest acceptable segment size in the object storage. Default: `cloud_storage_segment_size_target`/2."
    },
    "cloud_storage_segment_size_target": {
      "description": "Desired segment size in the object storage. The default is set in the topic-level `segment.bytes` property."
    },
    "cloud_storage_segment_upload_timeout_ms": {
      "description": "Log segment upload timeout, in milliseconds."
    },
    "cloud_storage_spillover_manifest_max_segments": {
      "description": "Maximum number of segments in the spillover manifest that can be offloaded to the object storage. This setting serves as a threshold for triggering data offload based on the number of segments, rather than the total size of the manifest. It is designed for use in testing environments to control the offload behavior more granularly. In production settings, manage offloads based on the manifest size through `cloud_storage_spillover_manifest_size` for more predictable outcomes."
    },
    "cloud_storage_spillover_manifest_size": {
      "description": "The size of the manifest which can be offloaded to the cloud. If the size of the local manifest stored in Redpanda exceeds `cloud_storage_spillover_manifest_size` by two times the spillover mechanism will split the manifest into two parts and one will be uploaded to object storage."
    },
    "cloud_storage_throughput_limit_percent": {
      "description": "Maximum throughput used by Tiered Storage per broker expressed as a percentage of the disk bandwidth. If the server has several disks, Redpanda uses the one that stores the Tiered Storage cache. Even if Tiered Storage is allowed to use the full bandwidth of the disk (100%), it won't necessarily use it in full. The actual usage depends on your workload and the state of the Tiered Storage cache. This setting is a safeguard that prevents Tiered Storage from using too many system resources: it is not a performance tuning knob."
    },
    "cloud_storage_topic_purge_grace_period_ms": {
      "description": "Grace period during which the purger refuses to purge the topic."
    },
    "cloud_storage_upload_ctrl_d_coeff": {
      "description": "Derivative coefficient for upload PID controller."
    },
    "cloud_storage_upload_ctrl_max_shares": {
      "description": "Maximum number of I/O and CPU shares that archival upload can use."
    },
    "cloud_storage_upload_ctrl_min_shares": {
      "description": "Minimum number of I/O and CPU shares that archival upload can use."
    },
    "cloud_storage_upload_ctrl_p_coeff": {
      "description": "Proportional coefficient for upload PID controller."
    },
    "cloud_storage_upload_ctrl_update_interval_ms": {
      "description": "The interval (in milliseconds) for updating the controller that manages the priority of Tiered Storage uploads. This property determines how frequently the system recalculates and adjusts the work scheduling for uploads to object storage.\n\nThis is an internal-only configuration and should be enabled only after consulting with Redpanda support."
    },
    "cloud_storage_upload_loop_initial_backoff_ms": {
      "description": "Initial backoff interval when there is nothing to upload for a partition, in milliseconds."
    },
    "cloud_storage_upload_loop_max_backoff_ms": {
      "description": "Maximum backoff interval when there is nothing to upload for a partition, in milliseconds."
    },
    "cloud_storage_url_style": {
      "description": "Configure the addressing style that controls how Redpanda formats bucket URLs for S3-compatible object storage.\n\nLeave this property unset (`null`) to use automatic configuration:\n\n* For AWS S3: Redpanda attempts `virtual_host` addressing first, then falls back to `path` style if needed\n* For MinIO: Redpanda automatically uses `path` style regardless of `MINIO_DOMAIN` configuration\n\nSet this property explicitly to override automatic configuration, ensure consistent behavior across deployments, or when using S3-compatible storage that requires a specific URL format."
    },
    "cluster_id": {
      "description": "NOTE: This property is read-only in Redpanda Cloud.\n\nCluster identifier.",
      "config_scope": "cluster"
    },
    "compaction.strategy": {
      "description": "Specifies the strategy used to determine which records to remove during log compaction. The compaction strategy controls how Redpanda identifies and removes duplicate records while preserving the latest value for each key.",
      "related_topics": [
        "xref:./cluster-properties.adoc#compaction_strategy[`compaction_strategy`]"
      ],
      "config_scope": "topic"
    },
    "compaction_ctrl_update_interval_ms": {
      "description": "The interval (in milliseconds) for updating the controller responsible for compaction tasks. The controller uses this interval to decide how to prioritize background compaction work, which is essential for maintaining efficient storage use.\n\nThis is an internal-only configuration and should be enabled only after consulting with Redpanda support.",
      "config_scope": "cluster"
    },
    "compression.type": {
      "description": "Redpanda ignores this property and always uses producer compression semantics. If producers send compressed data, Redpanda stores and serves it as-is. If producers send uncompressed data, Redpanda stores it uncompressed.\n\nThis property exists for Apache Kafka compatibility. Configure compression in your producers instead of using this topic property.\n\nCompression reduces message size and improves throughput, but increases CPU utilization. Enable producer batching to increase compression efficiency.\n\nWhen set, this property overrides the cluster property xref:./cluster-properties.adoc#log_compression_type[`log_compression_type`] for the topic.",
      "related_topics": [
        "xref:./cluster-properties.adoc#log_compression_type[`log_compression_type`]",
        "xref:./cluster-properties.adoc#log_compression_type[`log_compression_type`]",
        "xref:develop:produce-data/configure-producers.adoc#message-batching[Message batching]",
        "xref:develop:produce-data/configure-producers.adoc#commonly-used-producer-configuration-options[Common producer configuration options]"
      ],
      "config_scope": "topic"
    },
    "confluent.key.schema.validation": {
      "description": "Enable validation of the schema ID for keys on a record. This is a compatibility alias for `redpanda.key.schema.id.validation`. When enabled, Redpanda validates that the schema ID encoded in the record's key is registered in the Schema Registry according to the configured subject name strategy.",
      "config_scope": "topic"
    },
    "confluent.key.subject.name.strategy": {
      "description": "The subject name strategy for keys when `confluent.key.schema.validation` is enabled. This is a compatibility alias for `redpanda.key.subject.name.strategy` that determines how the topic and schema are mapped to a subject name in the Schema Registry.",
      "config_scope": "topic"
    },
    "confluent.value.schema.validation": {
      "description": "Enable validation of the schema ID for values on a record. This is a compatibility alias for `redpanda.value.schema.id.validation`. When enabled, Redpanda validates that the schema ID encoded in the record's value is registered in the Schema Registry according to the configured subject name strategy.",
      "config_scope": "topic"
    },
    "confluent.value.subject.name.strategy": {
      "description": "The subject name strategy for values when `confluent.value.schema.validation` is enabled. This is a compatibility alias for `redpanda.value.subject.name.strategy`. This determines how the topic and schema are mapped to a subject name in the Schema Registry.",
      "config_scope": "topic"
    },
    "consumer_group_lag_collection_interval_sec": {
      "description": "How often to run the collection loop when <<enable_consumer_group_metrics,`enable_consumer_group_metrics`>> contains `consumer_lag`.\n\nReducing the value of `consumer_group_lag_collection_interval_sec` increases the metric collection frequency, which may raise resource utilization. In most environments, this impact is minimal, but it's best practice to monitor broker resource usage in high-scale settings.",
      "config_scope": "cluster"
    },
    "consumer_heartbeat_interval_ms": {
      "description": "Interval (in milliseconds) for consumer heartbeats.",
      "config_scope": "broker",
      "category": "pandaproxy-client"
    },
    "consumer_instance_timeout_ms": {
      "description": "How long to wait for an idle consumer before removing it. A consumer is considered idle when it's not making requests or heartbeats.",
      "config_scope": "broker",
      "category": "pandaproxy"
    },
    "consumer_offsets_topic_batch_cache_enabled": {
      "description": "This property lets you enable the batch cache for the consumer offsets topic. By default, the cache for consumer offsets topic is disabled. Changing this property is not recommended in production systems, as it may affect performance. The change is applied only after the restart.",
      "config_scope": "cluster"
    },
    "consumer_rebalance_timeout_ms": {
      "description": "Timeout (in milliseconds) for consumer rebalance.",
      "config_scope": "broker",
      "category": "pandaproxy-client"
    },
    "consumer_request_max_bytes": {
      "description": "Maximum bytes to fetch per request.",
      "config_scope": "broker",
      "category": "pandaproxy-client"
    },
    "consumer_request_min_bytes": {
      "description": "Minimum bytes to fetch per request.",
      "config_scope": "broker",
      "category": "pandaproxy-client"
    },
    "consumer_request_timeout_ms": {
      "description": "Interval (in milliseconds) for consumer request timeout.",
      "config_scope": "broker",
      "category": "pandaproxy-client"
    },
    "consumer_session_timeout_ms": {
      "description": "Timeout (in milliseconds) for consumer session.",
      "config_scope": "broker",
      "category": "pandaproxy-client"
    },
    "controller_log_accummulation_rps_capacity_topic_operations": {
      "description": "Maximum capacity of rate limit accumulation in controller topic operations limit.",
      "config_scope": "cluster"
    },
    "core_balancing_continuous": {
      "related_topics": [],
      "config_scope": "cluster"
    },
    "core_balancing_debounce_timeout": {
      "description": "Interval, in milliseconds, between trigger and invocation of core balancing.",
      "config_scope": "cluster"
    },
    "crash_loop_limit": {
      "config_scope": "broker",
      "category": "redpanda",
      "description": "A limit on the number of consecutive times a broker can crash within one hour before its crash-tracking logic is reset. This limit prevents a broker from getting stuck in an infinite cycle of crashes.\n\nIf `null`, the property is disabled and no limit is applied.\n\nThe crash-tracking logic is reset (to zero consecutive crashes) by any of the following conditions:\n\n* The broker shuts down cleanly.\n* One hour passes since the last crash.\n* The `redpanda.yaml` broker configuration file is updated.\n* The `startup_log` file in the broker's <<data_directory, `data_directory`>> broker property is manually deleted."
    },
    "crash_loop_sleep_sec": {
      "description": "The amount of time the broker sleeps before terminating when the limit on consecutive broker crashes (<<crash_loop_limit, `crash_loop_limit`>>) is reached. This property provides a debugging window for you to access the broker before it terminates, and is particularly useful in Kubernetes environments.\n\nIf `null`, the property is disabled, and the broker terminates immediately after reaching the crash loop limit.\n\nFor information about how to reset the crash loop limit, see the <<crash_loop_limit, `crash_loop_limit`>> broker property.",
      "version": "v24.3.4",
      "config_scope": "broker",
      "category": "redpanda"
    },
    "data_directory": {
      "config_scope": "broker",
      "category": "redpanda"
    },
    "data_transforms_binary_max_size": {
      "description": "ifdef::env-cloud[]\nNOTE: This property is read-only in Redpanda Cloud.\nendif::[]\n\nThe maximum size for a deployable WebAssembly binary that the broker can store.",
      "config_scope": "cluster"
    },
    "data_transforms_per_core_memory_reservation": {
      "description": "ifdef::env-cloud[]\nNOTE: This property is read-only in Redpanda Cloud.\nendif::[]\n\nThe amount of memory to reserve per core for data transform (Wasm) virtual machines. Memory is reserved on boot. The maximum number of functions that can be deployed to a cluster is equal to `data_transforms_per_core_memory_reservation` / `data_transforms_per_function_memory_limit`.",
      "config_scope": "cluster"
    },
    "data_transforms_per_function_memory_limit": {
      "description": "ifdef::env-cloud[]\nNOTE: This property is read-only in Redpanda Cloud.\nendif::[]\n\nThe amount of memory to give an instance of a data transform (Wasm) virtual machine. The maximum number of functions that can be deployed to a cluster is equal to `data_transforms_per_core_memory_reservation` / `data_transforms_per_function_memory_limit`.",
      "config_scope": "cluster"
    },
    "data_transforms_read_buffer_memory_percentage": {
      "description": "include::reference:partial$internal-use-property.adoc[]\n\nThe percentage of available memory in the transform subsystem to use for read buffers.",
      "config_scope": "cluster"
    },
    "data_transforms_write_buffer_memory_percentage": {
      "description": "include::reference:partial$internal-use-property.adoc[]\n\nThe percentage of available memory in the transform subsystem to use for write buffers.",
      "config_scope": "cluster"
    },
    "datalake_coordinator_snapshot_max_delay_secs": {
      "description": "Maximum amount of time the coordinator waits to snapshot after a command appears in the log.",
      "config_scope": "cluster"
    },
    "datalake_disk_space_monitor_enable": {
      "description": "Option to explicitly disable enforcement of datalake disk space usage.",
      "config_scope": "cluster"
    },
    "datalake_scheduler_max_concurrent_translations": {
      "description": "The maximum number of translations that the datalake scheduler will allow to run at a given time. If a translation is requested, but the number of running translations exceeds this value, the request will be put to sleep temporarily, polling until capacity becomes available.",
      "config_scope": "cluster"
    },
    "datalake_scheduler_time_slice_ms": {
      "description": "Time, in milliseconds, for a datalake translation as scheduled by the datalake scheduler. After a translation is scheduled, it will run until either the time specified has elapsed or all pending records on its source partition have been translated.",
      "config_scope": "cluster"
    },
    "datalake_scratch_space_soft_limit_size_percent": {
      "description": "Size of the scratch space datalake soft limit expressed as a percentage of the `datalake_scratch_space_size_bytes` configuration value.",
      "config_scope": "cluster"
    },
    "default_leaders_preference": {
      "description": "Default settings for preferred location of topic partition leaders. It can be either \"none\" (no preference), or \"racks:<rack1>,<rack2>,...\" (prefer brokers with rack ID from the list).\n\nThe list can contain one or more rack IDs. If you specify multiple IDs, Redpanda tries to distribute the partition leader locations equally across brokers in these racks.\n\nIf config_ref:enable_rack_awareness,true,properties/cluster-properties[] is set to `false`, leader pinning is disabled across the cluster.",
      "related_topics": [],
      "config_scope": "cluster"
    },
    "delete.retention.ms": {
      "description": "The retention time for tombstone records in a compacted topic. Redpanda removes tombstone records after the retention limit is exceeded.\n\nIf you have enabled Tiered Storage and set <<redpandaremoteread,`redpanda.remote.read`>> or <<redpandaremotewrite,`redpanda.remote.write`>> for the topic, you cannot enable tombstone removal.\n\nIf both `delete.retention.ms` and the cluster property config_ref:tombstone_retention_ms,true,properties/cluster-properties[] are set, `delete.retention.ms` overrides the cluster level tombstone retention for an individual topic.",
      "related_topics": [
        "xref:./cluster-properties.adoc#tombstone_retention_ms[`tombstone_retention_ms`]",
        "xref:manage:cluster-maintenance/compaction-settings.adoc#tombstone-record-removal[Tombstone record removal]"
      ],
      "config_scope": "topic"
    },
    "developer_mode": {
      "description": "CAUTION: Enabling `developer_mode` isn't recommended for production use.\n\nEnable developer mode, which skips most of the checks performed at startup.",
      "config_scope": "broker",
      "category": "redpanda"
    },
    "disable_cluster_recovery_loop_for_tests": {
      "description": "include::reference:partial$internal-use-property.adoc[]\n\nDisables the cluster recovery loop. This property is used to simplify testing and should not be set in production.",
      "config_scope": "cluster"
    },
    "disk_reservation_percent": {
      "description": "The percentage of total disk capacity that Redpanda will avoid using. This applies both when cloud cache and log data share a disk, as well \nas when cloud cache uses a dedicated disk. \n\nIt is recommended to not run disks near capacity to avoid blocking I/O due to low disk space, as well as avoiding performance issues associated with SSD garbage collection.",
      "config_scope": "cluster"
    },
    "election_timeout_ms": {
      "description": "Raft election timeout expressed in milliseconds.",
      "config_scope": "cluster"
    },
    "emergency_disable_data_transforms": {
      "description": "Override the cluster property xref:reference:properties/cluster-properties.adoc#data_transforms_enabled[`data_transforms_enabled`] and disable Wasm-powered data transforms. This is an emergency shutoff button.",
      "related_topics": [
        "xref:reference:properties/cluster-properties.adoc#data_transforms_enabled[`data_transforms_enabled`]"
      ],
      "config_scope": "broker",
      "category": "redpanda"
    },
    "empty_seed_starts_cluster": {
      "description": "Controls how a new cluster is formed. All brokers in a cluster must have the same value.\n\n<<seed_servers,See how the `empty_seed_starts_cluster` broker property works with the `seed_servers` broker property>> to form a cluster.\n\nTIP: For backward compatibility, `true` is the default. Redpanda recommends using `false` in production environments to prevent accidental cluster formation.",
      "config_scope": "broker",
      "category": "redpanda"
    },
    "enable_cluster_metadata_upload_loop": {
      "description": "Enables cluster metadata uploads. Required for xref:manage:whole-cluster-restore.adoc[whole cluster restore].",
      "related_topics": [
        "xref:manage:whole-cluster-restore.adoc[whole cluster restore]"
      ],
      "config_scope": "cluster"
    },
    "enable_consumer_group_metrics": {
      "description": "List of enabled consumer group metrics.\n\n*Accepted values:*\n\n- `group`: Enables the xref:reference:public-metrics-reference.adoc#redpanda_kafka_consumer_group_consumers[`redpanda_kafka_consumer_group_consumers`] and xref:reference:public-metrics-reference.adoc#redpanda_kafka_consumer_group_topics[`redpanda_kafka_consumer_group_topics`] metrics.\n- `partition`: Enables the xref:reference:public-metrics-reference.adoc#redpanda_kafka_consumer_group_committed_offset[`redpanda_kafka_consumer_group_committed_offset`] metric.\n- `consumer_lag`: Enables the xref:reference:public-metrics-reference.adoc#redpanda_kafka_consumer_group_lag_max[`redpanda_kafka_consumer_group_lag_max`] and xref:reference:public-metrics-reference.adoc#redpanda_kafka_consumer_group_lag_sum[`redpanda_kafka_consumer_group_lag_sum`] metrics\n+\nEnabling `consumer_lag` may add a small amount of additional processing overhead to the brokers, especially in environments with a high number of consumer groups or partitions.\n+\nifndef::env-cloud[]\nUse the xref:reference:properties/cluster-properties.adoc#consumer_group_lag_collection_interval_sec[`consumer_group_lag_collection_interval_sec`] property to control the frequency of consumer lag metric collection.\nendif::[]",
      "related_topics": [
        "xref:reference:public-metrics-reference.adoc#redpanda_kafka_consumer_group_consumers[`redpanda_kafka_consumer_group_consumers`]",
        "xref:reference:public-metrics-reference.adoc#redpanda_kafka_consumer_group_topics[`redpanda_kafka_consumer_group_topics`]",
        "xref:reference:public-metrics-reference.adoc#redpanda_kafka_consumer_group_committed_offset[`redpanda_kafka_consumer_group_committed_offset`]",
        "xref:reference:public-metrics-reference.adoc#redpanda_kafka_consumer_group_lag_max[`redpanda_kafka_consumer_group_lag_max`]",
        "xref:reference:public-metrics-reference.adoc#redpanda_kafka_consumer_group_lag_sum[`redpanda_kafka_consumer_group_lag_sum`]",
        "xref:reference:properties/cluster-properties.adoc#consumer_group_lag_collection_interval_sec[`consumer_group_lag_collection_interval_sec`]",
        "xref:manage:monitoring.adoc#consumers[Monitor consumer group lag]"
      ],
      "config_scope": "cluster"
    },
    "enable_developmental_unrecoverable_data_corrupting_features": {
      "description": "Development features should never be enabled in a production cluster, or any cluster where stability, data loss, or the ability to upgrade are a concern. To enable experimental features, set the value of this configuration option to the current unix epoch expressed in seconds. The value must be within one hour of the current time on the broker.Once experimental features are enabled they cannot be disabled",
      "config_scope": "cluster"
    },
    "enable_host_metrics": {
      "description": "Enable exporting of some host metrics like `/proc/diskstats`, `/proc/snmp` and `/proc/net/netstat`.\n\nHost metrics are prefixed with xref:reference:internal-metrics-reference.adoc#vectorized_host_diskstats_discards[`vectorized_host`] and are available on the `/metrics` endpoint.",
      "related_topics": [
        "xref:reference:internal-metrics-reference.adoc#vectorized_host_diskstats_discards[`vectorized_host`]"
      ],
      "config_scope": "cluster"
    },
    "enable_metrics_reporter": {
      "description": "Enable the cluster metrics reporter. If `true`, the metrics reporter collects and exports to Redpanda Data a set of customer usage metrics at the interval set by <<metrics_reporter_report_interval,`metrics_reporter_report_interval`>>.\n\n[NOTE]\n====\nThe cluster metrics of the metrics reporter are different from xref:manage:monitoring.adoc[monitoring metrics].\n\n* The metrics reporter exports customer usage metrics for consumption by Redpanda Data.\n* Monitoring metrics are exported for consumption by Redpanda users.\n====",
      "related_topics": [
        "xref:manage:monitoring.adoc[monitoring metrics]"
      ],
      "config_scope": "cluster"
    },
    "enable_sasl": {
      "description": "Enable SASL authentication for Kafka connections. Authorization is required to modify this property. See also <<kafka_enable_authorization,`kafka_enable_authorization`>>.",
      "config_scope": "cluster"
    },
    "enable_schema_id_validation": {
      "related_topics": [
        "xref:manage:schema-reg/schema-id-validation.adoc[Server-Side Schema ID Validation]"
      ],
      "description": "Mode to enable server-side schema ID validation.\n\n*Accepted values:*\n\n* `none`: Schema validation is disabled (no schema ID checks are done). Associated topic properties cannot be modified.\n* `redpanda`: Schema validation is enabled. Only Redpanda topic properties are accepted.\n* `compat`: Schema validation is enabled. Both Redpanda and compatible topic properties are accepted.",
      "config_scope": "cluster"
    },
    "fetch_read_strategy": {
      "description": "The strategy used to fulfill fetch requests.\n\n* `polling`: Repeatedly polls every partition in the request for new data. The polling interval is set by <<fetch_reads_debounce_timeout,`fetch_reads_debounce_timeout`>> (deprecated).\n\n* `non_polling`: The backend is signaled when a partition has new data, so Redpanda doesn't need to repeatedly read from every partition in the fetch. Redpanda Data recommends using this value for most workloads, because it can improve fetch latency and CPU utilization.\n\n* `non_polling_with_debounce`: This option behaves like `non_polling`, but it includes a debounce mechanism with a fixed delay specified by <<fetch_reads_debounce_timeout,`fetch_reads_debounce_timeout`>> at the start of each fetch. By introducing this delay, Redpanda can accumulate more data before processing, leading to fewer fetch operations and returning larger amounts of data. Enabling this option reduces reactor utilization, but it may also increase end-to-end latency.",
      "config_scope": "cluster"
    },
    "fips_mode": {
      "config_scope": "broker",
      "category": "redpanda",
      "description": "Controls whether Redpanda starts in FIPS mode.  This property allows for three values: \n\n* Disabled - Redpanda does not start in FIPS mode.\n\n* Permissive - Redpanda performs the same check as enabled, but a warning is logged, and Redpanda continues to run. Redpanda loads the OpenSSL FIPS provider into the OpenSSL library. After this completes, Redpanda is operating in FIPS mode, which means that the TLS cipher suites available to users are limited to the TLSv1.2 and TLSv1.3 NIST-approved cryptographic methods.\n\n* Enabled - Redpanda verifies that the operating system is enabled for FIPS by checking `/proc/sys/crypto/fips_enabled`. If the file does not exist or does not return `1`, Redpanda immediately exits."
    },
    "flush.bytes": {
      "description": "The maximum bytes not fsynced per partition. If this configured threshold is reached, the log is automatically fsynced, even though it wasn't explicitly requested.",
      "related_topics": [
        "xref:./cluster-properties.adoc#flush_bytes[`flush_bytes`]"
      ],
      "config_scope": "topic"
    },
    "flush.ms": {
      "description": "The maximum delay (in ms) between two subsequent fsyncs. After this delay, the log is automatically fsynced.",
      "related_topics": [
        "xref:./cluster-properties.adoc#flush_ms[`flush_ms`]"
      ],
      "config_scope": "topic"
    },
    "http_authentication": {
      "description": "A list of supported HTTP authentication mechanisms.\n\n*Accepted values:*\n\n* `BASIC`: Basic authentication\n* `OIDC`: OpenID Connect",
      "related_topics": [],
      "config_scope": "cluster"
    },
    "iceberg_backlog_controller_i_coeff": {
      "description": "Controls how much past backlog (unprocessed work) affects the priority of processing new data in the Iceberg system. The system accumulates backlog errors over time, and this coefficient determines how much that accumulated backlog influences the urgency of data translation.",
      "config_scope": "cluster"
    },
    "iceberg_backlog_controller_p_coeff": {
      "description": "Proportional coefficient for the Iceberg backlog controller. Number of shares assigned to the datalake scheduling group will be proportional to the backlog size error. A negative value means larger and faster changes in the number of shares in the datalake scheduling group.",
      "config_scope": "cluster"
    },
    "iceberg_catalog_type": {
      "description": "Iceberg catalog type that Redpanda will use to commit table metadata updates. Supported types: `rest`, `object_storage`.\nNOTE: You must set <<iceberg_rest_catalog_endpoint,`iceberg_rest_catalog_endpoint`>> at the same time that you set `iceberg_catalog_type` to `rest`.",
      "config_scope": "cluster"
    },
    "iceberg_default_partition_spec": {
      "description": "ifndef::env-cloud[]\nDefault value for the xref:reference:properties/topic-properties.adoc#redpanda-iceberg-partition-spec[`redpanda.iceberg.partition.spec`] topic property that determines the partition spec for the Iceberg table corresponding to the topic.\nendif::[]\n\nifdef::env-cloud[]\nDefault value for the `redpanda.iceberg.partition.spec` topic property that determines the partition spec for the Iceberg table corresponding to the topic.\nendif::[]",
      "related_topics": [
        "self-managed-only: xref:reference:properties/topic-properties.adoc#redpanda-iceberg-partition-spec[`redpanda.iceberg.partition.spec`]"
      ],
      "config_scope": "cluster"
    },
    "iceberg_delete": {
      "description": "Default value for the `redpanda.iceberg.delete` topic property that determines if the corresponding Iceberg table is deleted upon deleting the topic.",
      "config_scope": "cluster"
    },
    "iceberg_disable_automatic_snapshot_expiry": {
      "description": "Whether to disable automatic Iceberg snapshot expiry. This property may be useful if the Iceberg catalog expects to perform snapshot expiry on its own.",
      "config_scope": "cluster"
    },
    "iceberg_disable_snapshot_tagging": {
      "description": "Whether to disable tagging of Iceberg snapshots. These tags are used to ensure that the snapshots that Redpanda writes are retained during snapshot removal, which in turn, helps Redpanda ensure exactly-once delivery of records. Disabling tags is therefore not recommended, but it may be useful if the Iceberg catalog does not support tags.",
      "config_scope": "cluster"
    },
    "iceberg_enabled": {
      "description": "ifndef::env-cloud[]\nEnables the translation of topic data into Iceberg tables. Setting `iceberg_enabled` to `true` activates the feature at the cluster level, but each topic must also set the xref:reference:properties/topic-properties.adoc#redpanda-iceberg-enabled[`redpanda.iceberg.enabled`] topic-level property to `true` to use it. If `iceberg_enabled` is set to `false`, then the feature is disabled for all topics in the cluster, overriding any topic-level settings.\nendif::[]\nifdef::env-cloud[]\nEnables the translation of topic data into Iceberg tables. Setting `iceberg_enabled` to `true` activates the feature at the cluster level, but each topic must also set the `redpanda.iceberg.enabled` topic-level property to `true` to use it. If `iceberg_enabled` is set to `false`, then the feature is disabled for all topics in the cluster, overriding any topic-level settings.\nendif::[]",
      "related_topics": [
        "self-managed-only: xref:reference:properties/topic-properties.adoc#redpanda-iceberg-enabled[`redpanda.iceberg.enabled`]"
      ],
      "config_scope": "cluster"
    },
    "iceberg_invalid_record_action": {
      "description": "ifndef::env-cloud[]\nDefault value for the xref:reference:properties/topic-properties.adoc#redpanda-iceberg-invalid-record-action[`redpanda.iceberg.invalid.record.action`] topic property.\nendif::[]\nifdef::env-cloud[]\nDefault value for the `redpanda.iceberg.invalid.record.action` topic property.\nendif::[]",
      "related_topics": [
        "self-managed-only: xref:reference:properties/topic-properties.adoc#redpanda-iceberg-invalid-record-action[`redpanda.iceberg.invalid.record.action`]",
        "self-managed-only: xref:manage:iceberg/about-iceberg-topics.adoc#troubleshoot-errors[Troubleshoot errors]"
      ],
      "config_scope": "cluster"
    },
    "iceberg_latest_schema_cache_ttl_ms": {
      "description": "The TTL for caching the latest schema during translation when using the xref:manage:iceberg/specify-iceberg-schema.adoc#value_schema_latest[`value_schema_latest`] iceberg mode. This setting controls how long the latest schema remains cached during translation, which affects schema refresh behavior and performance.",
      "related_topics": [
        "xref:manage:iceberg/specify-iceberg-schema.adoc#value_schema_latest[`value_schema_latest`]"
      ],
      "config_scope": "cluster"
    },
    "iceberg_rest_catalog_authentication_mode": {
      "description": "The authentication mode for client requests made to the Iceberg catalog. Choose from: `none`, `bearer`, `oauth2`, and `aws_sigv4`. In `bearer` mode, the token specified in `iceberg_rest_catalog_token` is used unconditonally, and no attempts are made to refresh the token. In `oauth2` mode, the credentials specified in `iceberg_rest_catalog_client_id` and `iceberg_rest_catalog_client_secret` are used to obtain a bearer token from the URI defined by `iceberg_rest_catalog_oauth2_server_uri`. In `aws_sigv4` mode, the same AWS credentials used for cloud storage (see `cloud_storage_region`, `cloud_storage_access_key`, `cloud_storage_secret_key`, and `cloud_storage_credentials_source`) are used to sign requests to AWS Glue catalog with SigV4.",
      "config_scope": "cluster"
    },
    "iceberg_rest_catalog_aws_access_key": {
      "description": "AWS access key for Iceberg REST catalog SigV4 authentication. If not set, falls back to xref:reference:properties/object-storage-properties.adoc#cloud_storage_access_key[`cloud_storage_access_key`] when using aws_sigv4 authentication mode.",
      "related_topics": [
        "xref:reference:properties/object-storage-properties.adoc#cloud_storage_access_key[`cloud_storage_access_key`]"
      ],
      "config_scope": "cluster"
    },
    "iceberg_rest_catalog_aws_credentials_source": {
      "description": "ifndef::env-cloud[]\nSource of AWS credentials for Iceberg REST catalog SigV4 authentication. If not set, falls back to xref:reference:properties/object-storage-properties.adoc#cloud_storage_credentials_source[`cloud_storage_credentials_source`] when using aws_sigv4 authentication mode.\nendif::[]\n\nifdef::env-cloud[]\nSource of AWS credentials for Iceberg REST catalog SigV4 authentication. If providing explicit credentials using `iceberg_rest_catalog_aws_access_key` and `iceberg_rest_catalog_aws_secret_key` for Glue catalog authentication, you must set this property to `config_file`.\nendif::[]\n\n*Accepted values*: `aws_instance_metadata`, `azure_aks_oidc_federation`, `azure_vm_instance_metadata`, `config_file`, `gcp_instance_metadata`, `sts`.",
      "related_topics": [
        "xref:reference:properties/object-storage-properties.adoc#cloud_storage_credentials_source[`cloud_storage_credentials_source`]"
      ],
      "config_scope": "cluster"
    },
    "iceberg_rest_catalog_aws_region": {
      "description": "AWS region for Iceberg REST catalog SigV4 authentication. If not set, falls back to xref:reference:properties/object-storage-properties.adoc#cloud_storage_region[`cloud_storage_region`] when using aws_sigv4 authentication mode.",
      "related_topics": [
        "xref:reference:properties/object-storage-properties.adoc#cloud_storage_region[`cloud_storage_region`]"
      ],
      "config_scope": "cluster"
    },
    "iceberg_rest_catalog_aws_secret_key": {
      "description": "AWS secret key for Iceberg REST catalog SigV4 authentication. If not set, falls back to xref:reference:properties/object-storage-properties.adoc#cloud_storage_secret_key[`cloud_storage_secret_key`] when using aws_sigv4 authentication mode.",
      "related_topics": [
        "xref:reference:properties/object-storage-properties.adoc#cloud_storage_secret_key[`cloud_storage_secret_key`]"
      ],
      "config_scope": "cluster"
    },
    "iceberg_rest_catalog_client_id": {
      "description": "Iceberg REST catalog user ID. This ID is used to query the catalog API for the OAuth token. Required if catalog type is set to `rest` and `iceberg_rest_catalog_authentication_mode` is set to `oauth2`.",
      "config_scope": "cluster"
    },
    "iceberg_rest_catalog_client_secret": {
      "description": "Secret used with the client ID to query the OAuth token endpoint for Iceberg REST catalog authentication. Required if catalog type is set to `rest` and `iceberg_rest_catalog_authentication_mode` is set to `oauth2`.",
      "config_scope": "cluster"
    },
    "iceberg_rest_catalog_crl": {
      "description": "The contents of a certificate revocation list for `iceberg_rest_catalog_trust`. Takes precedence over `iceberg_rest_catalog_crl_file`.",
      "config_scope": "cluster"
    },
    "iceberg_rest_catalog_endpoint": {
      "description": "URL of Iceberg REST catalog endpoint.\nNOTE: If you set <<iceberg_catalog_type,`iceberg_catalog_type`>> to `rest`, you must also set this property at the same time.",
      "config_scope": "cluster"
    },
    "iceberg_rest_catalog_oauth2_scope": {
      "description": "The OAuth scope used to retrieve access tokens for Iceberg catalog authentication. Only meaningful when `iceberg_rest_catalog_authentication_mode` is set to `oauth2`",
      "config_scope": "cluster"
    },
    "iceberg_rest_catalog_oauth2_server_uri": {
      "description": "The OAuth URI used to retrieve access tokens for Iceberg catalog authentication. If left undefined, the deprecated Iceberg catalog endpoint `/v1/oauth/tokens` is used instead.",
      "config_scope": "cluster"
    },
    "iceberg_rest_catalog_request_timeout_ms": {
      "description": "Maximum length of time that Redpanda waits for a response from the REST catalog before aborting the request",
      "config_scope": "cluster"
    },
    "iceberg_rest_catalog_token": {
      "description": "Token used to access the REST Iceberg catalog. If the token is present, Redpanda ignores credentials stored in the properties <<iceberg_rest_catalog_client_id,`iceberg_rest_catalog_client_id`>>  and <<iceberg_rest_catalog_client_secret,`iceberg_rest_catalog_client_secret`>>.\n\nRequired if <<iceberg_rest_catalog_authentication_mode, `iceberg_rest_catalog_authentication_mode`>> is set to `bearer`.",
      "config_scope": "cluster"
    },
    "iceberg_rest_catalog_trust": {
      "description": "The contents of a certificate chain to trust for the REST Iceberg catalog.\nifndef::env-cloud[]\nTakes precedence over <<iceberg_rest_catalog_trust_file, `iceberg_rest_catalog_trust_file`>>.\nendif::[]",
      "config_scope": "cluster"
    },
    "iceberg_rest_catalog_trust_file": {
      "description": "Path to a file containing a certificate chain to trust for the REST Iceberg catalog.",
      "config_scope": "cluster"
    },
    "iceberg_rest_catalog_warehouse": {
      "description": "Warehouse to use for the Iceberg REST catalog. Redpanda queries the catalog to retrieve warehouse-specific configurations and automatically configures settings like the appropriate prefix. The prefix is appended to the catalog path (for example, `/v1/\\{prefix}/namespaces`).",
      "config_scope": "cluster"
    },
    "iceberg_target_backlog_size": {
      "description": "Average size per partition of the datalake translation backlog that the backlog controller tries to maintain. When the backlog size is larger than the set point, the backlog controller will increase the translation scheduling group priority.",
      "config_scope": "cluster"
    },
    "iceberg_target_lag_ms": {
      "related_topics": [
        "self-managed-only: xref:reference:properties/topic-properties.adoc#redpanda-iceberg-target-lag-ms[`redpanda.iceberg.target.lag.ms`]"
      ],
      "config_scope": "cluster"
    },
    "iceberg_throttle_backlog_size_ratio": {
      "description": "Ration of the total backlog size to the disk space at which the throttle to iceberg producers is applied.",
      "config_scope": "cluster"
    },
    "iceberg_topic_name_dot_replacement": {
      "description": "Optional replacement string for dots in topic names when deriving Iceberg table names, useful when downstream systems do not permit dots in table names. The replacement string cannot contain dots. Be careful to avoid table name collisions caused by the replacement.If an Iceberg topic with dots in the name exists in the cluster, the value of this property should not be changed.",
      "config_scope": "cluster"
    },
    "initial.retention.local.target.bytes": {
      "description": "A size-based initial retention limit for Tiered Storage that determines how much data in local storage is transferred to a partition replica when a cluster is resized. If `null` (default), all locally retained data is transferred.",
      "related_topics": [
        "xref:./cluster-properties.adoc#initial_retention_local_target_bytes[`initial_retention_local_target_bytes`]",
        "xref:manage:tiered-storage.adoc#fast-commission-and-decommission[Fast commission and decommission through Tiered Storage]"
      ],
      "config_scope": "topic"
    },
    "initial.retention.local.target.ms": {
      "description": "A time-based initial retention limit for Tiered Storage that determines how much data in local storage is transferred to a partition replica when a cluster is resized. If `null` (default), all locally retained data is transferred.",
      "related_topics": [
        "xref:./cluster-properties.adoc#initial_retention_local_target_ms[`initial_retention_local_target_ms`]",
        "xref:manage:tiered-storage.adoc#fast-commission-and-decommission[Fast commission and decommission through Tiered Storage]"
      ],
      "config_scope": "topic"
    },
    "initial_retention_local_target_bytes_default": {
      "description": "Initial local retention size target for partitions of topics with xref:manage:tiered-storage.adoc[Tiered Storage] enabled. If no initial local target retention is configured, then  all locally-retained data will be delivered to learner when joining the partition replica set.",
      "related_topics": [
        "xref:manage:tiered-storage.adoc[Tiered Storage]"
      ],
      "config_scope": "cluster"
    },
    "initial_retention_local_target_ms_default": {
      "description": "Initial local retention time target for partitions of topics with xref:manage:tiered-storage.adoc[Tiered Storage] enabled. If no initial local target retention is configured, then all locally-retained data will be delivered to learner when joining the partition replica is set.",
      "related_topics": [
        "xref:manage:tiered-storage.adoc[Tiered Storage]"
      ],
      "config_scope": "cluster"
    },
    "internal_topic_replication_factor": {
      "description": "Target replication factor for internal topics",
      "config_scope": "cluster"
    },
    "kafka_api": {
      "description": "IP address and port of the Kafka API endpoint that handles requests. Supports multiple listeners with different configurations.",
      "related_topics": [
        "xref:reference:properties/cluster-properties.adoc#sasl_mechanisms[`sasl_mechanisms`]"
      ],
      "example": [
        ".Basic example",
        "[,yaml]",
        "----",
        "redpanda:",
        "  kafka_api:",
        "    - address: <bind-address>",
        "      port: <kafka-port>",
        "      authentication_method: sasl",
        "----",
        "",
        ".Multiple listeners example (for different networks or authentication methods)",
        "[,yaml]",
        "----",
        "redpanda:",
        "  kafka_api:",
        "    - name: <internal-listener-name>",
        "      address: <internal-bind-address>",
        "      port: <internal-kafka-port>",
        "      authentication_method: none",
        "    - name: <external-listener-name>",
        "      address: <external-bind-address>",
        "      port: <external-kafka-port>",
        "      authentication_method: sasl",
        "    - name: <mtls-listener-name>",
        "      address: <mtls-bind-address>",
        "      port: <mtls-kafka-port>",
        "      authentication_method: mtls_identity",
        "----",
        "",
        "Replace the following placeholders with your values:",
        "",
        "* `<bind-address>`: The IP address to bind the listener to (typically `0.0.0.0` for all interfaces)",
        "* `<kafka-port>`: The port number for the Kafka API endpoint",
        "* `<internal-listener-name>`: Name for internal network connections (for example, `internal`)",
        "* `<external-listener-name>`: Name for external network connections (for example, `external`)",
        "* `<mtls-listener-name>`: Name for mTLS connections (for example, `mtls`)",
        "* `<internal-bind-address>`: The IP address for internal connections",
        "* `<internal-kafka-port>`: The port number for internal Kafka API connections",
        "* `<external-bind-address>`: The IP address for external connections",
        "* `<external-kafka-port>`: The port number for external Kafka API connections",
        "* `<mtls-bind-address>`: The IP address for mTLS connections",
        "* `<mtls-kafka-port>`: The port number for mTLS Kafka API connections"
      ],
      "config_scope": "broker",
      "category": "redpanda"
    },
    "kafka_api_tls": {
      "example": [
        ".Example",
        "[,yaml]",
        "----",
        "redpanda:",
        "  kafka_api_tls:",
        "    - name: <kafka-api-listener-name>",
        "      enabled: true",
        "      cert_file: <path-to-cert-file>",
        "      key_file: <path-to-key-file>",
        "      truststore_file: <path-to-truststore-file>",
        "      require_client_auth: false",
        "----",
        "",
        "Replace the following placeholders with your values:",
        "",
        "* `<kafka-api-listener-name>`: Name that matches your Kafka API listener (defined in the <<kafka_api, `kafka_api`>> broker property)",
        "* `<path-to-cert-file>`: Full path to the TLS certificate file",
        "* `<path-to-key-file>`: Full path to the TLS private key file",
        "* `<path-to-truststore-file>`: Full path to the Certificate Authority file",
        "",
        "NOTE: Set `require_client_auth: true` for mutual TLS (mTLS) authentication, or `false` for server-side TLS only."
      ],
      "config_scope": "broker",
      "category": "redpanda"
    },
    "kafka_connection_rate_limit_overrides": {
      "related_topics": [
        "xref:manage:cluster-maintenance/configure-availability.adoc#limit-client-connections[Limit client connections]"
      ],
      "config_scope": "cluster"
    },
    "kafka_connections_max": {
      "related_topics": [
        "xref:manage:cluster-maintenance/configure-availability.adoc#limit-client-connections[Limit client connections]"
      ],
      "config_scope": "cluster"
    },
    "kafka_connections_max_overrides": {
      "related_topics": [
        "xref:manage:cluster-maintenance/configure-availability.adoc#limit-client-connections[Limit client connections]"
      ],
      "config_scope": "cluster",
      "description": "A list of IP addresses for which Kafka client connection limits are overridden and don't apply. For example, `(['127.0.0.1:90', '50.20.1.1:40']).`."
    },
    "kafka_connections_max_per_ip": {
      "related_topics": [
        "xref:manage:cluster-maintenance/configure-availability.adoc#limit-client-connections[Limit client connections]"
      ],
      "config_scope": "cluster"
    },
    "kafka_enable_authorization": {
      "description": "Flag to require authorization for Kafka connections. If `null`, the property is disabled, and authorization is instead enabled by <<enable_sasl, `enable_sasl`>>.\n\n* `null`: Ignored. Authorization is enabled with `enable_sasl`: `true`\n* `true`: authorization is required.\n* `false`: authorization is disabled.",
      "config_scope": "cluster"
    },
    "kafka_nodelete_topics": {
      "related_topics": [
        "xref:develop:consume-data/consumer-offsets.adoc[Consumer Offsets]",
        "xref:manage:schema-registry.adoc[Schema Registry]"
      ],
      "config_scope": "cluster"
    },
    "kafka_produce_batch_validation": {
      "description": "Controls the level of validation performed on batches produced to Redpanda. When set to `legacy`, there is minimal validation performed on the produce path. When set to `relaxed`, full validation is performed on uncompressed batches and on compressed batches with the `max_timestamp` value left unset. When set to `strict`, full validation of uncompressed and compressed batches is performed. This should be the default in environments where producing clients are not trusted.",
      "config_scope": "cluster"
    },
    "kafka_qdc_max_depth": {
      "description": "Maximum queue depth used in Kafka queue depth control.",
      "config_scope": "cluster"
    },
    "kafka_qdc_window_count": {
      "description": "Number of windows used in Kafka queue depth control latency tracking.",
      "config_scope": "cluster"
    },
    "kafka_sasl_max_reauth_ms": {
      "description": "The maximum time between Kafka client reauthentications. If a client has not reauthenticated a connection within this time frame, that connection is torn down.",
      "config_scope": "cluster"
    },
    "kafka_throughput_control": {
      "related_topics": [
        "xref:manage:cluster-maintenance/manage-throughput.adoc[Manage throughput]"
      ],
      "config_scope": "cluster",
      "description": "List of throughput control groups that define exclusions from broker-wide throughput limits. Clients excluded from broker-wide throughput limits are still potentially subject to client-specific throughput limits.\n\nEach throughput control group consists of:\n\n* `name` (optional) - any unique group name\n* `client_id` - regex to match client_id\n\nExample values:\n\n* `[{'name': 'first_group','client_id': 'client1'}, {'client_id': 'consumer-\\d+'}]`\n* `[{'name': 'catch all'}]`\n* `[{'name': 'missing_id', 'client_id': '+empty'}]`\n\nA connection is assigned the first matching group and is then excluded from throughput control. A `name` is not required, but can help you categorize the exclusions. Specifying `+empty` for the `client_id` will match on clients that opt not to send a `client_id`. You can also optionally omit the `client_id` and specify only a `name`, as shown. In this situation, all clients will match the rule and Redpanda will exclude them from all from broker-wide throughput control."
    },
    "kafka_throughput_limit_node_in_bps": {
      "related_topics": [
        "xref:manage:cluster-maintenance/manage-throughput.adoc#node-wide-throughput-limits[Node-wide throughput limits]"
      ],
      "config_scope": "cluster"
    },
    "kafka_throughput_limit_node_out_bps": {
      "related_topics": [
        "xref:manage:cluster-maintenance/manage-throughput.adoc#node-wide-throughput-limits[Node-wide throughput limits]"
      ],
      "config_scope": "cluster"
    },
    "kafka_throughput_replenish_threshold": {
      "related_topics": [
        "xref:reference:cluster-properties.adoc#kafka_throughput_limit_node_in_bps[`kafka_throughput_limit_node_in_bps`]",
        "xref:reference:cluster-properties.adoc#kafka_throughput_limit_node_out_bps[`kafka_throughput_limit_node_out_bps`]",
        "xref:manage:cluster-maintenance/manage-throughput.adoc[Manage Throughput]"
      ],
      "config_scope": "cluster",
      "description": "Threshold for refilling the token bucket as part of enforcing throughput limits.\n\nThis threshold is evaluated with each request for data. When the number of tokens to replenish exceeds this threshold, then tokens are added to the token bucket. This ensures that the atomic is not being updated for the token count with each request. The range for this threshold is automatically clamped to the corresponding throughput limit for ingress and egress."
    },
    "leader_balancer_idle_timeout": {
      "description": "Leadership rebalancing idle timeout.",
      "config_scope": "cluster"
    },
    "leader_balancer_mute_timeout": {
      "description": "The length of time that a glossterm:Raft[] group is muted after a leadership rebalance operation. Any group that has been moved, regardless of whether the move succeeded or failed, undergoes a cooling-off period. This prevents Raft groups from repeatedly experiencing leadership rebalance operations in a short time frame, which can lead to instability in the cluster.\n\nThe leader balancer maintains a list of muted groups and reevaluates muted status at the start of each balancing iteration. Muted groups still contribute to overall cluster balance calculations although they can't themselves be moved until the mute period is over.",
      "config_scope": "cluster"
    },
    "leader_balancer_node_mute_timeout": {
      "description": "The duration after which a broker that hasn't sent a heartbeat is considered muted. This timeout sets a threshold for identifying brokers that shouldn't be targeted for leadership transfers when the cluster rebalances, for example, because of unreliable network connectivity.",
      "config_scope": "cluster"
    },
    "log_cleanup_policy": {
      "description": "Default cleanup policy for topic logs.\n\nThe topic property xref:./topic-properties.adoc#cleanuppolicy[`cleanup.policy`] overrides the value of `log_cleanup_policy` at the topic level.",
      "related_topics": [
        "xref:./topic-properties.adoc#cleanuppolicy[`cleanup.policy`]"
      ],
      "config_scope": "cluster"
    },
    "log_compaction_merge_max_ranges": {
      "description": "The maximum range of segments that can be processed in a single round of adjacent segment compaction. If `null` (the default value), no maximum is imposed on the number of ranges that can be processed at once. A value below 1 effectively disables adjacent merge compaction.",
      "config_scope": "cluster"
    },
    "log_compaction_pause_use_sliding_window": {
      "description": "Pause use of sliding window compaction. Toggle to `true` _only_ when you want to force adjacent segment compaction. The memory reserved by `storage_compaction_key_map_memory` is not freed when this is set to `true`.",
      "config_scope": "cluster"
    },
    "log_compression_type": {
      "description": "IMPORTANT: This property is ignored regardless of the value specified. The behavior is always the same as the `producer` value. Redpanda brokers do not compress or recompress data based on this property. If producers send compressed data, Redpanda stores it as-is; if producers send uncompressed data, Redpanda stores it uncompressed. Other listed values are accepted for Apache Kafka compatibility but are ignored by the broker. This property may appear in Admin API and `rpk topic describe` outputs for compatibility.\n\nDefault for the Kafka-compatible compression.type property. Redpanda does not recompress data.\n\nThe topic property xref:./topic-properties.adoc#compressiontype[`compression.type`] overrides the value of `log_compression_type` at the topic level.",
      "related_topics": [
        "xref:./topic-properties.adoc#compressiontype[`compression.type`]"
      ],
      "config_scope": "cluster"
    },
    "log_message_timestamp_type": {
      "description": "Default timestamp type for topic messages (CreateTime or LogAppendTime).\n\nThe topic property xref:./topic-properties.adoc#messagetimestamptype[`message.timestamp.type`] overrides the value of `log_message_timestamp_type` at the topic level.",
      "related_topics": [
        "xref:./topic-properties.adoc#messagetimestamptype[`message.timestamp.type`]"
      ],
      "config_scope": "cluster"
    },
    "log_retention_ms": {
      "config_scope": "cluster",
      "description": "The amount of time to keep a log file before deleting it (in milliseconds). If set to `-1`, no time limit is applied. This is a cluster-wide default when a topic does not set or disable `retention.ms`."
    },
    "log_segment_ms": {
      "config_scope": "cluster"
    },
    "log_segment_ms_max": {
      "description": "Upper bound on topic `segment.ms`: higher values will be clamped to this value.",
      "config_scope": "cluster"
    },
    "log_segment_ms_min": {
      "description": "Lower bound on topic `segment.ms`: lower values will be clamped to this value.",
      "config_scope": "cluster"
    },
    "log_segment_size": {
      "description": "Default log segment size in bytes for topics which do not set `segment.bytes`.",
      "config_scope": "cluster"
    },
    "max.compaction.lag.ms": {
      "description": "The maximum amount of time (in ms) that a log segment can remain unaltered before it is eligible for compaction in a compact topic. Overrides the cluster property xref:cluster-properties.adoc#max_compaction_lag_ms[`max_compaction_lag_ms`] for the topic.",
      "related_topics": [
        "xref:cluster-properties.adoc#max_compaction_lag_ms[`max_compaction_lag_ms`]",
        "xref:./cluster-properties.adoc#max_compaction_lag_ms[`max_compaction_lag_ms`]",
        "xref:manage:cluster-maintenance/compaction-settings.adoc#configuration-options[Configure maximum compaction lag]"
      ],
      "config_scope": "topic"
    },
    "max.message.bytes": {
      "description": "The maximum size of a message or batch of a topic. If a compression type is enabled, `max.message.bytes` sets the maximum size of the compressed message or batch.\n\nIf `max.message.bytes` is set to a positive value, it overrides the cluster property xref:./cluster-properties.adoc#kafka_batch_max_bytes[`kafka_batch_max_bytes`] for the topic.",
      "related_topics": [
        "xref:./cluster-properties.adoc#kafka_batch_max_bytes[`kafka_batch_max_bytes`]",
        "xref:./cluster-properties.adoc#kafka_batch_max_bytes[`kafka_batch_max_bytes`]",
        "xref:develop:produce-data/configure-producers.adoc#message-batching[Message batching]"
      ],
      "config_scope": "topic"
    },
    "max_compaction_lag_ms": {
      "related_topics": [
        "xref:reference:properties/topic-properties.adoc#max.compaction.lag.ms[`max.compaction.lag.ms`]"
      ],
      "config_scope": "cluster"
    },
    "max_concurrent_producer_ids": {
      "description": "Maximum number of active producer sessions. When the threshold is passed, Redpanda terminates old sessions. When an idle producer corresponding to the terminated session wakes up and produces, its message batches are rejected, and an out of order sequence error is emitted. Consumers don't affect this setting.",
      "config_scope": "cluster"
    },
    "max_transactions_per_coordinator": {
      "description": "Specifies the maximum number of active transaction sessions per coordinator. When the threshold is passed Redpanda terminates old sessions. When an idle producer corresponding to the terminated session wakes up and produces, it leads to its batches being rejected with invalid producer epoch or invalid_producer_id_mapping error (depends on the transaction execution phase).\n\nFor details, see xref:develop:transactions#transaction-usage-tips[Transaction usage tips].",
      "related_topics": [
        "xref:develop:transactions#transaction-usage-tips[Transaction usage tips]"
      ],
      "config_scope": "cluster"
    },
    "memory_allocation_warning_threshold": {
      "config_scope": "broker",
      "category": "redpanda",
      "description": "Threshold for log messages that contain a larger memory allocation than specified."
    },
    "message.timestamp.type": {
      "description": "The source of a message's timestamp: either the message's creation time or its log append time.\n\nWhen `message.timestamp.type` is set, it overrides the cluster property xref:./cluster-properties.adoc#log_message_timestamp_type[`log_message_timestamp_type`] for the topic.",
      "related_topics": [
        "xref:./cluster-properties.adoc#log_message_timestamp_type[`log_message_timestamp_type`]",
        "xref:./cluster-properties.adoc#log_message_timestamp_type[`log_message_timestamp_type`]"
      ],
      "config_scope": "topic"
    },
    "metadata_dissemination_retries": {
      "description": "Number of attempts to look up a topic's metadata-like shard before a request fails. This configuration controls the number of retries that request handlers perform when internal topic metadata (for topics like tx, consumer offsets, etc) is missing. These topics are usually created on demand when users try to use the cluster for the first time and it may take some time for the creation to happen and the metadata to propagate to all the brokers (particularly the broker handling the request). In the meantime Redpanda waits and retries. This configuration controls the number retries.",
      "config_scope": "cluster"
    },
    "min.cleanable.dirty.ratio": {
      "description": "The minimum ratio between the number of bytes in dirty segments and the total number of bytes in closed segments that must be reached before a partition's log is eligible for compaction in a compact topic.",
      "related_topics": [
        "xref:./cluster-properties.adoc#min_cleanable_dirty_ratio[`min_cleanable_dirty_ratio`]"
      ],
      "config_scope": "topic"
    },
    "min.compaction.lag.ms": {
      "description": "The minimum amount of time (in ms) that a log segment must remain unaltered before it can be compacted in a compact topic. Overrides the cluster property xref:cluster-properties.adoc#min_compaction_lag_ms[`min_compaction_lag_ms`] for the topic.",
      "related_topics": [
        "xref:cluster-properties.adoc#min_compaction_lag_ms[`min_compaction_lag_ms`]",
        "xref:./cluster-properties.adoc#min_compaction_lag_ms[`min_compaction_lag_ms`]",
        "xref:manage:cluster-maintenance/compaction-settings.adoc#configure-min-compaction-lag[Configure minimum compaction lag]"
      ],
      "config_scope": "topic"
    },
    "min_cleanable_dirty_ratio": {
      "description": "The minimum ratio between the number of bytes in dirty segments and the total number of bytes in closed segments that must be reached before a partition's log is eligible for compaction in a compact topic. The topic property `min.cleanable.dirty.ratio` overrides this value at the topic level.",
      "config_scope": "cluster"
    },
    "min_compaction_lag_ms": {
      "related_topics": [
        "xref:reference:properties/topic-properties.adoc#min.compaction.lag.ms[`min.compaction.lag.ms`]"
      ],
      "config_scope": "cluster",
      "description": "The minimum amount of time (in ms) that a log segment must remain unaltered before it can be compacted in a compact topic."
    },
    "mode_mutability": {
      "description": "Enable modifications to the read-only `mode` of the Schema Registry. When set to `true`, the entire Schema Registry or its subjects can be switched to `READONLY` or `READWRITE`. This property is useful for preventing unwanted changes to the entire Schema Registry or specific subjects.",
      "config_scope": "broker",
      "category": "schema-registry"
    },
    "node_id": {
      "config_scope": "broker",
      "category": "redpanda",
      "description": "A number that uniquely identifies the broker within the cluster. If `null` (the default value), Redpanda automatically assigns an ID. If set, it must be non-negative value.\n\n.Do not set `node_id` manually.\n[WARNING]\n====\nRedpanda assigns unique IDs automatically to prevent issues such as:\n\n- Brokers with empty disks rejoining the cluster.\n- Conflicts during recovery or scaling.\n\nManually setting or reusing `node_id` values, even for decommissioned brokers, can cause cluster inconsistencies and operational failures.\n====\n\nBroker IDs are immutable. After a broker joins the cluster, its `node_id` *cannot* be changed."
    },
    "node_id_overrides": {
      "description": "List of node ID and UUID overrides applied at broker startup. Each entry includes the current UUID, the desired new ID and UUID, and an ignore flag. An entry applies only if `current_uuid` matches the broker's actual UUID.\n\nRemove this property after the cluster restarts successfully and operates normally. This prevents reapplication and maintains consistent configuration across brokers.",
      "example": [
        ".Example",
        "[,yaml]",
        "----",
        "redpanda:",
        "  node_id_overrides:",
        "    - current_uuid: \"<current-broker-uuid>\"",
        "      new_id: <new-broker-id>",
        "      new_uuid: \"<new-broker-uuid>\"",
        "      ignore_existing_node_id: <ignore-existing-flag>",
        "    - current_uuid: \"<another-current-uuid>\"",
        "      new_id: <another-new-broker-id>",
        "      new_uuid: \"<another-new-uuid>\"",
        "      ignore_existing_node_id: <another-ignore-flag>",
        "----",
        "",
        "Replace the following placeholders with your values:",
        "",
        "* `<current-broker-uuid>`: The current UUID of the broker to override",
        "* `<new-broker-id>`: The new broker ID to assign",
        "* `<new-broker-uuid>`: The new UUID to assign to the broker",
        "* `<ignore-existing-flag>`: Set to `true` to force override on brokers that already have a node ID, or `false` to apply override only to brokers without existing node IDs",
        "* `<another-current-uuid>`: Additional broker UUID for multiple overrides",
        "* `<another-new-broker-id>`: Additional new broker ID",
        "* `<another-new-uuid>`: Additional new UUID",
        "* `<another-ignore-flag>`: Additional ignore existing node ID flag"
      ],
      "config_scope": "broker",
      "category": "redpanda"
    },
    "oidc_clock_skew_tolerance": {
      "description": "The amount of time (in seconds) to allow for when validating the expiry claim in the token.",
      "config_scope": "cluster"
    },
    "oidc_discovery_url": {
      "description": "ifdef::env-cloud[]\nNOTE: This property is read-only in Redpanda Cloud.\nendif::[]\n\nThe URL pointing to the well-known discovery endpoint for the OIDC provider.",
      "config_scope": "cluster"
    },
    "oidc_principal_mapping": {
      "description": "ifdef::env-cloud[]\nNOTE: This property is read-only in Redpanda Cloud.\nendif::[]\n\nRule for mapping JWT payload claim to a Redpanda user principal.",
      "related_topics": [
        "xref:manage:security/authentication.adoc#oidc[OpenID Connect authentication]",
        "self-managed-only: xref:manage:kubernetes/security/authentication/k-authentication.adoc[OpenID Connect authentication in Kubernetes]"
      ],
      "config_scope": "cluster"
    },
    "oidc_token_audience": {
      "description": "ifdef::env-cloud[]\nNOTE: This property is read-only in Redpanda Cloud.\nendif::[]\n\nA string representing the intended recipient of the token.",
      "config_scope": "cluster"
    },
    "openssl_config_file": {
      "config_scope": "broker",
      "category": "redpanda"
    },
    "openssl_module_directory": {
      "config_scope": "broker",
      "category": "redpanda"
    },
    "pandaproxy_api": {
      "description": "Rest API listener address and port.",
      "example": [
        ".Example",
        "[,yaml]",
        "----",
        "pandaproxy:",
        "  pandaproxy_api:",
        "    address: 0.0.0.0",
        "    port: 8082",
        "    authentication_method: http_basic",
        "----"
      ],
      "config_scope": "broker",
      "category": "pandaproxy"
    },
    "pandaproxy_api_tls": {
      "description": "TLS configuration for Pandaproxy API.",
      "config_scope": "broker",
      "category": "pandaproxy"
    },
    "partition_autobalancing_max_disk_usage_percent": {
      "related_topics": [
        "xref:manage:cluster-maintenance/continuous-data-balancing.adoc[Configure Continuous Data Balancing]"
      ],
      "config_scope": "cluster"
    },
    "partition_autobalancing_mode": {
      "related_topics": [
        "xref:manage:cluster-maintenance/cluster-balancing.adoc[partition balancing]",
        "xref:manage:cluster-maintenance/continuous-data-balancing.adoc[Configure Continuous Data Balancing]"
      ],
      "description": "Mode of xref:manage:cluster-maintenance/cluster-balancing.adoc[partition balancing] for a cluster.\n\n*Accepted values:*\n\n* `continuous`: partition balancing happens automatically to maintain optimal performance and availability, based on continuous monitoring for node changes (same as `node_add`) and also high disk usage. This option is customized by <<partition_autobalancing_node_availability_timeout_sec,`partition_autobalancing_node_availability_timeout_sec`>> and <<partition_autobalancing_max_disk_usage_percent,`partition_autobalancing_max_disk_usage_percent`>> properties.\n* `node_add`: partition balancing happens when a node is added.\n* `off`: partition balancing is disabled. This option is not recommended for production clusters.",
      "config_scope": "cluster"
    },
    "partition_autobalancing_node_availability_timeout_sec": {
      "related_topics": [
        "xref:manage:cluster-maintenance/continuous-data-balancing.adoc[Configure Continuous Data Balancing]"
      ],
      "config_scope": "cluster"
    },
    "partition_manager_shutdown_watchdog_timeout": {
      "description": "A threshold value to detect partitions which might have been stuck while shutting down. After this threshold, a watchdog in partition manager will log information about partition shutdown not making progress.",
      "config_scope": "cluster"
    },
    "pp_sr_smp_max_non_local_requests": {
      "description": "Maximum number of Cross-core(Inter-shard communication) requests pending in HTTP Proxy and Schema Registry seastar::smp group. (For more details, see the `seastar::smp_service_group` documentation).\n\nSee https://docs.seastar.io/master/[Seastar documentation^]",
      "config_scope": "cluster"
    },
    "produce_ack_level": {
      "config_scope": "broker",
      "category": "pandaproxy-client",
      "description": "Number of acknowledgments the producer requires the leader to have received before considering a request complete."
    },
    "produce_batch_delay_ms": {
      "description": "Delay (in milliseconds) to wait before sending batch.",
      "config_scope": "broker",
      "category": "pandaproxy-client"
    },
    "produce_batch_record_count": {
      "config_scope": "broker",
      "category": "pandaproxy-client"
    },
    "produce_batch_size_bytes": {
      "config_scope": "broker",
      "category": "pandaproxy-client"
    },
    "produce_compression_type": {
      "config_scope": "broker",
      "category": "pandaproxy-client",
      "description": "Enable or disable compression by the Kafka client. Specify `none` to disable compression or one of the supported types [gzip, snappy, lz4, zstd]."
    },
    "produce_shutdown_delay_ms": {
      "description": "Delay (in milliseconds) to allow for final flush of buffers before shutting down.",
      "config_scope": "broker",
      "category": "pandaproxy-client"
    },
    "rack": {
      "related_topics": [
        "xref:./cluster-properties.adoc#enable_rack_awareness[enable_rack_awareness]"
      ],
      "config_scope": "broker",
      "category": "redpanda",
      "description": "A label that identifies a failure zone. Apply the same label to all brokers in the same failure zone. When xref:./cluster-properties.adoc#enable_rack_awareness[enable_rack_awareness] is set to `true` at the cluster level, the system uses the rack labels to spread partition replicas across different failure zones."
    },
    "raft_max_buffered_follower_append_entries_bytes_per_shard": {
      "description": "The total size of append entry requests that may be cached per shard, using the Raft-buffered protocol. When an entry is cached, the leader can continue serving requests because the ordering of the cached requests cannot change. When the total size of cached requests reaches the set limit, back pressure is applied to throttle producers.",
      "config_scope": "cluster"
    },
    "raft_max_inflight_follower_append_entries_requests_per_shard": {
      "description": "The maximum number of append entry requests that may be sent from Raft groups on a Seastar shard to the current node, and are awaiting a reply. This property replaces `raft_max_concurrent_append_requests_per_follower`.",
      "config_scope": "cluster"
    },
    "raft_recovery_throttle_disable_dynamic_mode": {
      "description": "include::reference:partial$internal-use-property.adoc[]\n\nDisables cross shard sharing used to throttle recovery traffic. Should only be used to debug unexpected problems.",
      "config_scope": "cluster"
    },
    "raft_smp_max_non_local_requests": {
      "description": "Maximum number of Cross-core(Inter-shard communication) requests pending in Raft seastar::smp group. For details, refer to the `seastar::smp_service_group` documentation).\n\nSee https://docs.seastar.io/master/[Seastar documentation^]",
      "config_scope": "cluster"
    },
    "reclaim_stable_window": {
      "description": "If the duration since the last time memory was reclaimed is longer than the amount of time specified in this property, the memory usage of the batch cache is considered stable, so only the minimum size (<<reclaim_min_size,`reclaim_min_size`>>) is set to be reclaimed.",
      "config_scope": "cluster"
    },
    "recovery_mode_enabled": {
      "description": "If `true`, start Redpanda in xref:manage:recovery-mode.adoc[recovery mode], where user partitions are not loaded and only administrative operations are allowed.",
      "related_topics": [
        "xref:manage:recovery-mode.adoc[recovery mode]"
      ],
      "config_scope": "broker",
      "category": "redpanda"
    },
    "redpanda.cloud_topic.enabled": {
      "config_scope": "topic",
      "category": "tiered-storage"
    },
    "redpanda.iceberg.delete": {
      "description": "Whether the corresponding Iceberg table is deleted upon deleting the topic.",
      "config_scope": "topic"
    },
    "redpanda.iceberg.invalid.record.action": {
      "description": "Whether to write invalid records to a dead-letter queue (DLQ).",
      "related_topics": [
        "xref:manage:iceberg/about-iceberg-topics.adoc#troubleshoot-errors[Troubleshoot errors]"
      ],
      "config_scope": "topic"
    },
    "redpanda.iceberg.mode": {
      "description": "Enable the Iceberg integration for the topic. You can choose one of four modes.",
      "related_topics": [
        "xref:manage:iceberg/choose-iceberg-mode.adoc#override-value-schema-latest-default[Choose an Iceberg Mode]"
      ],
      "config_scope": "topic"
    },
    "redpanda.iceberg.partition.spec": {
      "description": "The link:https://iceberg.apache.org/docs/nightly/partitioning/[partitioning^] specification for the Iceberg table.",
      "related_topics": [
        "xref:manage:iceberg/about-iceberg-topics.adoc#use-custom-partitioning[Use custom partitioning]"
      ],
      "config_scope": "topic"
    },
    "redpanda.iceberg.target.lag.ms": {
      "description": "Controls how often the data in the Iceberg table is refreshed with new data from the topic. Redpanda attempts to commit all data produced to the topic within the lag target, subject to resource availability.",
      "config_scope": "topic"
    },
    "redpanda.leaders.preference": {
      "description": "The preferred location (rack) for partition leaders of a topic.\n\nThis property inherits the value from the config_ref:default_leaders_preference,true,properties/cluster-properties[] cluster configuration property. You may override the cluster-wide setting by specifying the value for individual topics.\n\nIf the cluster configuration property config_ref:enable_rack_awareness,true,properties/cluster-properties[] is set to `false`, Leader Pinning is disabled across the cluster.",
      "related_topics": [
        "xref:develop:produce-data/leader-pinning.adoc[Leader pinning]"
      ],
      "config_scope": "topic"
    },
    "redpanda.remote.allowgaps": {
      "exclude_from_docs": true,
      "config_scope": "topic"
    },
    "redpanda.remote.delete": {
      "description": "A flag that enables deletion of data from object storage for Tiered Storage when it's deleted from local storage for a topic.\n\nNOTE: `redpanda.remote.delete` doesn't apply to Remote Read Replica topics: a Remote Read Replica topic isn't deleted from object storage when this flag is `true`.",
      "related_topics": [
        "xref:manage:tiered-storage.adoc[Tiered Storage]"
      ],
      "config_scope": "topic"
    },
    "redpanda.remote.read": {
      "description": "A flag for enabling Redpanda to fetch data for a topic from object storage to local storage. When set to `true` together with <<redpandaremotewrite, `redpanda.remote.write`>>, it enables the xref:manage:tiered-storage.adoc[Tiered Storage] feature.",
      "related_topics": [
        "xref:manage:tiered-storage.adoc[Tiered Storage]"
      ],
      "config_scope": "topic"
    },
    "redpanda.remote.readreplica": {
      "description": "The name of the object storage bucket for a Remote Read Replica topic.\n\nCAUTION: Setting `redpanda.remote.readreplica` together with either `redpanda.remote.read` or `redpanda.remote.write` results in an error.",
      "related_topics": [
        "xref:manage:remote-read-replicas.adoc[Remote Read Replicas]"
      ],
      "config_scope": "topic",
      "type": "string"
    },
    "redpanda.remote.recovery": {
      "description": "A flag that enables the recovery or reproduction of a topic from object storage for Tiered Storage. The recovered data is saved in local storage, and the maximum amount of recovered data is determined by the local storage retention limits of the topic.\n\nTIP: You can only configure `redpanda.remote.recovery` when you create a topic. You cannot apply this setting to existing topics.",
      "related_topics": [
        "xref:manage:tiered-storage.adoc[Tiered Storage]"
      ],
      "config_scope": "topic"
    },
    "redpanda.remote.write": {
      "description": "A flag for enabling Redpanda to upload data for a topic from local storage to object storage. When set to `true` together with <<redpandaremoteread, `redpanda.remote.read`>>, it enables the xref:manage:tiered-storage.adoc[Tiered Storage] feature.",
      "related_topics": [
        "xref:manage:tiered-storage.adoc[Tiered Storage]"
      ],
      "config_scope": "topic"
    },
    "redpanda.virtual.cluster.id": {
      "exclude_from_docs": true,
      "config_scope": "topic"
    },
    "replication.factor": {
      "description": "The number of replicas of a topic to save in different nodes (brokers) of a cluster.\n\nIf `replication.factor` is set to a positive value, it overrides the cluster property xref:./cluster-properties.adoc#default_topic_replication[default_topic_replication] for the topic.\n\nNOTE: Although `replication.factor` isn't returned or displayed by xref:reference:rpk/rpk-topic/rpk-topic-describe.adoc[`rpk topic describe`] as a valid Kafka property, you can set it using xref:reference:rpk/rpk-topic/rpk-topic-alter-config.adoc[`rpk topic alter-config`]. When the `replication.factor` of a topic is altered, it isn't simply a property value that's updated, but rather the actual replica sets of topic partitions that are changed.",
      "related_topics": [
        "xref:./cluster-properties.adoc#default_topic_replication[default_topic_replication]",
        "xref:reference:rpk/rpk-topic/rpk-topic-describe.adoc[`rpk topic describe`]",
        "xref:reference:rpk/rpk-topic/rpk-topic-alter-config.adoc[`rpk topic alter-config`]",
        "xref:./cluster-properties.adoc#default_topic_replication[`default_topic_replication`]",
        "xref:develop:config-topics.adoc#choose-the-replication-factor[Choose the replication factor]",
        "xref:develop:config-topics.adoc#change-the-replication-factor[Change the replication factor]"
      ],
      "config_scope": "topic"
    },
    "retention.bytes": {
      "description": "A size-based retention limit that configures the maximum size that a topic partition can grow before becoming eligible for cleanup.\n\nIf `retention.bytes` is set to a positive value, it overrides the cluster property xref:cluster-properties.adoc#retention_bytes[`retention_bytes`] for the topic, and the total retained size for the topic is `retention.bytes` multiplied by the number of partitions for the topic.\n\nWhen both size-based (`retention.bytes`) and time-based (`retention.ms`) retention limits are set, cleanup occurs when either limit is reached.",
      "related_topics": [
        "xref:cluster-properties.adoc#retention_bytes[`retention_bytes`]",
        "xref:./cluster-properties.adoc#retention_bytes[`retention_bytes`]",
        "xref:manage:cluster-maintenance/disk-utilization.adoc#configure-message-retention[Configure message retention]"
      ],
      "config_scope": "topic"
    },
    "retention.local.target.bytes": {
      "description": "A size-based retention limit for Tiered Storage that configures the maximum size that a topic partition in local storage can grow before becoming eligible for cleanup. It applies per partition and is equivalent to <<retentionbytes, `retention.bytes`>> without Tiered Storage.",
      "related_topics": [
        "xref:./cluster-properties.adoc#retention_local_target_bytes[`retention_local_target_bytes`]",
        "xref:manage:tiered-storage.adoc[Tiered Storage]"
      ],
      "config_scope": "topic"
    },
    "retention.local.target.ms": {
      "description": "A time-based retention limit for Tiered Storage that sets the maximum duration that a log's segment file for a topic is retained in local storage before it's eligible for cleanup. This property is equivalent to <<retentionms, `retention.ms`>> without Tiered Storage.",
      "related_topics": [
        "xref:./cluster-properties.adoc#retention_local_target_ms[`retention_local_target_ms`]",
        "xref:manage:tiered-storage.adoc[Tiered Storage]",
        "xref:manage:remote-read-replicas.adoc[Remote Read Replicas]"
      ],
      "config_scope": "topic"
    },
    "retention.ms": {
      "description": "A time-based retention limit that configures the maximum duration that a log's segment file for a topic is retained before it becomes eligible to be cleaned up. To consume all data, a consumer of the topic must read from a segment before its `retention.ms` elapses, otherwise the segment may be compacted and/or deleted. If a non-positive value, no per-topic limit is applied.\n\nIf `retention.ms` is set to a positive value, it overrides the cluster property xref:./cluster-properties.adoc#log_retention_ms[`log_retention_ms`] for the topic.\n\nWhen both size-based (`retention.bytes`) and time-based (`retention.ms`) retention limits are set, the earliest occurring limit applies.",
      "related_topics": [
        "xref:./cluster-properties.adoc#log_retention_ms[`log_retention_ms`]",
        "xref:./cluster-properties.adoc#log_retention_ms[`log_retention_ms`]",
        "xref:manage:cluster-maintenance/disk-utilization.adoc#configure-message-retention[Configure message retention]"
      ],
      "config_scope": "topic"
    },
    "retention_bytes": {
      "description": "Default maximum number of bytes per partition on disk before triggering deletion of the oldest messages. If `null` (the default value), no limit is applied.\n\nThe topic property xref:./topic-properties.adoc#retentionbytes[`retention.bytes`] overrides the value of `retention_bytes` at the topic level.",
      "related_topics": [
        "xref:./topic-properties.adoc#retentionbytes[`retention.bytes`]"
      ],
      "config_scope": "cluster"
    },
    "retention_local_target_bytes_default": {
      "related_topics": [
        "xref:manage:cluster-maintenance/disk-utilization.adoc#configure-message-retention[Configure message retention]"
      ],
      "config_scope": "cluster",
      "description": "Local retention size target for partitions of topics with object storage write enabled. If `null`, the property is disabled.\n\nThis property can be overridden on a per-topic basis by setting `retention.local.target.bytes` in each topic enabled for Tiered Storage. See xref:manage:cluster-maintenance/disk-utilization.adoc#configure-message-retention[Configure message retention]."
    },
    "retention_local_target_capacity_bytes": {
      "description": "The target capacity (in bytes) that log storage will try to use before additional retention rules take over to trim data to meet the target. When no target is specified, storage usage is unbounded.\n\nNOTE: Redpanda Data recommends setting only one of <<retention_local_target_capacity_bytes,`retention_local_target_capacity_bytes`>> or <<retention_local_target_capacity_percent,`retention_local_target_capacity_percent`>>. If both are set, the minimum of the two is used as the effective target capacity.",
      "config_scope": "cluster"
    },
    "retention_local_target_capacity_percent": {
      "description": "The target capacity in percent of unreserved space (<<disk_reservation_percent,`disk_reservation_percent`>>) that log storage will try to use before additional retention rules will take over to trim data in order to meet the target. When no target is specified storage usage is unbounded.\n\nNOTE: Redpanda Data recommends setting only one of <<retention_local_target_capacity_bytes,`retention_local_target_capacity_bytes`>> or <<retention_local_target_capacity_percent,`retention_local_target_capacity_percent`>>. If both are set, the minimum of the two is used as the effective target capacity.",
      "config_scope": "cluster"
    },
    "retention_local_target_ms_default": {
      "related_topics": [
        "xref:manage:cluster-maintenance/disk-utilization.adoc#configure-message-retention[Configure message retention]"
      ],
      "config_scope": "cluster",
      "description": "Local retention time target for partitions of topics with object storage write enabled.\n\nThis property can be overridden on a per-topic basis by setting `retention.local.target.ms` in each topic enabled for Tiered Storage. See xref:manage:cluster-maintenance/disk-utilization.adoc#configure-message-retention[Configure message retention]."
    },
    "retention_local_trim_interval": {
      "config_scope": "cluster"
    },
    "retries": {
      "config_scope": "broker",
      "category": "pandaproxy-client"
    },
    "retry_base_backoff_ms": {
      "description": "Delay (in milliseconds) for initial retry backoff.",
      "config_scope": "broker",
      "category": "pandaproxy-client"
    },
    "rpc_server": {
      "config_scope": "broker",
      "category": "redpanda"
    },
    "rpc_server_listen_backlog": {
      "description": "Maximum TCP connection queue length for Kafka server and internal RPC server. If `null` (the default value), no queue length is set.",
      "config_scope": "cluster"
    },
    "rpc_server_tls": {
      "example": [
        ".Example",
        "[,yaml]",
        "----",
        "redpanda:",
        "  rpc_server_tls:",
        "    enabled: true",
        "    cert_file: \"<path-to-cert-file>\"",
        "    key_file: \"<path-to-key-file>\"",
        "    truststore_file: \"<path-to-truststore-file>\"",
        "    require_client_auth: true",
        "----",
        "",
        "Replace the following placeholders with your values:",
        "",
        "* `<path-to-cert-file>`: Full path to the RPC TLS certificate file",
        "* `<path-to-key-file>`: Full path to the RPC TLS private key file",
        "* `<path-to-truststore-file>`: Full path to the certificate authority file"
      ],
      "config_scope": "broker",
      "category": "redpanda"
    },
    "rpk_path": {
      "description": "Path to RPK binary.",
      "config_scope": "cluster"
    },
    "sasl_mechanism": {
      "description": "The SASL mechanism to use when the HTTP Proxy client connects to the Kafka API. These credentials are used when the HTTP Proxy API listener has <<http_proxy_auth_method, `authentication_method`>>: `none` but the cluster requires authenticated access to the Kafka API.\n\nThis property specifies which individual SASL mechanism the HTTP Proxy client should use, while the cluster-wide available mechanisms are configured using the xref:reference:properties/cluster-properties.adoc#sasl_mechanisms[`sasl_mechanisms`] cluster property.\n\ninclude::shared:partial$http-proxy-ephemeral-credentials-breaking-change.adoc[]\n\nNOTE: While the cluster-wide xref:reference:properties/cluster-properties.adoc#sasl_mechanisms[`sasl_mechanisms`] property may support additional mechanisms (PLAIN, GSSAPI, OAUTHBEARER), HTTP Proxy client connections only support SCRAM mechanisms.",
      "related_topics": [
        "xref:reference:properties/cluster-properties.adoc#sasl_mechanisms[`sasl_mechanisms`]"
      ],
      "config_scope": "broker",
      "category": "pandaproxy-client"
    },
    "sasl_mechanisms": {
      "description": "A list of supported SASL mechanisms.\n\n*Accepted values:*\n\n* `SCRAM`\n* `GSSAPI`\n* `OAUTHBEARER`\n* `PLAIN`\n\nNote that in order to enable PLAIN, you must also enable SCRAM.",
      "related_topics": [],
      "config_scope": "cluster"
    },
    "sasl_mechanisms_overrides": {
      "description": "A list of overrides for SASL mechanisms, defined by listener. SASL mechanisms defined here will replace the ones set in `sasl_mechanisms`. The same limitations apply as for `sasl_mechanisms`.",
      "related_topics": [],
      "config_scope": "cluster"
    },
    "schema_registry_always_normalize": {
      "description": "Always normalize schemas. If set, this overrides the `normalize` parameter in requests to the Schema Registry API.",
      "config_scope": "cluster"
    },
    "schema_registry_api": {
      "example": [
        ".Example",
        "[,yaml]",
        "----",
        "schema_registry:",
        "  schema_registry_api:",
        "    address: 0.0.0.0",
        "    port: 8081",
        "    authentication_method: http_basic",
        "----"
      ],
      "config_scope": "broker",
      "category": "schema-registry"
    },
    "schema_registry_api_tls": {
      "config_scope": "broker",
      "category": "schema-registry"
    },
    "schema_registry_enable_authorization": {
      "description": "Enables ACL-based authorization for Schema Registry requests. When `true`, Schema Registry\nuses ACL-based authorization instead of the default `public/user/superuser` authorization model. \nifdef::env-cloud[]\nRequires authentication to be enabled using the `authentication_method` property in the `schema_registry_api` broker configuration.\nendif::[]",
      "related_topics": [],
      "config_scope": "cluster"
    },
    "schema_registry_replication_factor": {
      "description": "Replication factor for internal `_schemas` topic. If unset, defaults to the xref:../cluster-properties.adoc#default_topic_replication[`default_topic_replication`] cluster property.",
      "related_topics": [
        "xref:../cluster-properties.adoc#default_topic_replication[`default_topic_replication`]"
      ],
      "config_scope": "broker",
      "category": "schema-registry"
    },
    "scram_password": {
      "description": "Password to use for SCRAM authentication mechanisms when the HTTP Proxy client connects to the Kafka API. This property is required when the HTTP Proxy API listener has <<http_proxy_auth_method, `authentication_method`>>: `none` but the cluster requires authenticated access to the Kafka API.\n\ninclude::shared:partial$http-proxy-ephemeral-credentials-breaking-change.adoc[]",
      "config_scope": "broker",
      "category": "pandaproxy-client"
    },
    "scram_username": {
      "description": "Username to use for SCRAM authentication mechanisms when the HTTP Proxy client connects to the Kafka API. This property is required when the HTTP Proxy API listener has <<http_proxy_auth_method, `authentication_method`>>: `none` but the cluster requires authenticated access to the Kafka API.\n\ninclude::shared:partial$http-proxy-ephemeral-credentials-breaking-change.adoc[]",
      "config_scope": "broker",
      "category": "pandaproxy-client"
    },
    "seed_servers": {
      "description": "List of the seed servers used to join current cluster. If the `seed_servers` list is empty the broker will be a cluster root and it will form a new cluster.\n\n* When `empty_seed_starts_cluster` is `true`, Redpanda enables one broker with an empty `seed_servers` list to initiate a new cluster. The broker with an empty `seed_servers` becomes the cluster root, to which other brokers must connect to join the cluster.  Brokers looking to join the cluster should have their `seed_servers` populated with the cluster root's address, facilitating their connection to the cluster.\n+\n[IMPORTANT]\n====\nOnly one broker, the designated cluster root, should have an empty `seed_servers` list during the initial cluster bootstrapping. This ensures a single initiation point for cluster formation.\n====\n\n* When `empty_seed_starts_cluster` is `false`, Redpanda requires all brokers to start with a known set of brokers listed in `seed_servers`. The `seed_servers` list must not be empty and should be identical across these initial seed brokers, containing the addresses of all seed brokers. Brokers not included in the `seed_servers` list use it to discover and join the cluster, allowing for expansion beyond the foundational members.\n+\n[NOTE]\n====\nThe `seed_servers` list must be consistent across all seed brokers to prevent cluster fragmentation and ensure stable cluster formation.\n====",
      "example": [
        ".Example with `empty_seed_starts_cluster: true`\n[,yaml]\n----\n# Cluster root broker (seed starter)\nredpanda:\n  empty_seed_starts_cluster: true\n  seed_servers: []\n----\n\n[,yaml]\n----\n# Additional brokers joining the cluster\nredpanda:\n  empty_seed_starts_cluster: true\n  seed_servers:\n    - host:\n        address: <seed-broker-ip>\n        port: <rpc-port>\n----\n\n.Example with `empty_seed_starts_cluster: false`\n[,yaml]\n----\n# All initial seed brokers use the same configuration\nredpanda:\n  empty_seed_starts_cluster: false\n  seed_servers:\n    - host:\n        address: <seed-broker-1-ip>\n        port: <rpc-port>\n    - host:\n        address: <seed-broker-2-ip>\n        port: <rpc-port>\n    - host:\n        address: <seed-broker-3-ip>\n        port: <rpc-port>\n----\n\nReplace the following placeholders with your values:\n\n* `<seed-broker-ip>`: IP address of the cluster root broker\n* `<seed-broker-x-ip>`: IP addresses of each seed broker in the cluster\n* `<rpc-port>`: RPC port for brokers (default: `33145`)"
      ],
      "config_scope": "broker",
      "category": "redpanda"
    },
    "segment.bytes": {
      "description": "The maximum size of an active log segment for a topic. When the size of an active segment exceeds `segment.bytes`, the segment is closed and a new active segment is created. The closed, inactive segment is then eligible to be cleaned up according to retention properties.\n\nWhen `segment.bytes` is set to a positive value, it overrides the cluster property xref:./cluster-properties.adoc#log_segment_size[`log_segment_size`] for the topic.",
      "related_topics": [
        "xref:./cluster-properties.adoc#log_segment_size[`log_segment_size`]",
        "xref:./cluster-properties.adoc#log_segment_size[`log_segment_size`]",
        "xref:manage:cluster-maintenance/disk-utilization.adoc#configure-segment-size[Configure segment size]",
        "xref:manage:cluster-maintenance/disk-utilization.adoc#configure-message-retention[Configure message retention]",
        "xref:manage:remote-read-replicas.adoc[Remote Read Replicas]"
      ],
      "config_scope": "topic"
    },
    "segment.ms": {
      "description": "The maximum duration that a log segment of a topic is active (open for writes and not deletable). A periodic event, with `segment.ms` as its period, forcibly closes the active segment and transitions, or rolls, to a new active segment. The closed (inactive) segment is then eligible to be cleaned up according to cleanup and retention properties.\n\nIf set to a positive duration, `segment.ms` overrides the cluster property xref:./cluster-properties.adoc#log_segment_ms[`log_segment_ms`]. Values are automatically clamped between the cluster bounds set by xref:./cluster-properties.adoc#log_segment_ms_min[`log_segment_ms_min`] (default: 10 minutes) and xref:./cluster-properties.adoc#log_segment_ms_max[`log_segment_ms_max`] (default: 1 year). If your configured value exceeds these bounds, Redpanda uses the bound value and logs a warning. Check current cluster bounds with `rpk cluster config get log_segment_ms_min log_segment_ms_max`.",
      "related_topics": [
        "xref:./cluster-properties.adoc#log_segment_ms[`log_segment_ms`]",
        "xref:./cluster-properties.adoc#log_segment_ms_min[`log_segment_ms_min`]",
        "xref:./cluster-properties.adoc#log_segment_ms_max[`log_segment_ms_max`]",
        "xref:./cluster-properties.adoc#log_segment_ms[`log_segment_ms`]",
        "xref:manage:cluster-maintenance/disk-utilization.adoc#log-rolling[Log rolling]"
      ],
      "config_scope": "topic"
    },
    "storage_compaction_key_map_memory": {
      "description": "Maximum number of bytes that may be used on each shard by compaction key-offset maps. Only applies when <<log_compaction_use_sliding_window,`log_compaction_use_sliding_window`>> is set to `true`.",
      "config_scope": "cluster"
    },
    "storage_compaction_key_map_memory_limit_percent": {
      "description": "Limit on <<storage_compaction_key_map_memory,`storage_compaction_key_map_memory`>>, expressed as a percentage of memory per shard, that bounds the amount of memory used by compaction key-offset maps. \n\nNOTE: Memory per shard is computed after <<data_transforms_per_core_memory_reservation,`data_transforms_per_core_memory_reservation`>>, and only applies when <<log_compaction_use_sliding_window,`log_compaction_use_sliding_window`>> is set to `true`.",
      "config_scope": "cluster"
    },
    "storage_failure_injection_config_path": {
      "config_scope": "broker",
      "category": "redpanda"
    },
    "storage_failure_injection_enabled": {
      "config_scope": "broker",
      "category": "redpanda"
    },
    "storage_ignore_timestamps_in_future_sec": {
      "description": "The maximum number of seconds that a record's timestamp can be ahead of a Redpanda broker's clock and still be used when deciding whether to clean up the record for data retention. This property makes possible the timely cleanup of records from clients with clocks that are drastically unsynchronized relative to Redpanda.\n\nWhen determining whether to clean up a record with timestamp more than `storage_ignore_timestamps_in_future_sec` seconds ahead of the broker, Redpanda ignores the record's timestamp and instead uses a valid timestamp of another record in the same segment, or (if another record's valid timestamp is unavailable) the timestamp of when the segment file was last modified (mtime).\n\nBy default, `storage_ignore_timestamps_in_future_sec` is disabled (null).\n\n[TIP]\n====\nTo figure out whether to set `storage_ignore_timestamps_in_future_sec` for your system:\n\n. Look for logs with segments that are unexpectedly large and not being cleaned up.\n. In the logs, search for records with unsynchronized timestamps that are further into the future than tolerable by your data retention and storage settings. For example, timestamps 60 seconds or more into the future can be considered to be too unsynchronized.\n. If you find unsynchronized timestamps throughout your logs, determine the number of seconds that the timestamps are ahead of their actual time, and set `storage_ignore_timestamps_in_future_sec` to that value so data retention can proceed.\n. If you only find unsynchronized timestamps that are the result of transient behavior, you can disable `storage_ignore_timestamps_in_future_sec`.\n====",
      "config_scope": "cluster"
    },
    "storage_strict_data_init": {
      "description": "Requires that an empty file named `.redpanda_data_dir` be present in the xref:reference:properties/broker-properties.adoc#data_directory[`data_ directory`]. If set to `true`, Redpanda will refuse to start if the file is not found in the data directory.",
      "related_topics": [
        "xref:reference:properties/broker-properties.adoc#data_directory[`data_ directory`]"
      ],
      "config_scope": "cluster"
    },
    "tls_certificate_name_format": {
      "description": "The format of the certificates's distinguished name to use for mTLS principal mapping. The `legacy` format would appear as 'C=US,ST=California,L=San Francisco,O=Redpanda,CN=redpanda', while the `rfc2253` format would appear as 'CN=redpanda,O=Redpanda,L=San Francisco,ST=California,C=US'.",
      "config_scope": "cluster"
    },
    "tls_enable_renegotiation": {
      "description": "TLS client-initiated renegotiation is considered unsafe and is disabled by default . Only re-enable it if you are experiencing issues with your TLS-enabled client. This option has no effect on TLSv1.3 connections as client-initiated renegotiation was removed.",
      "config_scope": "cluster"
    },
    "tombstone_retention_ms": {
      "description": "The retention time for tombstone records in a compacted topic. Cannot be enabled at the same time as any of `cloud_storage_enabled`, `cloud_storage_enable_remote_read`, or `cloud_storage_enable_remote_write`. A typical default setting is `86400000`, or 24 hours.",
      "related_topics": [
        "xref:manage:cluster-maintenance/compaction-settings.adoc#tombstone-record-removal[Tombstone record removal]"
      ],
      "config_scope": "cluster"
    },
    "topic_fds_per_partition": {
      "description": "File descriptors required per partition replica. If topic creation results in the ratio of file descriptor limit to partition replicas being lower than this value, creation of new topics is fails.",
      "config_scope": "cluster"
    },
    "topic_label_aggregation_limit": {
      "description": "When the number of topics exceeds this limit, the topic label in generated metrics will be aggregated. If `null`, then there is no limit.",
      "config_scope": "cluster"
    },
    "topic_memory_per_partition": {
      "description": "Required memory in bytes per partition replica when creating or altering topics. The total size of the memory pool for partitions is the total memory available to Redpanda times `topic_partitions_memory_allocation_percent`. Each partition created requires `topic_memory_per_partition` bytes from that pool. If insufficient memory is available, creating or altering topics fails.",
      "config_scope": "cluster"
    },
    "topic_partitions_memory_allocation_percent": {
      "description": "Percentage of total memory to reserve for topic partitions. See <<topic_memory_per_partition, `topic_memory_per_partition`>> for details.",
      "config_scope": "cluster"
    },
    "topic_partitions_per_shard": {
      "description": "Maximum number of partition replicas per shard. If topic creation results in the ratio of partition replicas to shards being higher than this value, creation of new topics fails.",
      "config_scope": "cluster"
    },
    "topic_partitions_reserve_shard0": {
      "description": "Reserved partition slots on shard (CPU core) 0 on each node.  If this is greater than or equal to <<topic_partitions_per_core,`topic_partitions_per_core`>>, no data partitions will be scheduled on shard 0.",
      "config_scope": "cluster"
    },
    "transaction_coordinator_cleanup_policy": {
      "description": "Cleanup policy for a transaction coordinator topic.\n\n*Accepted values:*\n\n* `compact`\n* `delete`\n* `[\"compact\",\"delete\"]`\n* `none`",
      "config_scope": "cluster"
    },
    "transaction_coordinator_delete_retention_ms": {
      "description": "Delete segments older than this age. To ensure transaction state is retained for as long as the longest-running transaction, make sure this is greater than or equal to <<transactional_id_expiration_ms,`transactional_id_expiration_ms`>>.\n\nFor example, if your typical transactions run for one hour, consider setting both `transaction_coordinator_delete_retention_ms` and `transactional_id_expiration_ms` to at least 3600000 (one hour), or a little over.",
      "config_scope": "cluster"
    },
    "upgrade_override_checks": {
      "config_scope": "broker",
      "category": "redpanda"
    },
    "use_kafka_handler_scheduler_group": {
      "description": "Use a separate scheduler group to handle parsing Kafka protocol requests.",
      "config_scope": "cluster"
    },
    "use_produce_scheduler_group": {
      "description": "Use a separate scheduler group to process Kafka produce requests.",
      "config_scope": "cluster"
    },
    "verbose_logging_timeout_sec_max": {
      "config_scope": "broker",
      "category": "redpanda",
      "description": "Maximum duration in seconds for verbose (`TRACE` or `DEBUG`) logging. Values configured above this will be clamped. If null (the default) there is no limit. Can be overridden in the Admin API on a per-request basis."
    },
    "write.caching": {
      "description": "The write caching mode to apply to a topic.\n\nWhen `write.caching` is set, it overrides the cluster property xref:cluster-properties.adoc#write_caching_default[`write_caching_default`]. Write caching acknowledges a message as soon as it is received and acknowledged on a majority of brokers, without waiting for it to be written to disk. With `acks=all`, this provides lower latency while still ensuring that a majority of brokers acknowledge the write. Fsyncs follow <<flushms, `flush.ms`>> and <<flushbytes, `flush.bytes`>>, whichever is reached first.",
      "related_topics": [
        "xref:develop:config-topics.adoc#configure-write-caching[Write caching]",
        "xref:manage:tiered-storage.adoc[Tiered Storage]"
      ],
      "config_scope": "topic"
    },
    "write_caching_default": {
      "related_topics": [
        "xref:reference:properties/topic-properties.adoc#writecaching[`write.caching`]",
        "xref:develop:config-topics.adoc#configure-write-caching[Write caching]"
      ],
      "config_scope": "cluster",
      "description": "The default write caching mode to apply to user topics. Write caching acknowledges a message as soon as it is received and acknowledged on a majority of brokers, without waiting for it to be written to disk. With `acks=all`, this provides lower latency while still ensuring that a majority of brokers acknowledge the write. \n\nFsyncs follow <<raft_replica_max_pending_flush_bytes,`raft_replica_max_pending_flush_bytes`>> and <<raft_replica_max_flush_delay_ms,`raft_replica_max_flush_delay_ms`>>, whichever is reached first.\n\nThe `write_caching_default` cluster property can be overridden with the xref:reference:properties/topic-properties.adoc#writecaching[`write.caching`] topic property."
    }
  }
}