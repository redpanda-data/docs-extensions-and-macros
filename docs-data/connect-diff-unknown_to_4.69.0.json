{
  "comparison": {
    "oldVersion": "",
    "newVersion": "4.69.0",
    "timestamp": "2025-12-15T10:25:52.316Z"
  },
  "summary": {
    "newComponents": 544,
    "removedComponents": 0,
    "newFields": 0,
    "removedFields": 0,
    "deprecatedComponents": 0,
    "deprecatedFields": 0,
    "changedDefaults": 0
  },
  "details": {
    "newComponents": [
      {
        "name": "http",
        "type": "config",
        "status": "object",
        "version": "",
        "description": "Configures the service-wide HTTP server."
      },
      {
        "name": "input",
        "type": "config",
        "status": "input",
        "version": "",
        "description": "An input to source messages from."
      },
      {
        "name": "buffer",
        "type": "config",
        "status": "buffer",
        "version": "",
        "description": "An optional buffer to store messages during transit."
      },
      {
        "name": "pipeline",
        "type": "config",
        "status": "object",
        "version": "",
        "description": "Describes optional processing pipelines used for mutating messages."
      },
      {
        "name": "output",
        "type": "config",
        "status": "output",
        "version": "",
        "description": "An output to sink messages to."
      },
      {
        "name": "input_resources",
        "type": "config",
        "status": "input",
        "version": "",
        "description": "A list of input resources, each must have a unique label."
      },
      {
        "name": "processor_resources",
        "type": "config",
        "status": "processor",
        "version": "",
        "description": "A list of processor resources, each must have a unique label."
      },
      {
        "name": "output_resources",
        "type": "config",
        "status": "output",
        "version": "",
        "description": "A list of output resources, each must have a unique label."
      },
      {
        "name": "cache_resources",
        "type": "config",
        "status": "cache",
        "version": "",
        "description": "A list of cache resources, each must have a unique label."
      },
      {
        "name": "rate_limit_resources",
        "type": "config",
        "status": "rate_limit",
        "version": "",
        "description": "A list of rate limit resources, each must have a unique label."
      },
      {
        "name": "logger",
        "type": "config",
        "status": "object",
        "version": "",
        "description": "Describes how operational logs should be emitted."
      },
      {
        "name": "metrics",
        "type": "config",
        "status": "metrics",
        "version": "",
        "description": "A mechanism for exporting metrics."
      },
      {
        "name": "tracer",
        "type": "config",
        "status": "tracer",
        "version": "",
        "description": "A mechanism for exporting traces."
      },
      {
        "name": "shutdown_delay",
        "type": "config",
        "status": "string",
        "version": "",
        "description": "A period of time to wait for metrics and traces to be pulled or pushed from the process."
      },
      {
        "name": "shutdown_timeout",
        "type": "config",
        "status": "string",
        "version": "",
        "description": "The maximum period of time to wait for a clean shutdown. If this time is exceeded Redpanda Connect will forcefully close."
      },
      {
        "name": "tests",
        "type": "config",
        "status": "object",
        "version": "",
        "description": "A list of one or more unit tests to execute."
      },
      {
        "name": "redpanda",
        "type": "config",
        "status": "object",
        "version": "",
        "description": ""
      },
      {
        "name": "memory",
        "type": "buffers",
        "status": "stable",
        "version": "",
        "description": "\nThis buffer is appropriate when consuming messages from inputs that do not gracefully handle back pressure and where delivery guarantees aren't critical.\n\nThis buffer has a configurable limit, where consumption will be stopped with back pressure upstream if the total size of messages in the buffer reaches this amount. Since this calculation is only an estimate, and the real size of messages in RAM is always higher, it is recommended to set the limit significantly below the amount of RAM available.\n\n== Delivery guarantees\n\nThis buffer intentionally weakens the delivery guarantees of the pipeline and therefore should never be used in places where data loss is unacceptable.\n\n== Batching\n\nIt is possible to batch up messages sent from this buffer using a xref:configuration:batching.adoc#batch-policy[batch policy]."
      },
      {
        "name": "none",
        "type": "buffers",
        "status": "stable",
        "version": "",
        "description": "Selecting no buffer means the output layer is directly coupled with the input layer. This is the safest and lowest latency option since acknowledgements from at-least-once protocols can be propagated all the way from the output protocol to the input protocol.\n\nIf the output layer is hit with back pressure it will propagate all the way to the input layer, and further up the data stream. If you need to relieve your pipeline of this back pressure consider using a more robust buffering solution such as Kafka before resorting to alternatives."
      },
      {
        "name": "sqlite",
        "type": "buffers",
        "status": "stable",
        "version": "",
        "description": "\nStored messages are then consumed as a stream from the database and deleted only once they are successfully sent at the output level. If the service is restarted Redpanda Connect will make a best attempt to finish delivering messages that are already read from the database, and when it starts again it will consume from the oldest message that has not yet been delivered.\n\n== Delivery guarantees\n\nMessages are not acknowledged at the input level until they have been added to the SQLite database, and they are not removed from the SQLite database until they have been successfully delivered. This means at-least-once delivery guarantees are preserved in cases where the service is shut down unexpectedly. However, since this process relies on interaction with the disk (wherever the SQLite DB is stored) these delivery guarantees are not resilient to disk corruption or loss.\n\n== Batching\n\nMessages that are logically batched at the point where they are added to the buffer will continue to be associated with that batch when they are consumed. This buffer is also more efficient when storing messages within batches, and therefore it is recommended to use batching at the input level in high-throughput use cases even if they are not required for processing.\n"
      },
      {
        "name": "system_window",
        "type": "buffers",
        "status": "beta",
        "version": "3.53.0",
        "description": "\nA window is a grouping of messages that fit within a discrete measure of time following the system clock. Messages are allocated to a window either by the processing time (the time at which they're ingested) or by the event time, and this is controlled via the <<timestamp_mapping, `timestamp_mapping` field>>.\n\nIn tumbling mode (default) the beginning of a window immediately follows the end of a prior window. When the buffer is initialized the first window to be created and populated is aligned against the zeroth minute of the zeroth hour of the day by default, and may therefore be open for a shorter period than the specified size.\n\nA window is flushed only once the system clock surpasses its scheduled end. If an <<allowed_lateness, `allowed_lateness`>> is specified then the window will not be flushed until the scheduled end plus that length of time.\n\nWhen a message is added to a window it has a metadata field `window_end_timestamp` added to it containing the timestamp of the end of the window as an RFC3339 string.\n\n== Sliding windows\n\nSliding windows begin from an offset of the prior windows' beginning rather than its end, and therefore messages may belong to multiple windows. In order to produce sliding windows specify a <<slide, `slide` duration>>.\n\n== Back pressure\n\nIf back pressure is applied to this buffer either due to output services being unavailable or resources being saturated, windows older than the current and last according to the system clock will be dropped in order to prevent unbounded resource usage. This means you should ensure that under the worst case scenario you have enough system memory to store two windows' worth of data at a given time (plus extra for redundancy and other services).\n\nIf messages could potentially arrive with event timestamps in the future (according to the system clock) then you should also factor in these extra messages in memory usage estimates.\n\n== Delivery guarantees\n\nThis buffer honours the transaction model within Redpanda Connect in order to ensure that messages are not acknowledged until they are either intentionally dropped or successfully delivered to outputs. However, since messages belonging to an expired window are intentionally dropped there are circumstances where not all messages entering the system will be delivered.\n\nWhen this buffer is configured with a slide duration it is possible for messages to belong to multiple windows, and therefore be delivered multiple times. In this case the first time the message is delivered it will be acked (or nacked) and subsequent deliveries of the same message will be a \"best attempt\".\n\nDuring graceful termination if the current window is partially populated with messages they will be nacked such that they are re-consumed the next time the service starts.\n"
      },
      {
        "name": "aws_dynamodb",
        "type": "caches",
        "status": "stable",
        "version": "3.36.0",
        "description": "A prefix can be specified to allow multiple cache types to share a single DynamoDB table. An optional TTL duration (`ttl`) and field\n(`ttl_key`) can be specified if the backing table has TTL enabled.\n\nStrong read consistency can be enabled using the `consistent_read` configuration field."
      },
      {
        "name": "aws_s3",
        "type": "caches",
        "status": "stable",
        "version": "3.36.0",
        "description": "It is not possible to atomically upload S3 objects exclusively when the target does not already exist, therefore this cache is not suitable for deduplication."
      },
      {
        "name": "couchbase",
        "type": "caches",
        "status": "experimental",
        "version": "4.12.0",
        "description": ""
      },
      {
        "name": "file",
        "type": "caches",
        "status": "stable",
        "version": "",
        "description": "This type currently offers no form of item expiry or garbage collection, and is intended to be used for development and debugging purposes only."
      },
      {
        "name": "gcp_cloud_storage",
        "type": "caches",
        "status": "beta",
        "version": "",
        "description": "It is not possible to atomically upload cloud storage objects exclusively when the target does not already exist, therefore this cache is not suitable for deduplication."
      },
      {
        "name": "lru",
        "type": "caches",
        "status": "stable",
        "version": "",
        "description": "This provides the lru package which implements a fixed-size thread safe LRU cache.\n\nIt uses the package https://github.com/hashicorp/golang-lru/v2[`lru`^]\n\nThe field init_values can be used to pre-populate the memory cache with any number of key/value pairs:\n\n```yaml\ncache_resources:\n  - label: foocache\n    lru:\n      cap: 1024\n      init_values:\n        foo: bar\n```\n\nThese values can be overridden during execution."
      },
      {
        "name": "memcached",
        "type": "caches",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "memory",
        "type": "caches",
        "status": "stable",
        "version": "",
        "description": "The compaction interval determines how often the cache is cleared of expired items, and this process is only triggered on writes to the cache. Access to the cache is blocked during this process.\n\nItem expiry can be disabled entirely by setting the `compaction_interval` to an empty string.\n\nThe field `init_values` can be used to prepopulate the memory cache with any number of key/value pairs which are exempt from TTLs:\n\n```yaml\ncache_resources:\n  - label: foocache\n    memory:\n      default_ttl: 60s\n      init_values:\n        foo: bar\n```\n\nThese values can be overridden during execution, at which point the configured TTL is respected as usual."
      },
      {
        "name": "mongodb",
        "type": "caches",
        "status": "experimental",
        "version": "3.43.0",
        "description": ""
      },
      {
        "name": "multilevel",
        "type": "caches",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "nats_kv",
        "type": "caches",
        "status": "experimental",
        "version": "4.27.0",
        "description": "== Connection name\n\nWhen monitoring and managing a production NATS system, it is often useful to\nknow which connection a message was send/received from. This can be achieved by\nsetting the connection name option when creating a NATS connection.\n\nRedpanda Connect will automatically set the connection name based off the label of the given\nNATS component, so that monitoring tools between NATS and Redpanda Connect can stay in sync.\n\n\n== Authentication\n\nThere are several components within Redpanda Connect which uses NATS services. You will find that each of these components\nsupport optional advanced authentication parameters for https://docs.nats.io/nats-server/configuration/securing_nats/auth_intro/nkey_auth[NKeys^]\nand https://docs.nats.io/using-nats/developer/connecting/creds[User Credentials^].\n\nSee an https://docs.nats.io/running-a-nats-service/nats_admin/security/jwt[in-depth tutorial^].\n\n=== NKey file\n\nThe NATS server can use these NKeys in several ways for authentication. The simplest is for the server to be configured\nwith a list of known public keys and for the clients to respond to the challenge by signing it with its private NKey\nconfigured in the `nkey_file` or `nkey` field.\n\nhttps://docs.nats.io/running-a-nats-service/configuration/securing_nats/auth_intro/nkey_auth[More details^].\n\n=== User credentials\n\nNATS server supports decentralized authentication based on JSON Web Tokens (JWT). Clients need an https://docs.nats.io/nats-server/configuration/securing_nats/jwt#json-web-tokens[user JWT^]\nand a corresponding https://docs.nats.io/running-a-nats-service/configuration/securing_nats/auth_intro/nkey_auth[NKey secret^] when connecting to a server\nwhich is configured to use this authentication scheme.\n\nThe `user_credentials_file` field should point to a file containing both the private key and the JWT and can be\ngenerated with the https://docs.nats.io/nats-tools/nsc[nsc tool^].\n\nAlternatively, the `user_jwt` field can contain a plain text JWT and the `user_nkey_seed`can contain\nthe plain text NKey Seed.\n\nhttps://docs.nats.io/using-nats/developer/connecting/creds[More details^]."
      },
      {
        "name": "noop",
        "type": "caches",
        "status": "stable",
        "version": "4.27.0",
        "description": ""
      },
      {
        "name": "redis",
        "type": "caches",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "redpanda",
        "type": "caches",
        "status": "beta",
        "version": "",
        "description": "\nA cache that stores data in a Kafka topic.\n\nThis cache is useful for data that is written frequently and queried infreqently.\nReads of the cache require reading the entire topic partition, so if there is a need for frequent reads, it's recommended to put an in memory caching layer infront of this cache.\n\nTopics that are used as caches should be compacted so that reads are less expensive when they rescan the topic, as only the latest value is needed.\n\nThis cache does not support any special TTL mechanism, any TTL should be handled by the Kafka topic itself using data retention policies.\n"
      },
      {
        "name": "ristretto",
        "type": "caches",
        "status": "stable",
        "version": "",
        "description": "This cache is more efficient and appropriate for high-volume use cases than the standard memory cache. However, the add command is non-atomic, and therefore this cache is not suitable for deduplication."
      },
      {
        "name": "sql",
        "type": "caches",
        "status": "experimental",
        "version": "4.26.0",
        "description": "\nEach cache key/value pair will exist as a row within the specified table. Currently only the key and value columns are set, and therefore any other columns present within the target table must allow NULL values if this cache is going to be used for set and add operations.\n\nCache operations are translated into SQL statements as follows:\n\n== Get\n\nAll `get` operations are performed with a traditional `select` statement.\n\n== Delete\n\nAll `delete` operations are performed with a traditional `delete` statement.\n\n== Set\n\nThe `set` operation is performed with a traditional `insert` statement.\n\nThis will behave as an `add` operation by default, and so ideally needs to be adapted in order to provide updates instead of failing on collision\ts. Since different SQL engines implement upserts differently it is necessary to specify a `set_suffix` that modifies an `insert` statement in order to perform updates on conflict.\n\n== Add\n\nThe `add` operation is performed with a traditional `insert` statement.\n"
      },
      {
        "name": "ttlru",
        "type": "caches",
        "status": "stable",
        "version": "",
        "description": "The cache ttlru provides a simple, goroutine safe, cache with a fixed number of entries. Each entry has a per-cache defined TTL.\n\nThis TTL is reset on both modification and access of the value. As a result, if the cache is full, and no items have expired, when adding a new item, the item with the soonest expiration will be evicted.\n\nIt uses the package https://github.com/hashicorp/golang-lru/v2/expirable[`expirable`^]\n\nThe field init_values can be used to pre-populate the memory cache with any number of key/value pairs:\n\n```yaml\ncache_resources:\n  - label: foocache\n    ttlru:\n      default_ttl: '5m'\n      cap: 1024\n      init_values:\n        foo: bar\n```\n\nThese values can be overridden during execution."
      },
      {
        "name": "amqp_0_9",
        "type": "inputs",
        "status": "stable",
        "version": "",
        "description": "\nTLS is automatic when connecting to an `amqps` URL, but custom settings can be enabled in the `tls` section.\n\n== Metadata\n\nThis input adds the following metadata fields to each message:\n\n- amqp_content_type\n- amqp_content_encoding\n- amqp_delivery_mode\n- amqp_priority\n- amqp_correlation_id\n- amqp_reply_to\n- amqp_expiration\n- amqp_message_id\n- amqp_timestamp\n- amqp_type\n- amqp_user_id\n- amqp_app_id\n- amqp_consumer_tag\n- amqp_delivery_tag\n- amqp_redelivered\n- amqp_exchange\n- amqp_routing_key\n- All existing message headers, including nested headers prefixed with the key of their respective parent.\n\nYou can access these metadata fields using xref:configuration:interpolation.adoc#bloblang-queries[function interpolations]."
      },
      {
        "name": "amqp_1",
        "type": "inputs",
        "status": "stable",
        "version": "",
        "description": "== Metadata\n\nThis input adds the following metadata fields to each message:\n\n```text\n- amqp_content_type\n- amqp_content_encoding\n- amqp_creation_time\n- All string typed message annotations\n```\n\nYou can access these metadata fields using xref:configuration:interpolation.adoc#bloblang-queries[function interpolation].\n\nBy setting `read_header` to `true`, additional message header properties will be added to each message:\n\n```text\n- amqp_durable\n- amqp_priority\n- amqp_ttl\n- amqp_first_acquirer\n- amqp_delivery_count\n```\n\n== Performance\n\nThis input benefits from receiving multiple messages in flight in parallel for improved performance.\nYou can tune the max number of in flight messages with the field `credit`.\n"
      },
      {
        "name": "aws_kinesis",
        "type": "inputs",
        "status": "stable",
        "version": "3.36.0",
        "description": "\nConsumes messages from one or more Kinesis streams either by automatically balancing shards across other instances of this input, or by consuming shards listed explicitly. The latest message sequence consumed by this input is stored within a <<table-schema,DynamoDB table>>, which allows it to resume at the correct sequence of the shard during restarts. This table is also used for coordination across distributed inputs when shard balancing.\n\nRedpanda Connect will not store a consumed sequence unless it is acknowledged at the output level, which ensures at-least-once delivery guarantees.\n\n== Ordering\n\nBy default messages of a shard can be processed in parallel, up to a limit determined by the field `checkpoint_limit`. However, if strict ordered processing is required then this value must be set to 1 in order to process shard messages in lock-step. When doing so it is recommended that you perform batching at this component for performance as it will not be possible to batch lock-stepped messages at the output level.\n\n== Table schema\n\nIt's possible to configure Redpanda Connect to create the DynamoDB table required for coordination if it does not already exist. However, if you wish to create this yourself (recommended) then create a table with a string HASH key `StreamID` and a string RANGE key `ShardID`.\n\n== Batching\n\nUse the `batching` fields to configure an optional xref:configuration:batching.adoc#batch-policy[batching policy]. Each stream shard will be batched separately in order to ensure that acknowledgements aren't contaminated.\n"
      },
      {
        "name": "aws_s3",
        "type": "inputs",
        "status": "stable",
        "version": "",
        "description": "\n== Stream objects on upload with SQS\n\nA common pattern for consuming S3 objects is to emit upload notification events from the bucket either directly to an SQS queue, or to an SNS topic that is consumed by an SQS queue, and then have your consumer listen for events which prompt it to download the newly uploaded objects. More information about this pattern and how to set it up can be found at in the https://docs.aws.amazon.com/AmazonS3/latest/dev/ways-to-add-notification-config-to-bucket.html[Amazon S3 docs].\n\nRedpanda Connect is able to follow this pattern when you configure an `sqs.url`, where it consumes events from SQS and only downloads object keys received within those events. In order for this to work Redpanda Connect needs to know where within the event the key and bucket names can be found, specified as xref:configuration:field_paths.adoc[dot paths] with the fields `sqs.key_path` and `sqs.bucket_path`. The default values for these fields should already be correct when following the guide above.\n\nIf your notification events are being routed to SQS via an SNS topic then the events will be enveloped by SNS, in which case you also need to specify the field `sqs.envelope_path`, which in the case of SNS to SQS will usually be `Message`.\n\nWhen using SQS please make sure you have sensible values for `sqs.max_messages` and also the visibility timeout of the queue itself. When Redpanda Connect consumes an S3 object the SQS message that triggered it is not deleted until the S3 object has been sent onwards. This ensures at-least-once crash resiliency, but also means that if the S3 object takes longer to process than the visibility timeout of your queue then the same objects might be processed multiple times.\n\n== Download large files\n\nWhen downloading large files it's often necessary to process it in streamed parts in order to avoid loading the entire file in memory at a given time. In order to do this a <<scanner, `scanner`>> can be specified that determines how to break the input into smaller individual messages.\n\n== Credentials\n\nBy default Redpanda Connect will use a shared credentials file when connecting to AWS services. It's also possible to set them explicitly at the component level, allowing you to transfer data across accounts. You can find out more  in xref:guides:cloud/aws.adoc[].\n\n== Metadata\n\nThis input adds the following metadata fields to each message:\n\n- s3_key\n- s3_bucket\n- s3_last_modified_unix\n- s3_last_modified (RFC3339)\n- s3_content_type\n- s3_content_encoding\n- s3_version_id\n- All user defined metadata\n\nYou can access these metadata fields using xref:configuration:interpolation.adoc#bloblang-queries[function interpolation]. Note that user defined metadata is case insensitive within AWS, and it is likely that the keys will be received in a capitalized form, if you wish to make them consistent you can map all metadata keys to lower or uppercase using a Bloblang mapping such as `meta = meta().map_each_key(key -> key.lowercase())`."
      },
      {
        "name": "aws_sqs",
        "type": "inputs",
        "status": "stable",
        "version": "",
        "description": "\n== Credentials\n\nBy default Redpanda Connect will use a shared credentials file when connecting to AWS\nservices. It's also possible to set them explicitly at the component level,\nallowing you to transfer data across accounts. You can find out more in\nxref:guides:cloud/aws.adoc[].\n\n== Metadata\n\nThis input adds the following metadata fields to each message:\n\n- sqs_message_id\n- sqs_receipt_handle\n- sqs_approximate_receive_count\n- All message attributes\n\nYou can access these metadata fields using\nxref:configuration:interpolation.adoc#bloblang-queries[function interpolation]."
      },
      {
        "name": "azure_blob_storage",
        "type": "inputs",
        "status": "beta",
        "version": "3.36.0",
        "description": "\nSupports multiple authentication methods but only one of the following is required:\n\n- `storage_connection_string`\n- `storage_account` and `storage_access_key`\n- `storage_account` and `storage_sas_token`\n- `storage_account` to access via https://pkg.go.dev/github.com/Azure/azure-sdk-for-go/sdk/azidentity#DefaultAzureCredential[DefaultAzureCredential^]\n\nIf multiple are set then the `storage_connection_string` is given priority.\n\nIf the `storage_connection_string` does not contain the `AccountName` parameter, please specify it in the\n`storage_account` field.\n\n== Download large files\n\nWhen downloading large files it's often necessary to process it in streamed parts in order to avoid loading the entire file in memory at a given time. In order to do this a <<scanner, `scanner`>> can be specified that determines how to break the input into smaller individual messages.\n\n== Stream new files\n\nBy default this input will consume all files found within the target container and will then gracefully terminate. This is referred to as a \"batch\" mode of operation. However, it's possible to instead configure a container as https://learn.microsoft.com/en-gb/azure/event-grid/event-schema-blob-storage[an Event Grid source^] and then use this as a <<targetsinput, `targets_input`>>, in which case new files are consumed as they're uploaded and Redpanda Connect will continue listening for and downloading files as they arrive. This is referred to as a \"streamed\" mode of operation.\n\n== Metadata\n\nThis input adds the following metadata fields to each message:\n\n- blob_storage_key\n- blob_storage_container\n- blob_storage_last_modified\n- blob_storage_last_modified_unix\n- blob_storage_content_type\n- blob_storage_content_encoding\n- All user defined metadata\n\nYou can access these metadata fields using xref:configuration:interpolation.adoc#bloblang-queries[function interpolation]."
      },
      {
        "name": "azure_cosmosdb",
        "type": "inputs",
        "status": "experimental",
        "version": "v4.25.0",
        "description": "\n== Cross-partition queries\n\nCross-partition queries are currently not supported by the underlying driver. For every query, the PartitionKey values must be known in advance and specified in the config. https://github.com/Azure/azure-sdk-for-go/issues/18578#issuecomment-1222510989[See details^].\n\n\n== Credentials\n\nYou can use one of the following authentication mechanisms:\n\n- Set the `endpoint` field and the `account_key` field\n- Set only the `endpoint` field to use https://pkg.go.dev/github.com/Azure/azure-sdk-for-go/sdk/azidentity#DefaultAzureCredential[DefaultAzureCredential^]\n- Set the `connection_string` field\n\n\n== Metadata\n\nThis component adds the following metadata fields to each message:\n```\n- activity_id\n- request_charge\n```\n\nYou can access these metadata fields using xref:configuration:interpolation.adoc#bloblang-queries[function interpolation].\n"
      },
      {
        "name": "azure_queue_storage",
        "type": "inputs",
        "status": "beta",
        "version": "3.42.0",
        "description": "\nThis input adds the following metadata fields to each message:\n\n```\n- queue_storage_insertion_time\n- queue_storage_queue_name\n- queue_storage_message_lag (if 'track_properties' set to true)\n- All user defined queue metadata\n```\n\nOnly one authentication method is required, `storage_connection_string` or `storage_account` and `storage_access_key`. If both are set then the `storage_connection_string` is given priority."
      },
      {
        "name": "azure_table_storage",
        "type": "inputs",
        "status": "beta",
        "version": "4.10.0",
        "description": "\nQueries an Azure Storage Account Table, optionally with multiple filters.\n== Metadata\nThis input adds the following metadata fields to each message:\n\n- table_storage_name\n- row_num\n\nYou can access these metadata fields using xref:configuration:interpolation.adoc#bloblang-queries[function interpolation]."
      },
      {
        "name": "batched",
        "type": "inputs",
        "status": "stable",
        "version": "4.11.0",
        "description": "Batching at the input level is sometimes useful for processing across micro-batches, and can also sometimes be a useful performance trick. However, most inputs are fine without it so unless you have a specific plan for batching this component is not worth using."
      },
      {
        "name": "beanstalkd",
        "type": "inputs",
        "status": "experimental",
        "version": "4.7.0",
        "description": ""
      },
      {
        "name": "broker",
        "type": "inputs",
        "status": "stable",
        "version": "",
        "description": "\nA broker type is configured with its own list of input configurations and a field to specify how many copies of the list of inputs should be created.\n\nAdding more input types allows you to combine streams from multiple sources into one. For example, reading from both RabbitMQ and Kafka:\n\n```yaml\ninput:\n  broker:\n    copies: 1\n    inputs:\n      - amqp_0_9:\n          urls:\n            - amqp://guest:guest@localhost:5672/\n          consumer_tag: benthos-consumer\n          queue: benthos-queue\n\n        # Optional list of input specific processing steps\n        processors:\n          - mapping: |\n              root.message = this\n              root.meta.link_count = this.links.length()\n              root.user.age = this.user.age.number()\n\n      - kafka:\n          addresses:\n            - localhost:9092\n          client_id: benthos_kafka_input\n          consumer_group: benthos_consumer_group\n          topics: [ benthos_stream:0 ]\n```\n\nIf the number of copies is greater than zero the list will be copied that number of times. For example, if your inputs were of type foo and bar, with 'copies' set to '2', you would end up with two 'foo' inputs and two 'bar' inputs.\n\n== Batching\n\nIt's possible to configure a xref:configuration:batching.adoc#batch-policy[batch policy] with a broker using the `batching` fields. When doing this the feeds from all child inputs are combined. Some inputs do not support broker based batching and specify this in their documentation.\n\n== Processors\n\nIt is possible to configure xref:components:processors/about.adoc[processors] at the broker level, where they will be applied to _all_ child inputs, as well as on the individual child inputs. If you have processors at both the broker level _and_ on child inputs then the broker processors will be applied _after_ the child nodes processors."
      },
      {
        "name": "cassandra",
        "type": "inputs",
        "status": "experimental",
        "version": "",
        "description": ""
      },
      {
        "name": "cockroachdb_changefeed",
        "type": "inputs",
        "status": "experimental",
        "version": "",
        "description": "This input will continue to listen to the changefeed until shutdown. A backfill of the full current state of the table will be delivered upon each run unless a cache is configured for storing cursor timestamps, as this is how Redpanda Connect keeps track as to which changes have been successfully delivered.\n\nNote: You must have `SET CLUSTER SETTING kv.rangefeed.enabled = true;` on your CRDB cluster, for more information refer to https://www.cockroachlabs.com/docs/stable/changefeed-examples?filters=core[the official CockroachDB documentation^]."
      },
      {
        "name": "csv",
        "type": "inputs",
        "status": "stable",
        "version": "",
        "description": "\nThis input offers more control over CSV parsing than the xref:components:inputs/file.adoc[`file` input].\n\nWhen parsing with a header row each line of the file will be consumed as a structured object, where the key names are determined from the header now. For example, the following CSV file:\n\n```csv\nfoo,bar,baz\nfirst foo,first bar,first baz\nsecond foo,second bar,second baz\n```\n\nWould produce the following messages:\n\n```json\n{\"foo\":\"first foo\",\"bar\":\"first bar\",\"baz\":\"first baz\"}\n{\"foo\":\"second foo\",\"bar\":\"second bar\",\"baz\":\"second baz\"}\n```\n\nIf, however, the field `parse_header_row` is set to `false` then arrays are produced instead, like follows:\n\n```json\n[\"first foo\",\"first bar\",\"first baz\"]\n[\"second foo\",\"second bar\",\"second baz\"]\n```\n\n== Metadata\n\nThis input adds the following metadata fields to each message:\n\n```text\n- header\n- path\n- mod_time_unix\n- mod_time (RFC3339)\n```\n\nYou can access these metadata fields using xref:configuration:interpolation.adoc#bloblang-queries[function interpolation].\n\nNote: The `header` field is only set when `parse_header_row` is `true`.\n\n=== Output CSV column order\n\nWhen xref:guides:bloblang/advanced.adoc#creating-csv[creating CSV] from Redpanda Connect messages, the columns must be sorted lexicographically to make the output deterministic. Alternatively, when using the `csv` input, one can leverage the `header` metadata field to retrieve the column order:\n\n```yaml\ninput:\n  csv:\n    paths:\n      - ./foo.csv\n      - ./bar.csv\n    parse_header_row: true\n\n  processors:\n    - mapping: |\n        map escape_csv {\n          root = if this.re_match(\"[\\\"\\n,]+\") {\n            \"\\\"\" + this.replace_all(\"\\\"\", \"\\\"\\\"\") + \"\\\"\"\n          } else {\n            this\n          }\n        }\n\n        let header = if count(@path) == 1 {\n          @header.map_each(c -> c.apply(\"escape_csv\")).join(\",\") + \"\\n\"\n        } else { \"\" }\n\n        root = $header + @header.map_each(c -> this.get(c).string().apply(\"escape_csv\")).join(\",\")\n\noutput:\n  file:\n    path: ./output/${! @path.filepath_split().index(-1) }\n```\n"
      },
      {
        "name": "discord",
        "type": "inputs",
        "status": "experimental",
        "version": "",
        "description": "This input works by authenticating as a bot using token based authentication. The ID of the newest message consumed and acked is stored in a cache in order to perform a backfill of unread messages each time the input is initialised. Ideally this cache should be persisted across restarts."
      },
      {
        "name": "dynamic",
        "type": "inputs",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "file",
        "type": "inputs",
        "status": "stable",
        "version": "",
        "description": "\n== Metadata\n\nThis input adds the following metadata fields to each message:\n\n```text\n- path\n- mod_time_unix\n- mod_time (RFC3339)\n```\n\nYou can access these metadata fields using\nxref:configuration:interpolation.adoc#bloblang-queries[function interpolation]."
      },
      {
        "name": "gateway",
        "type": "inputs",
        "status": "stable",
        "version": "",
        "description": "\nThe field `rate_limit` allows you to specify an optional xref:components:rate_limits/about.adoc[`rate_limit` resource], which will be applied to each HTTP request made and each websocket payload received.\n\nWhen the rate limit is breached HTTP requests will have a 429 response returned with a Retry-After header.\n\n== Responses\n\nIt's possible to return a response for each message received using xref:guides:sync_responses.adoc[synchronous responses]. When doing so you can customize headers with the `sync_response` field `headers`, which can also use xref:configuration:interpolation.adoc#bloblang-queries[function interpolation] in the value based on the response message contents.\n\n== Metadata\n\nThis input adds the following metadata fields to each message:\n\n```text\n- http_server_user_agent\n- http_server_request_path\n- http_server_verb\n- http_server_remote_ip\n- All headers (only first values are taken)\n- All query parameters\n- All path parameters\n- All cookies\n```\n\nYou can access these metadata fields using xref:configuration:interpolation.adoc#bloblang-queries[function interpolation]."
      },
      {
        "name": "gcp_bigquery_select",
        "type": "inputs",
        "status": "beta",
        "version": "3.63.0",
        "description": "Once the rows from the query are exhausted, this input shuts down, allowing the pipeline to gracefully terminate (or the next input in a xref:components:inputs/sequence.adoc[sequence] to execute)."
      },
      {
        "name": "gcp_cloud_storage",
        "type": "inputs",
        "status": "beta",
        "version": "3.43.0",
        "description": "\n== Metadata\n\nThis input adds the following metadata fields to each message:\n\n```\n- gcs_key\n- gcs_bucket\n- gcs_last_modified\n- gcs_last_modified_unix\n- gcs_content_type\n- gcs_content_encoding\n- All user defined metadata\n```\n\nYou can access these metadata fields using xref:configuration:interpolation.adoc#bloblang-queries[function interpolation].\n\n=== Credentials\n\nBy default Redpanda Connect will use a shared credentials file when connecting to GCP services. You can find out more in xref:guides:cloud/gcp.adoc[]."
      },
      {
        "name": "gcp_pubsub",
        "type": "inputs",
        "status": "stable",
        "version": "",
        "description": "\nFor information on how to set up credentials see https://cloud.google.com/docs/authentication/production[this guide^].\n\n== Metadata\n\nThis input adds the following metadata fields to each message:\n\n- gcp_pubsub_publish_time_unix - The time at which the message was published to the topic.\n- gcp_pubsub_delivery_attempt - When dead lettering is enabled, this is set to the number of times PubSub has attempted to deliver a message.\n- gcp_pubsub_message_id - The unique identifier of the message.\n- gcp_pubsub_ordering_key - The ordering key of the message.\n- All message attributes\n\nYou can access these metadata fields using xref:configuration:interpolation.adoc#bloblang-queries[function interpolation].\n"
      },
      {
        "name": "gcp_spanner_cdc",
        "type": "inputs",
        "status": "beta",
        "version": "4.56.0",
        "description": "\nConsumes change records from a Google Cloud Spanner change stream. This input allows\nyou to track and process database changes in real-time, making it useful for data\nreplication, event-driven architectures, and maintaining derived data stores.\n\nThe input reads from a specified change stream within a Spanner database and converts\neach change record into a message. The message payload contains the change records in\nJSON format, and metadata is added with details about the Spanner instance, database,\nand stream.\n\nChange streams provide a way to track mutations to your Spanner database tables. For\nmore information about Spanner change streams, refer to the Google Cloud documentation:\nhttps://cloud.google.com/spanner/docs/change-streams\n"
      },
      {
        "name": "generate",
        "type": "inputs",
        "status": "stable",
        "version": "3.40.0",
        "description": ""
      },
      {
        "name": "git",
        "type": "inputs",
        "status": "beta",
        "version": "4.51.0",
        "description": "\nThe git input clones the specified repository (or pulls updates if already cloned) and reads \nthe content of the specified file. It periodically polls the repository for new commits and emits \na message when changes are detected.\n\n== Metadata\n\nThis input adds the following metadata fields to each message:\n\n- git_file_path\n- git_file_size\n- git_file_mode\n- git_file_modified\n- git_commit\n- git_mime_type\n- git_is_binary\n- git_encoding (present if the file was base64 encoded)\n- git_deleted (only present if the file was deleted)\n\nYou can access these metadata fields using function interpolation."
      },
      {
        "name": "hdfs",
        "type": "inputs",
        "status": "stable",
        "version": "",
        "description": "\n== Metadata\n\nThis input adds the following metadata fields to each message:\n\n- hdfs_name\n- hdfs_path\n\nYou can access these metadata fields using\nxref:configuration:interpolation.adoc#bloblang-queries[function interpolation]."
      },
      {
        "name": "http_client",
        "type": "inputs",
        "status": "stable",
        "version": "",
        "description": "\nThe URL and header values of this type can be dynamically set using function interpolations described xref:configuration:interpolation.adoc#bloblang-queries[here].\n\n== Streaming\n\nIf you enable streaming then Redpanda Connect will consume the body of the response as a continuous stream of data, breaking messages out following a chosen scanner. This allows you to consume APIs that provide long lived streamed data feeds (such as Twitter).\n\n== Pagination\n\nThis input supports interpolation functions in the `url` and `headers` fields where data from the previous successfully consumed message (if there was one) can be referenced. This can be used in order to support basic levels of pagination. However, in cases where pagination depends on logic it is recommended that you use an xref:components:processors/http.adoc[`http` processor] instead, often combined with a xref:components:inputs/generate.adoc[`generate` input] in order to schedule the processor."
      },
      {
        "name": "http_server",
        "type": "inputs",
        "status": "stable",
        "version": "",
        "description": "\nIf the `address` config field is left blank the xref:components:http/about.adoc[service-wide HTTP server] will be used.\n\nThe field `rate_limit` allows you to specify an optional xref:components:rate_limits/about.adoc[`rate_limit` resource], which will be applied to each HTTP request made and each websocket payload received.\n\nWhen the rate limit is breached HTTP requests will have a 429 response returned with a Retry-After header. Websocket payloads will be dropped and an optional response payload will be sent as per `ws_rate_limit_message`.\n\n== Responses\n\nIt's possible to return a response for each message received using xref:guides:sync_responses.adoc[synchronous responses]. When doing so you can customize headers with the `sync_response` field `headers`, which can also use xref:configuration:interpolation.adoc#bloblang-queries[function interpolation] in the value based on the response message contents.\n\n== Endpoints\n\nThe following fields specify endpoints that are registered for sending messages, and support path parameters of the form `/\\{foo}`, which are added to ingested messages as metadata. A path ending in `/` will match against all extensions of that path:\n\n=== `path` (defaults to `/post`)\n\nThis endpoint expects POST requests where the entire request body is consumed as a single message.\n\nIf the request contains a multipart `content-type` header as per https://www.w3.org/Protocols/rfc1341/7_2_Multipart.html[RFC1341^] then the multiple parts are consumed as a batch of messages, where each body part is a message of the batch.\n\n=== `ws_path` (defaults to `/post/ws`)\n\nCreates a websocket connection, where payloads received on the socket are passed through the pipeline as a batch of one message.\n\n\n[CAUTION]\n.Endpoint caveats\n====\nComponents within a Redpanda Connect config will register their respective endpoints in a non-deterministic order. This means that establishing precedence of endpoints that are registered via multiple `http_server` inputs or outputs (either within brokers or from cohabiting streams) is not possible in a predictable way.\n\nThis ambiguity makes it difficult to ensure that paths which are both a subset of a path registered by a separate component, and end in a slash (`/`) and will therefore match against all extensions of that path, do not prevent the more specific path from matching against requests.\n\nIt is therefore recommended that you ensure paths of separate components do not collide unless they are explicitly non-competing.\n\nFor example, if you were to deploy two separate `http_server` inputs, one with a path `/foo/` and the other with a path `/foo/bar`, it would not be possible to ensure that the path `/foo/` does not swallow requests made to `/foo/bar`.\n====\n\nYou may specify an optional `ws_welcome_message`, which is a static payload to be sent to all clients once a websocket connection is first established.\n\nIt's also possible to specify a `ws_rate_limit_message`, which is a static payload to be sent to clients that have triggered the servers rate limit.\n\n== Metadata\n\nThis input adds the following metadata fields to each message:\n\n```text\n- http_server_user_agent\n- http_server_request_path\n- http_server_verb\n- http_server_remote_ip\n- All headers (only first values are taken)\n- All query parameters\n- All path parameters\n- All cookies\n```\n\nIf HTTPS is enabled, the following fields are added as well:\n```text\n- http_server_tls_version\n- http_server_tls_subject\n- http_server_tls_cipher_suite\n```\n\nYou can access these metadata fields using xref:configuration:interpolation.adoc#bloblang-queries[function interpolation]."
      },
      {
        "name": "inproc",
        "type": "inputs",
        "status": "stable",
        "version": "",
        "description": "\nDirectly connect to an output within a Redpanda Connect process by referencing it by a chosen ID. This allows you to hook up isolated streams whilst running Redpanda Connect in xref:guides:streams_mode/about.adoc[streams mode], it is NOT recommended that you connect the inputs of a stream with an output of the same stream, as feedback loops can lead to deadlocks in your message flow.\n\nIt is possible to connect multiple inputs to the same inproc ID, resulting in messages dispatching in a round-robin fashion to connected inputs. However, only one output can assume an inproc ID, and will replace existing outputs if a collision occurs."
      },
      {
        "name": "kafka",
        "type": "inputs",
        "status": "deprecated",
        "version": "",
        "description": "\nOffsets are managed within Kafka under the specified consumer group, and partitions for each topic are automatically balanced across members of the consumer group.\n\nThe Kafka input allows parallel processing of messages from different topic partitions, and messages of the same topic partition are processed with a maximum parallelism determined by the field <<checkpoint_limit,`checkpoint_limit`>>.\n\nIn order to enforce ordered processing of partition messages set the <checkpoint_limit,`checkpoint_limit`>> to `1` and this will force partitions to be processed in lock-step, where a message will only be processed once the prior message is delivered.\n\nBatching messages before processing can be enabled using the <<batching,`batching`>> field, and this batching is performed per-partition such that messages of a batch will always originate from the same partition. This batching mechanism is capable of creating batches of greater size than the <<checkpoint_limit,`checkpoint_limit`>>, in which case the next batch will only be created upon delivery of the current one.\n\n== Metadata\n\nThis input adds the following metadata fields to each message:\n\n- kafka_key\n- kafka_topic\n- kafka_partition\n- kafka_offset\n- kafka_lag\n- kafka_timestamp_ms\n- kafka_timestamp_unix\n- kafka_tombstone_message\n- All existing message headers (version 0.11+)\n\nThe field `kafka_lag` is the calculated difference between the high water mark offset of the partition at the time of ingestion and the current message offset.\n\nYou can access these metadata fields using xref:configuration:interpolation.adoc#bloblang-queries[function interpolation].\n\n== Ordering\n\nBy default messages of a topic partition can be processed in parallel, up to a limit determined by the field `checkpoint_limit`. However, if strict ordered processing is required then this value must be set to 1 in order to process shard messages in lock-step. When doing so it is recommended that you perform batching at this component for performance as it will not be possible to batch lock-stepped messages at the output level.\n\n== Troubleshooting\n\nIf you're seeing issues writing to or reading from Kafka with this component then it's worth trying out the newer xref:components:inputs/kafka_franz.adoc[`kafka_franz` input].\n\n- I'm seeing logs that report `Failed to connect to kafka: kafka: client has run out of available brokers to talk to (Is your cluster reachable?)`, but the brokers are definitely reachable.\n\nUnfortunately this error message will appear for a wide range of connection problems even when the broker endpoint can be reached. Double check your authentication configuration and also ensure that you have <<tlsenabled, enabled TLS>> if applicable."
      },
      {
        "name": "kafka_franz",
        "type": "inputs",
        "status": "deprecated",
        "version": "3.61.0",
        "description": "\nWhen a consumer group is specified this input consumes one or more topics where partitions will automatically balance across any other connected clients with the same consumer group. When a consumer group is not specified topics can either be consumed in their entirety or with explicit partitions.\n\nThis input often out-performs the traditional `kafka` input as well as providing more useful logs and error messages.\n\n== Metadata\n\nThis input adds the following metadata fields to each message:\n\n```text\n- kafka_key\n- kafka_topic\n- kafka_partition\n- kafka_offset\n- kafka_lag\n- kafka_timestamp_ms\n- kafka_timestamp_unix\n- kafka_tombstone_message\n- All record headers\n```\n"
      },
      {
        "name": "legacy_redpanda_migrator",
        "type": "inputs",
        "status": "deprecated",
        "version": "4.37.0",
        "description": "\nReads a batch of messages from a Kafka broker and waits for the output to acknowledge the writes before updating the Kafka consumer group offset.\n\nThis input should be used in combination with a `legacy_redpanda_migrator` output.\n\nWhen a consumer group is specified this input consumes one or more topics where partitions will automatically balance across any other connected clients with the same consumer group. When a consumer group is not specified topics can either be consumed in their entirety or with explicit partitions.\n\nIt provides the same delivery guarantees and ordering semantics as the `redpanda` input.\n\n== Metrics\n\nEmits a `redpanda_lag` metric with `topic` and `partition` labels for each consumed topic.\n\n== Metadata\n\nThis input adds the following metadata fields to each message:\n\n```text\n- kafka_key\n- kafka_topic\n- kafka_partition\n- kafka_offset\n- kafka_lag\n- kafka_timestamp_ms\n- kafka_timestamp_unix\n- kafka_tombstone_message\n- All record headers\n```\n"
      },
      {
        "name": "legacy_redpanda_migrator_offsets",
        "type": "inputs",
        "status": "deprecated",
        "version": "4.45.0",
        "description": "\nThis input reads consumer group updates via the `OffsetFetch` API and should be used in combination with the `legacy_redpanda_migrator_offsets` output.\n\n== Metadata\n\nThis input adds the following metadata fields to each message:\n\n```text\n- kafka_offset_topic\n- kafka_offset_group\n- kafka_offset_partition\n- kafka_offset_commit_timestamp\n- kafka_offset_metadata\n- kafka_is_high_watermark\n```\n"
      },
      {
        "name": "microsoft_sql_server_cdc",
        "type": "inputs",
        "status": "beta",
        "version": "0.0.1",
        "description": "Streams changes from a Microsoft SQL Server database for Change Data Capture (CDC).\nAdditionally, if `stream_snapshot` is set to true, then the existing data in the database is also streamed too.\n\n== Metadata\n\nThis input adds the following metadata fields to each message:\n- schema (Schema of the table that the message originated from)\n- table (Name of the table that the message originated from)\n- operation (Type of operation that generated the message: \"read\", \"delete\", \"insert\", or \"update_before\" and \"update_after\". \"read\" is from messages that are read in the initial snapshot phase.)\n- lsn (the Log Sequence Number in Microsoft SQL Server)\n\n== Permissions\n\nWhen using the default Microsoft SQL Server based cache, the Connect user requires permission to create tables and stored procedures, and the rpcn  schema must already exist. Refer to `checkpoint_cache_table_name` for more information.\n\t\t"
      },
      {
        "name": "mongodb",
        "type": "inputs",
        "status": "experimental",
        "version": "3.64.0",
        "description": "Once the documents from the query are exhausted, this input shuts down, allowing the pipeline to gracefully terminate (or the next input in a xref:components:inputs/sequence.adoc[sequence] to execute)."
      },
      {
        "name": "mongodb_cdc",
        "type": "inputs",
        "status": "experimental",
        "version": "",
        "description": "Read from a MongoDB replica set using https://www.mongodb.com/docs/manual/changeStreams/[^Change Streams]. It's only possible to watch for changes when using a sharded MongoDB or a MongoDB cluster running as a replica set.\n\nBy default MongoDB does not propagate changes in all cases. In order to capture all changes (including deletes) in a MongoDB cluster one needs to enable pre and post image saving and the collection needs to also enable saving these pre and post images. For more information see https://www.mongodb.com/docs/manual/changeStreams/#change-streams-with-document-pre--and-post-images[^MongoDB documentation].\n\n== Metadata\n\nEach message omitted by this plugin has the following metadata:\n\n- operation: either \"create\", \"replace\", \"delete\" or \"update\" for changes streamed. Documents from the initial snapshot have the operation set to \"read\".\n- collection: the collection the document was written to.\n- operation_time: the oplog time for when this operation occurred.\n    "
      },
      {
        "name": "mqtt",
        "type": "inputs",
        "status": "stable",
        "version": "",
        "description": "\n== Metadata\n\nThis input adds the following metadata fields to each message:\n\n- mqtt_duplicate\n- mqtt_qos\n- mqtt_retained\n- mqtt_topic\n- mqtt_message_id\n\nYou can access these metadata fields using xref:configuration:interpolation.adoc#bloblang-queries[function interpolation]."
      },
      {
        "name": "mysql_cdc",
        "type": "inputs",
        "status": "beta",
        "version": "4.45.0",
        "description": "\n== Metadata\n\nThis input adds the following metadata fields to each message:\n\n- operation\n- table\n- binlog_position\n"
      },
      {
        "name": "nanomsg",
        "type": "inputs",
        "status": "stable",
        "version": "",
        "description": "Currently only PULL and SUB sockets are supported."
      },
      {
        "name": "nats",
        "type": "inputs",
        "status": "stable",
        "version": "",
        "description": "\n== Metadata\n\nThis input adds the following metadata fields to each message:\n\n```text\n- nats_subject\n- nats_reply_subject\n- All message headers (when supported by the connection)\n```\n\nYou can access these metadata fields using xref:configuration:interpolation.adoc#bloblang-queries[function interpolation].\n\n== Connection name\n\nWhen monitoring and managing a production NATS system, it is often useful to\nknow which connection a message was send/received from. This can be achieved by\nsetting the connection name option when creating a NATS connection.\n\nRedpanda Connect will automatically set the connection name based off the label of the given\nNATS component, so that monitoring tools between NATS and Redpanda Connect can stay in sync.\n\n\n== Authentication\n\nThere are several components within Redpanda Connect which uses NATS services. You will find that each of these components\nsupport optional advanced authentication parameters for https://docs.nats.io/nats-server/configuration/securing_nats/auth_intro/nkey_auth[NKeys^]\nand https://docs.nats.io/using-nats/developer/connecting/creds[User Credentials^].\n\nSee an https://docs.nats.io/running-a-nats-service/nats_admin/security/jwt[in-depth tutorial^].\n\n=== NKey file\n\nThe NATS server can use these NKeys in several ways for authentication. The simplest is for the server to be configured\nwith a list of known public keys and for the clients to respond to the challenge by signing it with its private NKey\nconfigured in the `nkey_file` or `nkey` field.\n\nhttps://docs.nats.io/running-a-nats-service/configuration/securing_nats/auth_intro/nkey_auth[More details^].\n\n=== User credentials\n\nNATS server supports decentralized authentication based on JSON Web Tokens (JWT). Clients need an https://docs.nats.io/nats-server/configuration/securing_nats/jwt#json-web-tokens[user JWT^]\nand a corresponding https://docs.nats.io/running-a-nats-service/configuration/securing_nats/auth_intro/nkey_auth[NKey secret^] when connecting to a server\nwhich is configured to use this authentication scheme.\n\nThe `user_credentials_file` field should point to a file containing both the private key and the JWT and can be\ngenerated with the https://docs.nats.io/nats-tools/nsc[nsc tool^].\n\nAlternatively, the `user_jwt` field can contain a plain text JWT and the `user_nkey_seed`can contain\nthe plain text NKey Seed.\n\nhttps://docs.nats.io/using-nats/developer/connecting/creds[More details^]."
      },
      {
        "name": "nats_jetstream",
        "type": "inputs",
        "status": "stable",
        "version": "3.46.0",
        "description": "\n== Consume mirrored streams\n\nIn the case where a stream being consumed is mirrored from a different JetStream domain the stream cannot be resolved from the subject name alone, and so the stream name as well as the subject (if applicable) must both be specified.\n\n== Metadata\n\nThis input adds the following metadata fields to each message:\n\n```text\n- nats_subject\n- nats_sequence_stream\n- nats_sequence_consumer\n- nats_num_delivered\n- nats_num_pending\n- nats_domain\n- nats_timestamp_unix_nano\n- nats_consumer\n```\n\nYou can access these metadata fields using\nxref:configuration:interpolation.adoc#bloblang-queries[function interpolation].\n\n== Connection name\n\nWhen monitoring and managing a production NATS system, it is often useful to\nknow which connection a message was send/received from. This can be achieved by\nsetting the connection name option when creating a NATS connection.\n\nRedpanda Connect will automatically set the connection name based off the label of the given\nNATS component, so that monitoring tools between NATS and Redpanda Connect can stay in sync.\n\n\n== Authentication\n\nThere are several components within Redpanda Connect which uses NATS services. You will find that each of these components\nsupport optional advanced authentication parameters for https://docs.nats.io/nats-server/configuration/securing_nats/auth_intro/nkey_auth[NKeys^]\nand https://docs.nats.io/using-nats/developer/connecting/creds[User Credentials^].\n\nSee an https://docs.nats.io/running-a-nats-service/nats_admin/security/jwt[in-depth tutorial^].\n\n=== NKey file\n\nThe NATS server can use these NKeys in several ways for authentication. The simplest is for the server to be configured\nwith a list of known public keys and for the clients to respond to the challenge by signing it with its private NKey\nconfigured in the `nkey_file` or `nkey` field.\n\nhttps://docs.nats.io/running-a-nats-service/configuration/securing_nats/auth_intro/nkey_auth[More details^].\n\n=== User credentials\n\nNATS server supports decentralized authentication based on JSON Web Tokens (JWT). Clients need an https://docs.nats.io/nats-server/configuration/securing_nats/jwt#json-web-tokens[user JWT^]\nand a corresponding https://docs.nats.io/running-a-nats-service/configuration/securing_nats/auth_intro/nkey_auth[NKey secret^] when connecting to a server\nwhich is configured to use this authentication scheme.\n\nThe `user_credentials_file` field should point to a file containing both the private key and the JWT and can be\ngenerated with the https://docs.nats.io/nats-tools/nsc[nsc tool^].\n\nAlternatively, the `user_jwt` field can contain a plain text JWT and the `user_nkey_seed`can contain\nthe plain text NKey Seed.\n\nhttps://docs.nats.io/using-nats/developer/connecting/creds[More details^]."
      },
      {
        "name": "nats_kv",
        "type": "inputs",
        "status": "beta",
        "version": "4.12.0",
        "description": "\n== Metadata\n\nThis input adds the following metadata fields to each message:\n\n``` text\n- nats_kv_key\n- nats_kv_bucket\n- nats_kv_revision\n- nats_kv_delta\n- nats_kv_operation\n- nats_kv_created\n```\n\n== Connection name\n\nWhen monitoring and managing a production NATS system, it is often useful to\nknow which connection a message was send/received from. This can be achieved by\nsetting the connection name option when creating a NATS connection.\n\nRedpanda Connect will automatically set the connection name based off the label of the given\nNATS component, so that monitoring tools between NATS and Redpanda Connect can stay in sync.\n\n\n== Authentication\n\nThere are several components within Redpanda Connect which uses NATS services. You will find that each of these components\nsupport optional advanced authentication parameters for https://docs.nats.io/nats-server/configuration/securing_nats/auth_intro/nkey_auth[NKeys^]\nand https://docs.nats.io/using-nats/developer/connecting/creds[User Credentials^].\n\nSee an https://docs.nats.io/running-a-nats-service/nats_admin/security/jwt[in-depth tutorial^].\n\n=== NKey file\n\nThe NATS server can use these NKeys in several ways for authentication. The simplest is for the server to be configured\nwith a list of known public keys and for the clients to respond to the challenge by signing it with its private NKey\nconfigured in the `nkey_file` or `nkey` field.\n\nhttps://docs.nats.io/running-a-nats-service/configuration/securing_nats/auth_intro/nkey_auth[More details^].\n\n=== User credentials\n\nNATS server supports decentralized authentication based on JSON Web Tokens (JWT). Clients need an https://docs.nats.io/nats-server/configuration/securing_nats/jwt#json-web-tokens[user JWT^]\nand a corresponding https://docs.nats.io/running-a-nats-service/configuration/securing_nats/auth_intro/nkey_auth[NKey secret^] when connecting to a server\nwhich is configured to use this authentication scheme.\n\nThe `user_credentials_file` field should point to a file containing both the private key and the JWT and can be\ngenerated with the https://docs.nats.io/nats-tools/nsc[nsc tool^].\n\nAlternatively, the `user_jwt` field can contain a plain text JWT and the `user_nkey_seed`can contain\nthe plain text NKey Seed.\n\nhttps://docs.nats.io/using-nats/developer/connecting/creds[More details^]."
      },
      {
        "name": "nats_stream",
        "type": "inputs",
        "status": "stable",
        "version": "",
        "description": "\n[CAUTION]\n.Deprecation notice\n====\nThe NATS Streaming Server is being deprecated. Critical bug fixes and security fixes will be applied until June of 2023. NATS-enabled applications requiring persistence should use https://docs.nats.io/nats-concepts/jetstream[JetStream^].\n====\n\nTracking and persisting offsets through a durable name is also optional and works with or without a queue. If a durable name is not provided then subjects are consumed from the most recently published message.\n\nWhen a consumer closes its connection it unsubscribes, when all consumers of a durable queue do this the offsets are deleted. In order to avoid this you can stop the consumers from unsubscribing by setting the field `unsubscribe_on_close` to `false`.\n\n== Metadata\n\nThis input adds the following metadata fields to each message:\n\n- nats_stream_subject\n- nats_stream_sequence\n\nYou can access these metadata fields using xref:configuration:interpolation.adoc#bloblang-queries[function interpolation].\n\n\n\n== Authentication\n\nThere are several components within Redpanda Connect which uses NATS services. You will find that each of these components\nsupport optional advanced authentication parameters for https://docs.nats.io/nats-server/configuration/securing_nats/auth_intro/nkey_auth[NKeys^]\nand https://docs.nats.io/using-nats/developer/connecting/creds[User Credentials^].\n\nSee an https://docs.nats.io/running-a-nats-service/nats_admin/security/jwt[in-depth tutorial^].\n\n=== NKey file\n\nThe NATS server can use these NKeys in several ways for authentication. The simplest is for the server to be configured\nwith a list of known public keys and for the clients to respond to the challenge by signing it with its private NKey\nconfigured in the `nkey_file` or `nkey` field.\n\nhttps://docs.nats.io/running-a-nats-service/configuration/securing_nats/auth_intro/nkey_auth[More details^].\n\n=== User credentials\n\nNATS server supports decentralized authentication based on JSON Web Tokens (JWT). Clients need an https://docs.nats.io/nats-server/configuration/securing_nats/jwt#json-web-tokens[user JWT^]\nand a corresponding https://docs.nats.io/running-a-nats-service/configuration/securing_nats/auth_intro/nkey_auth[NKey secret^] when connecting to a server\nwhich is configured to use this authentication scheme.\n\nThe `user_credentials_file` field should point to a file containing both the private key and the JWT and can be\ngenerated with the https://docs.nats.io/nats-tools/nsc[nsc tool^].\n\nAlternatively, the `user_jwt` field can contain a plain text JWT and the `user_nkey_seed`can contain\nthe plain text NKey Seed.\n\nhttps://docs.nats.io/using-nats/developer/connecting/creds[More details^]."
      },
      {
        "name": "nsq",
        "type": "inputs",
        "status": "stable",
        "version": "",
        "description": "\n== Metadata\n\nThis input adds the following metadata fields to each message:\n\n- nsq_attempts\n- nsq_id\n- nsq_nsqd_address\n- nsq_timestamp\n\nYou can access these metadata fields using xref:configuration:interpolation.adoc#bloblang-queries[function interpolation].\n"
      },
      {
        "name": "ockam_kafka",
        "type": "inputs",
        "status": "experimental",
        "version": "",
        "description": ""
      },
      {
        "name": "parquet",
        "type": "inputs",
        "status": "experimental",
        "version": "4.8.0",
        "description": "\nThis input uses https://github.com/parquet-go/parquet-go[https://github.com/parquet-go/parquet-go^], which is itself experimental. Therefore changes could be made into how this processor functions outside of major version releases.\n\nBy default any BYTE_ARRAY or FIXED_LEN_BYTE_ARRAY value will be extracted as a byte slice (`[]byte`) unless the logical type is UTF8, in which case they are extracted as a string (`string`).\n\nWhen a value extracted as a byte slice exists within a document which is later JSON serialized by default it will be base 64 encoded into strings, which is the default for arbitrary data fields. It is possible to convert these binary values to strings (or other data types) using Bloblang transformations such as `root.foo = this.foo.string()` or `root.foo = this.foo.encode(\"hex\")`, etc."
      },
      {
        "name": "pg_stream",
        "type": "inputs",
        "status": "deprecated",
        "version": "4.39.0",
        "description": "Streams changes from a PostgreSQL database for Change Data Capture (CDC).\nAdditionally, if `stream_snapshot` is set to true, then the existing data in the database is also streamed too.\n\n== Metadata\n\nThis input adds the following metadata fields to each message:\n- table (Name of the table that the message originated from)\n- operation (Type of operation that generated the message: \"read\", \"insert\", \"update\", or \"delete\". \"read\" is from messages that are read in the initial snapshot phase. This will also be \"begin\" and \"commit\" if `include_transaction_markers` is enabled)\n- lsn (the log sequence number in postgres)\n\t\t"
      },
      {
        "name": "postgres_cdc",
        "type": "inputs",
        "status": "beta",
        "version": "4.39.0",
        "description": "Streams changes from a PostgreSQL database for Change Data Capture (CDC).\nAdditionally, if `stream_snapshot` is set to true, then the existing data in the database is also streamed too.\n\n== Metadata\n\nThis input adds the following metadata fields to each message:\n- table (Name of the table that the message originated from)\n- operation (Type of operation that generated the message: \"read\", \"insert\", \"update\", or \"delete\". \"read\" is from messages that are read in the initial snapshot phase. This will also be \"begin\" and \"commit\" if `include_transaction_markers` is enabled)\n- lsn (the log sequence number in postgres)\n\t\t"
      },
      {
        "name": "pulsar",
        "type": "inputs",
        "status": "experimental",
        "version": "3.43.0",
        "description": "\n== Metadata\n\nThis input adds the following metadata fields to each message:\n\n```text\n- pulsar_message_id\n- pulsar_key\n- pulsar_ordering_key\n- pulsar_event_time_unix\n- pulsar_publish_time_unix\n- pulsar_topic\n- pulsar_producer_name\n- pulsar_redelivery_count\n- All properties of the message\n```\n\nYou can access these metadata fields using\nxref:configuration:interpolation.adoc#bloblang-queries[function interpolation].\n"
      },
      {
        "name": "read_until",
        "type": "inputs",
        "status": "stable",
        "version": "",
        "description": "\nMessages are read continuously while the query check returns false, when the query returns true the message that triggered the check is sent out and the input is closed. Use this to define inputs where the stream should end once a certain message appears.\n\nIf the idle timeout is configured, the input will be closed if no new messages arrive after that period of time. Use this field if you want to empty out and close an input that doesn't have a logical end.\n\nSometimes inputs close themselves. For example, when the `file` input type reaches the end of a file it will shut down. By default this type will also shut down. If you wish for the input type to be restarted every time it shuts down until the query check is met then set `restart_input` to `true`.\n\n== Metadata\n\nA metadata key `benthos_read_until` containing the value `final` is added to the first part of the message that triggers the input to stop."
      },
      {
        "name": "redis_list",
        "type": "inputs",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "redis_pubsub",
        "type": "inputs",
        "status": "stable",
        "version": "",
        "description": "\nIn order to subscribe to channels using the `PSUBSCRIBE` command set the field `use_patterns` to `true`, then you can include glob-style patterns in your channel names. For example:\n\n- `h?llo` subscribes to hello, hallo and hxllo\n- `h*llo` subscribes to hllo and heeeello\n- `h[ae]llo` subscribes to hello and hallo, but not hillo\n\nUse `\\` to escape special characters if you want to match them verbatim."
      },
      {
        "name": "redis_scan",
        "type": "inputs",
        "status": "experimental",
        "version": "4.27.0",
        "description": "Optionally, iterates only elements matching a blob-style pattern. For example:\n\n- `*foo*` iterates only keys which contain `foo` in it.\n- `foo*` iterates only keys starting with `foo`.\n\nThis input generates a message for each key value pair in the following format:\n\n```json\n{\"key\":\"foo\",\"value\":\"bar\"}\n```\n"
      },
      {
        "name": "redis_streams",
        "type": "inputs",
        "status": "stable",
        "version": "",
        "description": "Redis stream entries are key/value pairs, as such it is necessary to specify the key that contains the body of the message. All other keys/value pairs are saved as metadata fields."
      },
      {
        "name": "redpanda",
        "type": "inputs",
        "status": "beta",
        "version": "",
        "description": "\nWhen a consumer group is specified this input consumes one or more topics where partitions will automatically balance across any other connected clients with the same consumer group. When a consumer group is not specified topics can either be consumed in their entirety or with explicit partitions.\n\n== Delivery Guarantees\n\nWhen using consumer groups the offsets of \"delivered\" records will be committed automatically and continuously, and in the event of restarts these committed offsets will be used in order to resume from where the input left off. Redpanda Connect guarantees at least once delivery by ensuring that records are only considerd to be delivered when all configured outputs that the record is routed to have confirmed delivery.\n\n== Ordering\n\nIn order to preserve ordering of topic partitions, records consumed from each partition are processed and delivered in the order that they are received, and only one batch of records of a given partition will ever be processed at a time. This means that parallel processing can only occur when multiple topic partitions are being consumed, but ensures that data is processed in a sequential order as determined from the source partition.\n\nHowever, one way in which the order of records can be mixed is when delivery errors occur and error handling mechanisms kick in. Redpanda Connect always leans towards at least once delivery unless instructed otherwise, and this includes reattempting delivery of data when the ordering of that data can no longer be guaranteed.\n\nFor example, a batch of records may have been sent to an output broker and only a subset of records were delivered, in this case Redpanda Connect by default will reattempt to deliver the records that failed, even though these failed records may have come before records that were previously delivered successfully.\n\nIn order to avoid this scenario you must specify in your configuration an alternative way to handle delivery errors in the form of a xref:components:outputs/fallback.adoc[`fallback`] output. It is good practice to also disable the field `auto_retry_nacks` by setting it to `false` when you've added an explicit fallback output as this will improve the throughput of your pipeline. For example, the following config avoids ordering issues by specifying a fallback output into a DLQ topic, which is also retried indefinitely as a way to apply back pressure during connectivity issues:\n\n```yaml\noutput:\n  fallback:\n    - redpanda:\n        seed_brokers: [ localhost:9092 ]\n        topic: foo\n    - retry:\n        output:\n          redpanda:\n            seed_brokers: [ localhost:9092 ]\n            topic: foo_dlq\n```\n\n== Batching\n\nRecords are processed and delivered from each partition in batches as received from brokers. These batch sizes are therefore dynamically sized in order to optimise throughput, but can be tuned with the config fields `fetch_max_partition_bytes` and `fetch_max_bytes`. Batches can be further broken down using the xref:components:processors/split.adoc[`split`] processor.\n\n== Metrics\n\nEmits a `redpanda_lag` metric with `topic` and `partition` labels for each consumed topic.\n\n== Metadata\n\nThis input adds the following metadata fields to each message:\n\n```text\n- kafka_key\n- kafka_topic\n- kafka_partition\n- kafka_offset\n- kafka_lag\n- kafka_timestamp_ms\n- kafka_timestamp_unix\n- kafka_tombstone_message\n- All record headers\n```\n"
      },
      {
        "name": "redpanda_common",
        "type": "inputs",
        "status": "deprecated",
        "version": "",
        "description": "\nWhen a consumer group is specified this input consumes one or more topics where partitions will automatically balance across any other connected clients with the same consumer group. When a consumer group is not specified topics can either be consumed in their entirety or with explicit partitions.\n\n== Delivery Guarantees\n\nWhen using consumer groups the offsets of \"delivered\" records will be committed automatically and continuously, and in the event of restarts these committed offsets will be used in order to resume from where the input left off. Redpanda Connect guarantees at least once delivery by ensuring that records are only considerd to be delivered when all configured outputs that the record is routed to have confirmed delivery.\n\n== Ordering\n\nIn order to preserve ordering of topic partitions, records consumed from each partition are processed and delivered in the order that they are received, and only one batch of records of a given partition will ever be processed at a time. This means that parallel processing can only occur when multiple topic partitions are being consumed, but ensures that data is processed in a sequential order as determined from the source partition.\n\nHowever, one way in which the order of records can be mixed is when delivery errors occur and error handling mechanisms kick in. Redpanda Connect always leans towards at least once delivery unless instructed otherwise, and this includes reattempting delivery of data when the ordering of that data can no longer be guaranteed.\n\nFor example, a batch of records may have been sent to an output broker and only a subset of records were delivered, in this case Redpanda Connect by default will reattempt to deliver the records that failed, even though these failed records may have come before records that were previously delivered successfully.\n\nIn order to avoid this scenario you must specify in your configuration an alternative way to handle delivery errors in the form of a xref:components:outputs/fallback.adoc[`fallback`] output. It is good practice to also disable the field `auto_retry_nacks` by setting it to `false` when you've added an explicit fallback output as this will improve the throughput of your pipeline. For example, the following config avoids ordering issues by specifying a fallback output into a DLQ topic, which is also retried indefinitely as a way to apply back pressure during connectivity issues:\n\n```yaml\noutput:\n  fallback:\n    - redpanda_common:\n        topic: foo\n    - retry:\n        output:\n          redpanda_common:\n            topic: foo_dlq\n```\n\n== Batching\n\nRecords are processed and delivered from each partition in batches as received from brokers. These batch sizes are therefore dynamically sized in order to optimise throughput, but can be tuned with the config fields `fetch_max_partition_bytes` and `fetch_max_bytes`. Batches can be further broken down using the xref:components:processors/split.adoc[`split`] processor.\n\n== Metrics\n\nEmits a `redpanda_lag` metric with `topic` and `partition` labels for each consumed topic.\n\n== Metadata\n\nThis input adds the following metadata fields to each message:\n\n```text\n- kafka_key\n- kafka_topic\n- kafka_partition\n- kafka_offset\n- kafka_lag\n- kafka_timestamp_ms\n- kafka_timestamp_unix\n- kafka_tombstone_message\n- All record headers\n```\n"
      },
      {
        "name": "redpanda_migrator",
        "type": "inputs",
        "status": "experimental",
        "version": "4.67.0",
        "description": "\nThe `redpanda_migrator` input simply consumes records from the source cluster and forwards them downstream. \nIt does not perform topic/schema/group synchronisation. \nAll migration features and coordination live in the paired `redpanda_migrator` output.\n\n**IMPORTANT:** This input requires a corresponding `redpanda_migrator` output in the same pipeline. \nEach pipeline must have both input and output components configured.\nFor capabilities, guarantees, scheduling, and examples, see the output documentation.\n\n**Multiple migrator pairs:** When using multiple migrator pairs in a single pipeline, \nthe mapping between input and output components is done based on the label field. \nThe label of the input and output must match exactly for proper coordination.\n\n**Performance tuning for high throughput:** For workloads with high message rates or large messages, \nadjust the following fields to increase buffer sizes and batch processing:\n- `partition_buffer_bytes`: Increase to 10MB or higher (default: 1MB)\n- `max_yield_batch_bytes`: Increase to 100MB or higher (default: 10MB)\n\nThese settings allow the consumer to buffer more data per partition and yield larger batches, \nreducing overhead and improving throughput at the cost of higher memory usage."
      },
      {
        "name": "redpanda_migrator_bundle",
        "type": "inputs",
        "status": "deprecated",
        "version": "",
        "description": "All-in-one input which reads messages and schemas from a Kafka or Redpanda cluster. This input is meant to be used\ntogether with the `redpanda_migrator_bundle` output.\n"
      },
      {
        "name": "resource",
        "type": "inputs",
        "status": "stable",
        "version": "",
        "description": "Resources allow you to tidy up deeply nested configs. For example, the config:\n\n```yaml\ninput:\n  broker:\n    inputs:\n      - kafka:\n          addresses: [ TODO ]\n          topics: [ foo ]\n          consumer_group: foogroup\n      - gcp_pubsub:\n          project: bar\n          subscription: baz\n```\n\nCould also be expressed as:\n\n```yaml\ninput:\n  broker:\n    inputs:\n      - resource: foo\n      - resource: bar\n\ninput_resources:\n  - label: foo\n    kafka:\n      addresses: [ TODO ]\n      topics: [ foo ]\n      consumer_group: foogroup\n\n  - label: bar\n    gcp_pubsub:\n      project: bar\n      subscription: baz\n```\n\nResources also allow you to reference a single input in multiple places, such as multiple streams mode configs, or multiple entries in a broker input. However, when a resource is referenced more than once the messages it produces are distributed across those references, so each message will only be directed to a single reference, not all of them.\n\nYou can find out more about resources in xref:configuration:resources.adoc[]."
      },
      {
        "name": "schema_registry",
        "type": "inputs",
        "status": "beta",
        "version": "4.32.2",
        "description": "\n== Metadata\n\nThis input adds the following metadata fields to each message:\n\n```text\n- schema_registry_subject\n- schema_registry_subject_compatibility_level\n- schema_registry_version\n```\n\nYou can access these metadata fields using\nxref:configuration:interpolation.adoc#bloblang-queries[function interpolation].\n\n"
      },
      {
        "name": "sequence",
        "type": "inputs",
        "status": "stable",
        "version": "",
        "description": "This input is useful for consuming from inputs that have an explicit end but must not be consumed in parallel."
      },
      {
        "name": "sftp",
        "type": "inputs",
        "status": "beta",
        "version": "3.39.0",
        "description": "\n== Metadata\n\nThis input adds the following metadata fields to each message:\n\n- sftp_path\n- sftp_mod_time\n\nYou can access these metadata fields using xref:configuration:interpolation.adoc#bloblang-queries[function interpolation]."
      },
      {
        "name": "slack",
        "type": "inputs",
        "status": "experimental",
        "version": "",
        "description": "Connects to Slack using https://api.slack.com/apis/socket-mode[^Socket Mode]. This allows for receiving events, interactions and slash commands. Each message emitted from this input has a @type metadata of the event type \"events_api\", \"interactions\" or \"slash_commands\"."
      },
      {
        "name": "slack_users",
        "type": "inputs",
        "status": "experimental",
        "version": "",
        "description": "Reads all users in a slack organization (optionally filtered by a team ID)."
      },
      {
        "name": "socket",
        "type": "inputs",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "socket_server",
        "type": "inputs",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "spicedb_watch",
        "type": "inputs",
        "status": "stable",
        "version": "",
        "description": "\nThe SpiceDB input allows you to consume messages from the Watch API of a SpiceDB instance.\nThis input is useful for applications that need to react to changes in the data managed by SpiceDB in real-time.\n\n== Credentials\n\nYou need to provide the endpoint of your SpiceDB instance and a Bearer token for authentication.\n\n== Cache\n\nThe zed token of the newest update consumed and acked is stored in a cache in order to start reading from it each time the input is initialised.\nIdeally this cache should be persisted across restarts.\n"
      },
      {
        "name": "splunk",
        "type": "inputs",
        "status": "beta",
        "version": "4.30.0",
        "description": ""
      },
      {
        "name": "sql_raw",
        "type": "inputs",
        "status": "beta",
        "version": "4.10.0",
        "description": "Once the rows from the query are exhausted this input shuts down, allowing the pipeline to gracefully terminate (or the next input in a xref:components:inputs/sequence.adoc[sequence] to execute)."
      },
      {
        "name": "sql_select",
        "type": "inputs",
        "status": "beta",
        "version": "3.59.0",
        "description": "Once the rows from the query are exhausted this input shuts down, allowing the pipeline to gracefully terminate (or the next input in a xref:components:inputs/sequence.adoc[sequence] to execute)."
      },
      {
        "name": "stdin",
        "type": "inputs",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "subprocess",
        "type": "inputs",
        "status": "beta",
        "version": "",
        "description": "\nMessages are consumed according to a specified codec. The command is executed once and if it terminates the input also closes down gracefully. Alternatively, the field `restart_on_close` can be set to `true` in order to have Redpanda Connect re-execute the command each time it stops.\n\nThe field `max_buffer` defines the maximum message size able to be read from the subprocess. This value should be set significantly above the real expected maximum message size.\n\nThe execution environment of the subprocess is the same as the Redpanda Connect instance, including environment variables and the current working directory."
      },
      {
        "name": "timeplus",
        "type": "inputs",
        "status": "experimental",
        "version": "",
        "description": "\nThis input can execute a query on Timeplus Enterprise Cloud, Timeplus Enterprise (self-hosted) or Timeplusd. A structured message will be created\nfrom each row received.\n\nIf it is a streaming query, this input will keep running until the query is terminated. If it is a table query, this input will shut down once the rows from the query are exhausted."
      },
      {
        "name": "twitter_search",
        "type": "inputs",
        "status": "experimental",
        "version": "",
        "description": "Continuously polls the https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-recent[Twitter recent search V2 API^] for tweets that match a given search query.\n\nEach tweet received is emitted as a JSON object message, with a field `id` and `text` by default. Extra fields https://developer.twitter.com/en/docs/twitter-api/fields[can be obtained from the search API^] when listed with the `tweet_fields` field.\n\nIn order to paginate requests that are made the ID of the latest received tweet is stored in a xref:components:caches/about.adoc[cache resource], which is then used by subsequent requests to ensure only tweets after it are consumed. It is recommended that the cache you use is persistent so that Redpanda Connect can resume searches at the correct place on a restart.\n\nAuthentication is done using OAuth 2.0 credentials which can be generated within the https://developer.twitter.com[Twitter developer portal^].\n"
      },
      {
        "name": "websocket",
        "type": "inputs",
        "status": "stable",
        "version": "",
        "description": "It is possible to configure an `open_message`, which when set to a non-empty string will be sent to the websocket server each time a connection is first established."
      },
      {
        "name": "amqp_0_9",
        "type": "outputs",
        "status": "stable",
        "version": "",
        "description": "The metadata from each message are delivered as headers.\n\nIt's possible for this output type to create the target exchange by setting `exchange_declare.enabled` to `true`, if the exchange already exists then the declaration passively verifies that the settings match.\n\nTLS is automatic when connecting to an `amqps` URL, but custom settings can be enabled in the `tls` section.\n\nThe fields 'key', 'exchange' and 'type' can be dynamically set using xref:configuration:interpolation.adoc#bloblang-queries[function interpolations]."
      },
      {
        "name": "amqp_1",
        "type": "outputs",
        "status": "stable",
        "version": "",
        "description": "\n== Metadata\n\nMessage metadata is added to each AMQP message as string annotations. In order to control which metadata keys are added use the `metadata` config field.\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`."
      },
      {
        "name": "aws_dynamodb",
        "type": "outputs",
        "status": "stable",
        "version": "3.36.0",
        "description": "\nThe field `string_columns` is a map of column names to string values, where the values are xref:configuration:interpolation.adoc#bloblang-queries[function interpolated] per message of a batch. This allows you to populate string columns of an item by extracting fields within the document payload or metadata like follows:\n\n```yml\nstring_columns:\n  id: ${!json(\"id\")}\n  title: ${!json(\"body.title\")}\n  topic: ${!meta(\"kafka_topic\")}\n  full_content: ${!content()}\n```\n\nThe field `json_map_columns` is a map of column names to json paths, where the xref:configuration:field_paths.adoc[dot path] is extracted from each document and converted into a map value. Both an empty path and the path `.` are interpreted as the root of the document. This allows you to populate map columns of an item like follows:\n\n```yml\njson_map_columns:\n  user: path.to.user\n  whole_document: .\n```\n\nA column name can be empty:\n\n```yml\njson_map_columns:\n  \"\": .\n```\n\nIn which case the top level document fields will be written at the root of the item, potentially overwriting previously defined column values. If a path is not found within a document the column will not be populated.\n\n== Credentials\n\nBy default Redpanda Connect will use a shared credentials file when connecting to AWS services. It's also possible to set them explicitly at the component level, allowing you to transfer data across accounts. You can find out more in xref:guides:cloud/aws.adoc[].\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`.\n\nThis output benefits from sending messages as a batch for improved performance. Batches can be formed at both the input and output level. You can find out more xref:configuration:batching.adoc[in this doc].\n"
      },
      {
        "name": "aws_kinesis",
        "type": "outputs",
        "status": "stable",
        "version": "3.36.0",
        "description": "\nBoth the `partition_key`(required) and `hash_key` (optional) fields can be dynamically set using function interpolations described xref:configuration:interpolation.adoc#bloblang-queries[here]. When sending batched messages the interpolations are performed per message part.\n\n== Credentials\n\nBy default Redpanda Connect will use a shared credentials file when connecting to AWS services. It's also possible to set them explicitly at the component level, allowing you to transfer data across accounts. You can find out more in xref:guides:cloud/aws.adoc[].\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`.\n\nThis output benefits from sending messages as a batch for improved performance. Batches can be formed at both the input and output level. You can find out more xref:configuration:batching.adoc[in this doc]."
      },
      {
        "name": "aws_kinesis_firehose",
        "type": "outputs",
        "status": "stable",
        "version": "3.36.0",
        "description": "\n== Credentials\n\nBy default Redpanda Connect will use a shared credentials file when connecting to AWS services. It's also possible to set them explicitly at the component level, allowing you to transfer data across accounts. You can find out more in xref:guides:cloud/aws.adoc[].\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`.\n\nThis output benefits from sending messages as a batch for improved performance. Batches can be formed at both the input and output level. You can find out more xref:configuration:batching.adoc[in this doc].\n"
      },
      {
        "name": "aws_s3",
        "type": "outputs",
        "status": "stable",
        "version": "3.36.0",
        "description": "\nIn order to have a different path for each object you should use function interpolations described in xref:configuration:interpolation.adoc#bloblang-queries[Bloblang queries], which are calculated per message of a batch.\n\n== Metadata\n\nMetadata fields on messages will be sent as headers, in order to mutate these values (or remove them) check out the xref:configuration:metadata.adoc[metadata docs].\n\n== Tags\n\nThe tags field allows you to specify key/value pairs to attach to objects as tags, where the values support xref:configuration:interpolation.adoc#bloblang-queries[interpolation functions]:\n\n```yaml\noutput:\n  aws_s3:\n    bucket: TODO\n    path: ${!counter()}-${!timestamp_unix_nano()}.tar.gz\n    tags:\n      Key1: Value1\n      Timestamp: ${!meta(\"Timestamp\")}\n```\n\n=== Credentials\n\nBy default Redpanda Connect will use a shared credentials file when connecting to AWS services. It's also possible to set them explicitly at the component level, allowing you to transfer data across accounts. You can find out more in xref:guides:cloud/aws.adoc[].\n\n== Batching\n\nIt's common to want to upload messages to S3 as batched archives, the easiest way to do this is to batch your messages at the output level and join the batch of messages with an xref:components:processors/archive.adoc[`archive`] and/or xref:components:processors/compress.adoc[`compress`] processor.\n\nFor example, if we wished to upload messages as a .tar.gz archive of documents we could achieve that with the following config:\n\n```yaml\noutput:\n  aws_s3:\n    bucket: TODO\n    path: ${!counter()}-${!timestamp_unix_nano()}.tar.gz\n    batching:\n      count: 100\n      period: 10s\n      processors:\n        - archive:\n            format: tar\n        - compress:\n            algorithm: gzip\n```\n\nAlternatively, if we wished to upload JSON documents as a single large document containing an array of objects we can do that with:\n\n```yaml\noutput:\n  aws_s3:\n    bucket: TODO\n    path: ${!counter()}-${!timestamp_unix_nano()}.json\n    batching:\n      count: 100\n      processors:\n        - archive:\n            format: json_array\n```\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`."
      },
      {
        "name": "aws_sns",
        "type": "outputs",
        "status": "stable",
        "version": "3.36.0",
        "description": "\n== Credentials\n\nBy default Redpanda Connect will use a shared credentials file when connecting to AWS services. It's also possible to set them explicitly at the component level, allowing you to transfer data across accounts. You can find out more in xref:guides:cloud/aws.adoc[].\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`."
      },
      {
        "name": "aws_sqs",
        "type": "outputs",
        "status": "stable",
        "version": "3.36.0",
        "description": "\nMetadata values are sent along with the payload as attributes with the data type String. If the number of metadata values in a message exceeds the message attribute limit (10) then the top ten keys ordered alphabetically will be selected.\n\nThe fields `message_group_id`, `message_deduplication_id` and `delay_seconds` can be set dynamically using xref:configuration:interpolation.adoc#bloblang-queries[function interpolations], which are resolved individually for each message of a batch.\n\n== Credentials\n\nBy default Redpanda Connect will use a shared credentials file when connecting to AWS services. It's also possible to set them explicitly at the component level, allowing you to transfer data across accounts. You can find out more in xref:guides:cloud/aws.adoc[].\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`.\n\nThis output benefits from sending messages as a batch for improved performance. Batches can be formed at both the input and output level. You can find out more xref:configuration:batching.adoc[in this doc]."
      },
      {
        "name": "azure_blob_storage",
        "type": "outputs",
        "status": "beta",
        "version": "3.36.0",
        "description": "\nIn order to have a different path for each object you should use function\ninterpolations described xref:configuration:interpolation.adoc#bloblang-queries[here], which are\ncalculated per message of a batch.\n\nSupports multiple authentication methods but only one of the following is required:\n\n- `storage_connection_string`\n- `storage_account` and `storage_access_key`\n- `storage_account` and `storage_sas_token`\n- `storage_account` to access via https://pkg.go.dev/github.com/Azure/azure-sdk-for-go/sdk/azidentity#DefaultAzureCredential[DefaultAzureCredential^]\n\nIf multiple are set then the `storage_connection_string` is given priority.\n\nIf the `storage_connection_string` does not contain the `AccountName` parameter, please specify it in the\n`storage_account` field.\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`."
      },
      {
        "name": "azure_cosmosdb",
        "type": "outputs",
        "status": "experimental",
        "version": "v4.25.0",
        "description": "\nWhen creating documents, each message must have the `id` property (case-sensitive) set (or use `auto_id: true`). It is the unique name that identifies the document, that is, no two documents share the same `id` within a logical partition. The `id` field must not exceed 255 characters. https://learn.microsoft.com/en-us/rest/api/cosmos-db/documents[See details^].\n\nThe `partition_keys` field must resolve to the same value(s) across the entire message batch.\n\n\n== Credentials\n\nYou can use one of the following authentication mechanisms:\n\n- Set the `endpoint` field and the `account_key` field\n- Set only the `endpoint` field to use https://pkg.go.dev/github.com/Azure/azure-sdk-for-go/sdk/azidentity#DefaultAzureCredential[DefaultAzureCredential^]\n- Set the `connection_string` field\n\n\n== Batching\n\nCosmosDB limits the maximum batch size to 100 messages and the payload must not exceed 2MB (https://learn.microsoft.com/en-us/azure/cosmos-db/concepts-limits#per-request-limits[details here^]).\n\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`.\n\nThis output benefits from sending messages as a batch for improved performance. Batches can be formed at both the input and output level. You can find out more xref:configuration:batching.adoc[in this doc]."
      },
      {
        "name": "azure_data_lake_gen2",
        "type": "outputs",
        "status": "beta",
        "version": "4.38.0",
        "description": "\nIn order to have a different path for each file you should use function\ninterpolations described xref:configuration:interpolation.adoc#bloblang-queries[here], which are\ncalculated per message of a batch.\n\nSupports multiple authentication methods but only one of the following is required:\n\n- `storage_connection_string`\n- `storage_account` and `storage_access_key`\n- `storage_account` and `storage_sas_token`\n- `storage_account` to access via https://pkg.go.dev/github.com/Azure/azure-sdk-for-go/sdk/azidentity#DefaultAzureCredential[DefaultAzureCredential^]\n\nIf multiple are set then the `storage_connection_string` is given priority.\n\nIf the `storage_connection_string` does not contain the `AccountName` parameter, please specify it in the\n`storage_account` field.\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`."
      },
      {
        "name": "azure_queue_storage",
        "type": "outputs",
        "status": "beta",
        "version": "3.36.0",
        "description": "\nOnly one authentication method is required, `storage_connection_string` or `storage_account` and `storage_access_key`. If both are set then the `storage_connection_string` is given priority.\n\nIn order to set the `queue_name` you can use function interpolations described xref:configuration:interpolation.adoc#bloblang-queries[here], which are calculated per message of a batch.\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`.\n\nThis output benefits from sending messages as a batch for improved performance. Batches can be formed at both the input and output level. You can find out more xref:configuration:batching.adoc[in this doc]."
      },
      {
        "name": "azure_table_storage",
        "type": "outputs",
        "status": "beta",
        "version": "3.36.0",
        "description": "\nOnly one authentication method is required, `storage_connection_string` or `storage_account` and `storage_access_key`. If both are set then the `storage_connection_string` is given priority.\n\nIn order to set the `table_name`,  `partition_key` and `row_key` you can use function interpolations described xref:configuration:interpolation.adoc#bloblang-queries[here], which are calculated per message of a batch.\n\nIf the `properties` are not set in the config, all the `json` fields are marshalled and stored in the table, which will be created if it does not exist.\n\nThe `object` and `array` fields are marshaled as strings. e.g.:\n\nThe JSON message:\n\n```json\n{\n  \"foo\": 55,\n  \"bar\": {\n    \"baz\": \"a\",\n    \"bez\": \"b\"\n  },\n  \"diz\": [\"a\", \"b\"]\n}\n```\n\nWill store in the table the following properties:\n\n```yml\nfoo: '55'\nbar: '{ \"baz\": \"a\", \"bez\": \"b\" }'\ndiz: '[\"a\", \"b\"]'\n```\n\nIt's also possible to use function interpolations to get or transform the properties values, e.g.:\n\n```yml\nproperties:\n  device: '${! json(\"device\") }'\n  timestamp: '${! json(\"timestamp\") }'\n```\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`.\n\nThis output benefits from sending messages as a batch for improved performance. Batches can be formed at both the input and output level. You can find out more xref:configuration:batching.adoc[in this doc]."
      },
      {
        "name": "beanstalkd",
        "type": "outputs",
        "status": "experimental",
        "version": "4.7.0",
        "description": ""
      },
      {
        "name": "broker",
        "type": "outputs",
        "status": "stable",
        "version": "",
        "description": "\nxref:components:processors/about.adoc[Processors] can be listed to apply across individual outputs or all outputs:\n\n```yaml\noutput:\n  broker:\n    pattern: fan_out\n    outputs:\n      - resource: foo\n      - resource: bar\n        # Processors only applied to messages sent to bar.\n        processors:\n          - resource: bar_processor\n\n  # Processors applied to messages sent to all brokered outputs.\n  processors:\n    - resource: general_processor\n```"
      },
      {
        "name": "cache",
        "type": "outputs",
        "status": "stable",
        "version": "",
        "description": "Caches are configured as xref:components:caches/about.adoc[resources], where there's a wide variety to choose from.\n\n:cache-support: aws_dynamodb=certified, aws_s3=certified, file=certified, memcached=certified, memory=certified, nats_kv=certified, redis=certified, ristretto=certified, couchbase=community, mongodb=community, sql=community, multilevel=community, ttlru=community, gcp_cloud_storage=community, lru=community, noop=community\n\nThe `target` field must reference a configured cache resource label like follows:\n\n```yaml\noutput:\n  cache:\n    target: foo\n    key: ${!json(\"document.id\")}\n\ncache_resources:\n  - label: foo\n    memcached:\n      addresses:\n        - localhost:11211\n      default_ttl: 60s\n```\n\nIn order to create a unique `key` value per item you should use function interpolations described in xref:configuration:interpolation.adoc#bloblang-queries[Bloblang queries].\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`."
      },
      {
        "name": "cassandra",
        "type": "outputs",
        "status": "beta",
        "version": "",
        "description": "\nQuery arguments can be set using a bloblang array for the fields using the `args_mapping` field.\n\nWhen populating timestamp columns the value must either be a string in ISO 8601 format (2006-01-02T15:04:05Z07:00), or an integer representing unix time in seconds.\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`.\n\nThis output benefits from sending messages as a batch for improved performance. Batches can be formed at both the input and output level. You can find out more xref:configuration:batching.adoc[in this doc]."
      },
      {
        "name": "couchbase",
        "type": "outputs",
        "status": "experimental",
        "version": "4.37.0",
        "description": "When inserting, replacing or upserting documents, each must have the `content` property set.\n\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`.\n\nThis output benefits from sending messages as a batch for improved performance. Batches can be formed at both the input and output level. You can find out more xref:configuration:batching.adoc[in this doc]."
      },
      {
        "name": "cyborgdb",
        "type": "outputs",
        "status": "experimental",
        "version": "",
        "description": "\nThis output allows you to write vectors to a CyborgDB encrypted index. CyborgDB provides\nend-to-end encrypted vector storage with automatic dimension detection and index optimization.\n\nAll vector data is encrypted client-side before being sent to the server, ensuring complete\ndata privacy. The encryption key never leaves your infrastructure.\n"
      },
      {
        "name": "cypher",
        "type": "outputs",
        "status": "experimental",
        "version": "4.37.0",
        "description": "The cypher output type writes a batch of messages to any graph database that supports the Neo4j or Bolt protocols."
      },
      {
        "name": "discord",
        "type": "outputs",
        "status": "experimental",
        "version": "",
        "description": "\nThis output POSTs messages to the `/channels/\\{channel_id}/messages` Discord API endpoint authenticated as a bot using token based authentication.\n\nIf the format of a message is a JSON object matching the https://discord.com/developers/docs/resources/channel#message-object[Discord API message type^] then it is sent directly, otherwise an object matching the API type is created with the content of the message added as a string.\n"
      },
      {
        "name": "drop",
        "type": "outputs",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "drop_on",
        "type": "outputs",
        "status": "stable",
        "version": "",
        "description": "Regular Redpanda Connect outputs will apply back pressure when downstream services aren't accessible, and Redpanda Connect retries (or nacks) all messages that fail to be delivered. However, in some circumstances, or for certain output types, we instead might want to relax these mechanisms, which is when this output becomes useful."
      },
      {
        "name": "dynamic",
        "type": "outputs",
        "status": "stable",
        "version": "",
        "description": "The broker pattern used is always `fan_out`, meaning each message will be delivered to each dynamic output."
      },
      {
        "name": "elasticsearch_v8",
        "type": "outputs",
        "status": "stable",
        "version": "",
        "description": "\nBoth the `id` and `index` fields can be dynamically set using function interpolations described xref:configuration:interpolation.adoc#bloblang-queries[here]. When sending batched messages these interpolations are performed per message part.\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`.\n\nThis output benefits from sending messages as a batch for improved performance. Batches can be formed at both the input and output level. You can find out more xref:configuration:batching.adoc[in this doc]."
      },
      {
        "name": "fallback",
        "type": "outputs",
        "status": "stable",
        "version": "3.58.0",
        "description": "\nThis pattern is useful for triggering events in the case where certain output targets have broken. For example, if you had an output type `http_client` but wished to reroute messages whenever the endpoint becomes unreachable you could use this pattern:\n\n```yaml\noutput:\n  fallback:\n    - http_client:\n        url: http://foo:4195/post/might/become/unreachable\n        retries: 3\n        retry_period: 1s\n    - http_client:\n        url: http://bar:4196/somewhere/else\n        retries: 3\n        retry_period: 1s\n      processors:\n        - mapping: 'root = \"failed to send this message to foo: \" + content()'\n    - file:\n        path: /usr/local/benthos/everything_failed.jsonl\n```\n\n== Metadata\n\nWhen a given output fails the message routed to the following output will have a metadata value named `fallback_error` containing a string error message outlining the cause of the failure. The content of this string will depend on the particular output and can be used to enrich the message or provide information used to broker the data to an appropriate output using something like a `switch` output.\n\n== Batching\n\nWhen an output within a fallback sequence uses batching, like so:\n\n```yaml\noutput:\n  fallback:\n    - aws_dynamodb:\n        table: foo\n        string_columns:\n          id: ${!json(\"id\")}\n          content: ${!content()}\n        batching:\n          count: 10\n          period: 1s\n    - file:\n        path: /usr/local/benthos/failed_stuff.jsonl\n```\n\nRedpanda Connect makes a best attempt at inferring which specific messages of the batch failed, and only propagates those individual messages to the next fallback tier.\n\nHowever, depending on the output and the error returned it is sometimes not possible to determine the individual messages that failed, in which case the whole batch is passed to the next tier in order to preserve at-least-once delivery guarantees."
      },
      {
        "name": "file",
        "type": "outputs",
        "status": "stable",
        "version": "",
        "description": "Messages can be written to different files by using xref:configuration:interpolation.adoc#bloblang-queries[interpolation functions] in the path field. However, only one file is ever open at a given time, and therefore when the path changes the previously open file is closed."
      },
      {
        "name": "gcp_bigquery",
        "type": "outputs",
        "status": "beta",
        "version": "3.55.0",
        "description": "\n== Credentials\n\nBy default Redpanda Connect will use a shared credentials file when connecting to GCP services. You can find out more in xref:guides:cloud/gcp.adoc[].\n\n== Format\n\nThis output currently supports only CSV, NEWLINE_DELIMITED_JSON and PARQUET, formats. Learn more about how to use GCP BigQuery with them here:\n\n- https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-json[`NEWLINE_DELIMITED_JSON`^]\n- https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv[`CSV`^]\n- https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-parquet[`PARQUET`^]\n\nEach message may contain multiple elements separated by newlines. For example a single message containing:\n\n```json\n{\"key\": \"1\"}\n{\"key\": \"2\"}\n```\n\nIs equivalent to two separate messages:\n\n```json\n{\"key\": \"1\"}\n```\n\nAnd:\n\n```json\n{\"key\": \"2\"}\n```\n\nThe same is true for the CSV format.\n\n=== CSV\n\nFor the CSV format when the field `csv.header` is specified a header row will be inserted as the first line of each message batch. If this field is not provided then the first message of each message batch must include a header line.\n\n=== Parquet\n\nFor parquet, the data can be encoded using the `parquet_encode` processor and each message that is sent to the output must be a full parquet message.\n\n\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`.\n\nThis output benefits from sending messages as a batch for improved performance. Batches can be formed at both the input and output level. You can find out more xref:configuration:batching.adoc[in this doc]."
      },
      {
        "name": "gcp_cloud_storage",
        "type": "outputs",
        "status": "beta",
        "version": "3.43.0",
        "description": "\nIn order to have a different path for each object you should use function interpolations described in xref:configuration:interpolation.adoc#bloblang-queries[Bloblang queries], which are calculated per message of a batch.\n\n== Metadata\n\nMetadata fields on messages will be sent as headers, in order to mutate these values (or remove them) check out the xref:configuration:metadata.adoc[metadata docs].\n\n== Credentials\n\nBy default Redpanda Connect will use a shared credentials file when connecting to GCP services. You can find out more in xref:guides:cloud/gcp.adoc[].\n\n== Batching\n\nIt's common to want to upload messages to Google Cloud Storage as batched archives, the easiest way to do this is to batch your messages at the output level and join the batch of messages with an xref:components:processors/archive.adoc[`archive`] and/or xref:components:processors/compress.adoc[`compress`] processor.\n\nFor example, if we wished to upload messages as a .tar.gz archive of documents we could achieve that with the following config:\n\n```yaml\noutput:\n  gcp_cloud_storage:\n    bucket: TODO\n    path: ${!counter()}-${!timestamp_unix_nano()}.tar.gz\n    batching:\n      count: 100\n      period: 10s\n      processors:\n        - archive:\n            format: tar\n        - compress:\n            algorithm: gzip\n```\n\nAlternatively, if we wished to upload JSON documents as a single large document containing an array of objects we can do that with:\n\n```yaml\noutput:\n  gcp_cloud_storage:\n    bucket: TODO\n    path: ${!counter()}-${!timestamp_unix_nano()}.json\n    batching:\n      count: 100\n      processors:\n        - archive:\n            format: json_array\n```\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`.\n\nThis output benefits from sending messages as a batch for improved performance. Batches can be formed at both the input and output level. You can find out more xref:configuration:batching.adoc[in this doc]."
      },
      {
        "name": "gcp_pubsub",
        "type": "outputs",
        "status": "stable",
        "version": "",
        "description": "\nFor information on how to set up credentials, see https://cloud.google.com/docs/authentication/production[this guide^].\n\n== Troubleshooting\n\nIf you're consistently seeing `Failed to send message to gcp_pubsub: context deadline exceeded` error logs without any further information it is possible that you are encountering https://github.com/benthosdev/benthos/issues/1042, which occurs when metadata values contain characters that are not valid utf-8. This can frequently occur when consuming from Kafka as the key metadata field may be populated with an arbitrary binary value, but this issue is not exclusive to Kafka.\n\nIf you are blocked by this issue then a work around is to delete either the specific problematic keys:\n\n```yaml\npipeline:\n  processors:\n    - mapping: |\n        meta kafka_key = deleted()\n```\n\nOr delete all keys with:\n\n```yaml\npipeline:\n  processors:\n    - mapping: meta = deleted()\n```"
      },
      {
        "name": "hdfs",
        "type": "outputs",
        "status": "stable",
        "version": "",
        "description": "Each file is written with the path specified with the 'path' field, in order to have a different path for each object you should use function interpolations described xref:configuration:interpolation.adoc#bloblang-queries[here].\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`."
      },
      {
        "name": "http_client",
        "type": "outputs",
        "status": "stable",
        "version": "",
        "description": "\nWhen the number of retries expires the output will reject the message, the behavior after this will depend on the pipeline but usually this simply means the send is attempted again until successful whilst applying back pressure.\n\nThe URL and header values of this type can be dynamically set using function interpolations described in xref:configuration:interpolation.adoc#bloblang-queries[Bloblang queries].\n\nThe body of the HTTP request is the raw contents of the message payload. If the message has multiple parts (is a batch) the request will be sent according to https://www.w3.org/Protocols/rfc1341/7_2_Multipart.html[RFC1341^]. This behavior can be disabled by setting the field <<batch_as_multipart, `batch_as_multipart`>> to `false`.\n\n== Propagate responses\n\nIt's possible to propagate the response from each HTTP request back to the input source by setting `propagate_response` to `true`. Only inputs that support xref:guides:sync_responses.adoc[synchronous responses] are able to make use of these propagated responses.\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`.\n\nThis output benefits from sending messages as a batch for improved performance. Batches can be formed at both the input and output level. You can find out more xref:configuration:batching.adoc[in this doc]."
      },
      {
        "name": "http_server",
        "type": "outputs",
        "status": "stable",
        "version": "",
        "description": "Sets up an HTTP server that will send messages over HTTP(S) GET requests. If the `address` config field is left blank the xref:components:http/about.adoc[service-wide HTTP server] will be used.\n\nThree endpoints will be registered at the paths specified by the fields `path`, `stream_path` and `ws_path`. Which allow you to consume a single message batch, a continuous stream of line delimited messages, or a websocket of messages for each request respectively.\n\nWhen messages are batched the `path` endpoint encodes the batch according to https://www.w3.org/Protocols/rfc1341/7_2_Multipart.html[RFC1341^]. This behavior can be overridden by xref:configuration:batching.adoc#post-batch-processing[archiving your batches].\n\nPlease note, messages are considered delivered as soon as the data is written to the client. There is no concept of at least once delivery on this output.\n\n\n[CAUTION]\n.Endpoint caveats\n====\nComponents within a Redpanda Connect config will register their respective endpoints in a non-deterministic order. This means that establishing precedence of endpoints that are registered via multiple `http_server` inputs or outputs (either within brokers or from cohabiting streams) is not possible in a predictable way.\n\nThis ambiguity makes it difficult to ensure that paths which are both a subset of a path registered by a separate component, and end in a slash (`/`) and will therefore match against all extensions of that path, do not prevent the more specific path from matching against requests.\n\nIt is therefore recommended that you ensure paths of separate components do not collide unless they are explicitly non-competing.\n\nFor example, if you were to deploy two separate `http_server` inputs, one with a path `/foo/` and the other with a path `/foo/bar`, it would not be possible to ensure that the path `/foo/` does not swallow requests made to `/foo/bar`.\n====\n"
      },
      {
        "name": "inproc",
        "type": "outputs",
        "status": "stable",
        "version": "",
        "description": "\nSends data directly to Redpanda Connect inputs by connecting to a unique ID. This allows you to hook up isolated streams whilst running Redpanda Connect in xref:guides:streams_mode/about.adoc[streams mode], it is NOT recommended that you connect the inputs of a stream with an output of the same stream, as feedback loops can lead to deadlocks in your message flow.\n\nIt is possible to connect multiple inputs to the same inproc ID, resulting in messages dispatching in a round-robin fashion to connected inputs. However, only one output can assume an inproc ID, and will replace existing outputs if a collision occurs."
      },
      {
        "name": "kafka",
        "type": "outputs",
        "status": "stable",
        "version": "",
        "description": "\nThe config field `ack_replicas` determines whether we wait for acknowledgement from all replicas or just a single broker.\n\nBoth the `key` and `topic` fields can be dynamically set using function interpolations described in xref:configuration:interpolation.adoc#bloblang-queries[Bloblang queries].\n\nxref:configuration:metadata.adoc[Metadata] will be added to each message sent as headers (version 0.11+), but can be restricted using the field <<metadata, `metadata`>>.\n\n== Strict ordering and retries\n\nWhen strict ordering is required for messages written to topic partitions it is important to ensure that both the field `max_in_flight` is set to `1` and that the field `retry_as_batch` is set to `true`.\n\nYou must also ensure that failed batches are never rerouted back to the same output. This can be done by setting the field `max_retries` to `0` and `backoff.max_elapsed_time` to empty, which will apply back pressure indefinitely until the batch is sent successfully.\n\nHowever, this also means that manual intervention will eventually be required in cases where the batch cannot be sent due to configuration problems such as an incorrect `max_msg_bytes` estimate. A less strict but automated alternative would be to route failed batches to a dead letter queue using a xref:components:outputs/fallback.adoc[`fallback` broker], but this would allow subsequent batches to be delivered in the meantime whilst those failed batches are dealt with.\n\n== Troubleshooting\n\nIf you're seeing issues writing to or reading from Kafka with this component then it's worth trying out the newer xref:components:outputs/kafka_franz.adoc[`kafka_franz` output].\n\n- I'm seeing logs that report `Failed to connect to kafka: kafka: client has run out of available brokers to talk to (Is your cluster reachable?)`, but the brokers are definitely reachable.\n\nUnfortunately this error message will appear for a wide range of connection problems even when the broker endpoint can be reached. Double check your authentication configuration and also ensure that you have <<tlsenabled, enabled TLS>> if applicable.\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`.\n\nThis output benefits from sending messages as a batch for improved performance. Batches can be formed at both the input and output level. You can find out more xref:configuration:batching.adoc[in this doc]."
      },
      {
        "name": "kafka_franz",
        "type": "outputs",
        "status": "beta",
        "version": "3.61.0",
        "description": "\nWrites a batch of messages to Kafka brokers and waits for acknowledgement before propagating it back to the input.\n\nThis output often out-performs the traditional `kafka` output as well as providing more useful logs and error messages.\n"
      },
      {
        "name": "legacy_redpanda_migrator",
        "type": "outputs",
        "status": "deprecated",
        "version": "4.37.0",
        "description": "\nWrites a batch of messages to a Kafka broker and waits for acknowledgement before propagating it back to the input.\n\nThis output should be used in combination with a `legacy_redpanda_migrator` input identified by the label specified in\n`input_resource` which it can query for topic and ACL configurations. Once connected, the output will attempt to\ncreate all topics which the input consumes from along with their ACLs.\n\nIf the configured broker does not contain the current message topic, this output attempts to create it along with its\nACLs.\n\nACL migration adheres to the following principles:\n\n- `ALLOW WRITE` ACLs for topics are not migrated\n- `ALLOW ALL` ACLs for topics are downgraded to `ALLOW READ`\n- Only topic ACLs are migrated, group ACLs are not migrated\n"
      },
      {
        "name": "legacy_redpanda_migrator_offsets",
        "type": "outputs",
        "status": "deprecated",
        "version": "4.37.0",
        "description": "This output should be used in combination with the `legacy_redpanda_migrator_offsets` input"
      },
      {
        "name": "mongodb",
        "type": "outputs",
        "status": "experimental",
        "version": "3.43.0",
        "description": "\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`.\n\nThis output benefits from sending messages as a batch for improved performance. Batches can be formed at both the input and output level. You can find out more xref:configuration:batching.adoc[in this doc]."
      },
      {
        "name": "mqtt",
        "type": "outputs",
        "status": "stable",
        "version": "",
        "description": "\nThe `topic` field can be dynamically set using function interpolations described xref:configuration:interpolation.adoc#bloblang-queries[here]. When sending batched messages these interpolations are performed per message part.\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`."
      },
      {
        "name": "nanomsg",
        "type": "outputs",
        "status": "stable",
        "version": "",
        "description": "Currently only PUSH and PUB sockets are supported.\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`."
      },
      {
        "name": "nats",
        "type": "outputs",
        "status": "stable",
        "version": "",
        "description": "This output will interpolate functions within the subject field, you can find a list of functions xref:configuration:interpolation.adoc#bloblang-queries[here].\n\n== Connection name\n\nWhen monitoring and managing a production NATS system, it is often useful to\nknow which connection a message was send/received from. This can be achieved by\nsetting the connection name option when creating a NATS connection.\n\nRedpanda Connect will automatically set the connection name based off the label of the given\nNATS component, so that monitoring tools between NATS and Redpanda Connect can stay in sync.\n\n\n== Authentication\n\nThere are several components within Redpanda Connect which uses NATS services. You will find that each of these components\nsupport optional advanced authentication parameters for https://docs.nats.io/nats-server/configuration/securing_nats/auth_intro/nkey_auth[NKeys^]\nand https://docs.nats.io/using-nats/developer/connecting/creds[User Credentials^].\n\nSee an https://docs.nats.io/running-a-nats-service/nats_admin/security/jwt[in-depth tutorial^].\n\n=== NKey file\n\nThe NATS server can use these NKeys in several ways for authentication. The simplest is for the server to be configured\nwith a list of known public keys and for the clients to respond to the challenge by signing it with its private NKey\nconfigured in the `nkey_file` or `nkey` field.\n\nhttps://docs.nats.io/running-a-nats-service/configuration/securing_nats/auth_intro/nkey_auth[More details^].\n\n=== User credentials\n\nNATS server supports decentralized authentication based on JSON Web Tokens (JWT). Clients need an https://docs.nats.io/nats-server/configuration/securing_nats/jwt#json-web-tokens[user JWT^]\nand a corresponding https://docs.nats.io/running-a-nats-service/configuration/securing_nats/auth_intro/nkey_auth[NKey secret^] when connecting to a server\nwhich is configured to use this authentication scheme.\n\nThe `user_credentials_file` field should point to a file containing both the private key and the JWT and can be\ngenerated with the https://docs.nats.io/nats-tools/nsc[nsc tool^].\n\nAlternatively, the `user_jwt` field can contain a plain text JWT and the `user_nkey_seed`can contain\nthe plain text NKey Seed.\n\nhttps://docs.nats.io/using-nats/developer/connecting/creds[More details^]."
      },
      {
        "name": "nats_jetstream",
        "type": "outputs",
        "status": "stable",
        "version": "3.46.0",
        "description": "== Connection name\n\nWhen monitoring and managing a production NATS system, it is often useful to\nknow which connection a message was send/received from. This can be achieved by\nsetting the connection name option when creating a NATS connection.\n\nRedpanda Connect will automatically set the connection name based off the label of the given\nNATS component, so that monitoring tools between NATS and Redpanda Connect can stay in sync.\n\n\n== Authentication\n\nThere are several components within Redpanda Connect which uses NATS services. You will find that each of these components\nsupport optional advanced authentication parameters for https://docs.nats.io/nats-server/configuration/securing_nats/auth_intro/nkey_auth[NKeys^]\nand https://docs.nats.io/using-nats/developer/connecting/creds[User Credentials^].\n\nSee an https://docs.nats.io/running-a-nats-service/nats_admin/security/jwt[in-depth tutorial^].\n\n=== NKey file\n\nThe NATS server can use these NKeys in several ways for authentication. The simplest is for the server to be configured\nwith a list of known public keys and for the clients to respond to the challenge by signing it with its private NKey\nconfigured in the `nkey_file` or `nkey` field.\n\nhttps://docs.nats.io/running-a-nats-service/configuration/securing_nats/auth_intro/nkey_auth[More details^].\n\n=== User credentials\n\nNATS server supports decentralized authentication based on JSON Web Tokens (JWT). Clients need an https://docs.nats.io/nats-server/configuration/securing_nats/jwt#json-web-tokens[user JWT^]\nand a corresponding https://docs.nats.io/running-a-nats-service/configuration/securing_nats/auth_intro/nkey_auth[NKey secret^] when connecting to a server\nwhich is configured to use this authentication scheme.\n\nThe `user_credentials_file` field should point to a file containing both the private key and the JWT and can be\ngenerated with the https://docs.nats.io/nats-tools/nsc[nsc tool^].\n\nAlternatively, the `user_jwt` field can contain a plain text JWT and the `user_nkey_seed`can contain\nthe plain text NKey Seed.\n\nhttps://docs.nats.io/using-nats/developer/connecting/creds[More details^]."
      },
      {
        "name": "nats_kv",
        "type": "outputs",
        "status": "beta",
        "version": "4.12.0",
        "description": "\nThe field `key` supports\nxref:configuration:interpolation.adoc#bloblang-queries[interpolation functions], allowing\nyou to create a unique key for each message.\n\n== Connection name\n\nWhen monitoring and managing a production NATS system, it is often useful to\nknow which connection a message was send/received from. This can be achieved by\nsetting the connection name option when creating a NATS connection.\n\nRedpanda Connect will automatically set the connection name based off the label of the given\nNATS component, so that monitoring tools between NATS and Redpanda Connect can stay in sync.\n\n\n== Authentication\n\nThere are several components within Redpanda Connect which uses NATS services. You will find that each of these components\nsupport optional advanced authentication parameters for https://docs.nats.io/nats-server/configuration/securing_nats/auth_intro/nkey_auth[NKeys^]\nand https://docs.nats.io/using-nats/developer/connecting/creds[User Credentials^].\n\nSee an https://docs.nats.io/running-a-nats-service/nats_admin/security/jwt[in-depth tutorial^].\n\n=== NKey file\n\nThe NATS server can use these NKeys in several ways for authentication. The simplest is for the server to be configured\nwith a list of known public keys and for the clients to respond to the challenge by signing it with its private NKey\nconfigured in the `nkey_file` or `nkey` field.\n\nhttps://docs.nats.io/running-a-nats-service/configuration/securing_nats/auth_intro/nkey_auth[More details^].\n\n=== User credentials\n\nNATS server supports decentralized authentication based on JSON Web Tokens (JWT). Clients need an https://docs.nats.io/nats-server/configuration/securing_nats/jwt#json-web-tokens[user JWT^]\nand a corresponding https://docs.nats.io/running-a-nats-service/configuration/securing_nats/auth_intro/nkey_auth[NKey secret^] when connecting to a server\nwhich is configured to use this authentication scheme.\n\nThe `user_credentials_file` field should point to a file containing both the private key and the JWT and can be\ngenerated with the https://docs.nats.io/nats-tools/nsc[nsc tool^].\n\nAlternatively, the `user_jwt` field can contain a plain text JWT and the `user_nkey_seed`can contain\nthe plain text NKey Seed.\n\nhttps://docs.nats.io/using-nats/developer/connecting/creds[More details^]."
      },
      {
        "name": "nats_stream",
        "type": "outputs",
        "status": "stable",
        "version": "",
        "description": "\n[CAUTION]\n.Deprecation notice\n====\nThe NATS Streaming Server is being deprecated. Critical bug fixes and security fixes will be applied until June of 2023. NATS-enabled applications requiring persistence should use https://docs.nats.io/nats-concepts/jetstream[JetStream^].\n====\n\n\n\n== Authentication\n\nThere are several components within Redpanda Connect which uses NATS services. You will find that each of these components\nsupport optional advanced authentication parameters for https://docs.nats.io/nats-server/configuration/securing_nats/auth_intro/nkey_auth[NKeys^]\nand https://docs.nats.io/using-nats/developer/connecting/creds[User Credentials^].\n\nSee an https://docs.nats.io/running-a-nats-service/nats_admin/security/jwt[in-depth tutorial^].\n\n=== NKey file\n\nThe NATS server can use these NKeys in several ways for authentication. The simplest is for the server to be configured\nwith a list of known public keys and for the clients to respond to the challenge by signing it with its private NKey\nconfigured in the `nkey_file` or `nkey` field.\n\nhttps://docs.nats.io/running-a-nats-service/configuration/securing_nats/auth_intro/nkey_auth[More details^].\n\n=== User credentials\n\nNATS server supports decentralized authentication based on JSON Web Tokens (JWT). Clients need an https://docs.nats.io/nats-server/configuration/securing_nats/jwt#json-web-tokens[user JWT^]\nand a corresponding https://docs.nats.io/running-a-nats-service/configuration/securing_nats/auth_intro/nkey_auth[NKey secret^] when connecting to a server\nwhich is configured to use this authentication scheme.\n\nThe `user_credentials_file` field should point to a file containing both the private key and the JWT and can be\ngenerated with the https://docs.nats.io/nats-tools/nsc[nsc tool^].\n\nAlternatively, the `user_jwt` field can contain a plain text JWT and the `user_nkey_seed`can contain\nthe plain text NKey Seed.\n\nhttps://docs.nats.io/using-nats/developer/connecting/creds[More details^].\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`."
      },
      {
        "name": "nsq",
        "type": "outputs",
        "status": "stable",
        "version": "",
        "description": "The `topic` field can be dynamically set using function interpolations described xref:configuration:interpolation.adoc#bloblang-queries[here]. When sending batched messages these interpolations are performed per message part.\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`."
      },
      {
        "name": "ockam_kafka",
        "type": "outputs",
        "status": "experimental",
        "version": "",
        "description": ""
      },
      {
        "name": "opensearch",
        "type": "outputs",
        "status": "stable",
        "version": "",
        "description": "\nBoth the `id` and `index` fields can be dynamically set using function interpolations described xref:configuration:interpolation.adoc#bloblang-queries[here]. When sending batched messages these interpolations are performed per message part.\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`.\n\nThis output benefits from sending messages as a batch for improved performance. Batches can be formed at both the input and output level. You can find out more xref:configuration:batching.adoc[in this doc]."
      },
      {
        "name": "pinecone",
        "type": "outputs",
        "status": "experimental",
        "version": "4.31.0",
        "description": "\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`.\n\nThis output benefits from sending messages as a batch for improved performance. Batches can be formed at both the input and output level. You can find out more xref:configuration:batching.adoc[in this doc]."
      },
      {
        "name": "pulsar",
        "type": "outputs",
        "status": "experimental",
        "version": "3.43.0",
        "description": ""
      },
      {
        "name": "pusher",
        "type": "outputs",
        "status": "experimental",
        "version": "4.3.0",
        "description": ""
      },
      {
        "name": "qdrant",
        "type": "outputs",
        "status": "experimental",
        "version": "4.33.0",
        "description": "\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`.\n\nThis output benefits from sending messages as a batch for improved performance. Batches can be formed at both the input and output level. You can find out more xref:configuration:batching.adoc[in this doc]."
      },
      {
        "name": "questdb",
        "type": "outputs",
        "status": "experimental",
        "version": "",
        "description": "Important: We recommend that the dedupe feature is enabled on the QuestDB server. Please visit https://questdb.io/docs/ for more information about deploying, configuring, and using QuestDB.\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`.\n\nThis output benefits from sending messages as a batch for improved performance. Batches can be formed at both the input and output level. You can find out more xref:configuration:batching.adoc[in this doc]."
      },
      {
        "name": "redis_hash",
        "type": "outputs",
        "status": "stable",
        "version": "",
        "description": "\nThe field `key` supports xref:configuration:interpolation.adoc#bloblang-queries[interpolation functions], allowing you to create a unique key for each message.\n\nThe field `fields` allows you to specify an explicit map of field names to interpolated values, also evaluated per message of a batch:\n\n```yaml\noutput:\n  redis_hash:\n    url: tcp://localhost:6379\n    key: ${!json(\"id\")}\n    fields:\n      topic: ${!meta(\"kafka_topic\")}\n      partition: ${!meta(\"kafka_partition\")}\n      content: ${!json(\"document.text\")}\n```\n\nIf the field `walk_metadata` is set to `true` then Redpanda Connect will walk all metadata fields of messages and add them to the list of hash fields to set.\n\nIf the field `walk_json_object` is set to `true` then Redpanda Connect will walk each message as a JSON object, extracting keys and the string representation of their value and adds them to the list of hash fields to set.\n\nThe order of hash field extraction is as follows:\n\n1. Metadata (if enabled)\n2. JSON object (if enabled)\n3. Explicit fields\n\nWhere latter stages will overwrite matching field names of a former stage.\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`."
      },
      {
        "name": "redis_list",
        "type": "outputs",
        "status": "stable",
        "version": "",
        "description": "The field `key` supports xref:configuration:interpolation.adoc#bloblang-queries[interpolation functions], allowing you to create a unique key for each message.\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`.\n\nThis output benefits from sending messages as a batch for improved performance. Batches can be formed at both the input and output level. You can find out more xref:configuration:batching.adoc[in this doc]."
      },
      {
        "name": "redis_pubsub",
        "type": "outputs",
        "status": "stable",
        "version": "",
        "description": "\nThis output will interpolate functions within the channel field, you can find a list of functions xref:configuration:interpolation.adoc#bloblang-queries[here].\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`.\n\nThis output benefits from sending messages as a batch for improved performance. Batches can be formed at both the input and output level. You can find out more xref:configuration:batching.adoc[in this doc]."
      },
      {
        "name": "redis_streams",
        "type": "outputs",
        "status": "stable",
        "version": "",
        "description": "\nIt's possible to specify a maximum length of the target stream by setting it to a value greater than 0, in which case this cap is applied only when Redis is able to remove a whole macro node, for efficiency.\n\nRedis stream entries are key/value pairs, as such it is necessary to specify the key to be set to the body of the message. All metadata fields of the message will also be set as key/value pairs, if there is a key collision between a metadata item and the body then the body takes precedence.\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`.\n\nThis output benefits from sending messages as a batch for improved performance. Batches can be formed at both the input and output level. You can find out more xref:configuration:batching.adoc[in this doc]."
      },
      {
        "name": "redpanda",
        "type": "outputs",
        "status": "beta",
        "version": "",
        "description": "\nWrites a batch of messages to Kafka brokers and waits for acknowledgement before propagating it back to the input.\n"
      },
      {
        "name": "redpanda_common",
        "type": "outputs",
        "status": "deprecated",
        "version": "",
        "description": ""
      },
      {
        "name": "redpanda_migrator",
        "type": "outputs",
        "status": "experimental",
        "version": "4.67.0",
        "description": "\nThe `redpanda_migrator` output performs all migration work. \nIt coordinates topics, schema registry, and consumer groups to migrate data from a source Kafka/Redpanda cluster to a destination cluster.\n\n**IMPORTANT:** This output requires a corresponding `redpanda_migrator` input in the same pipeline. \nEach pipeline must have both input and output components configured.\n\n**Multiple migrator pairs:** When using multiple migrator pairs in a single pipeline, \nthe mapping between input and output components is done based on the label field. \nThe label of the input and output must match exactly for proper coordination.\n\n**Performance tuning for high throughput:** For workloads with high message rates or large messages, \nadjust buffer sizes on the paired input component to improve throughput. See the input documentation for details.\n\nWhat gets synchronised:\n\n- Topics\n  - Name resolution with interpolation (default: preserve source name)\n  - Automatic creation with mirrored partition counts\n  - Selectable replication factor (default: inherit from source)\n  - Copy of supported topic configuration keys (serverless-aware subset)\n  - Optional ACL replication with safe transforms:\n    - Excludes `ALLOW WRITE` entries\n    - Downgrades `ALLOW ALL` to `READ`\n    - Preserves resource pattern type and host filters\n\n- Schema Registry\n  - One-shot or periodic syncing\n  - Subject selection via include/exclude regex\n  - Subject renaming with interpolation\n  - Versions: `latest` or `all` (default: `all`)\n  - Optional include of soft-deleted subjects\n  - ID handling: translate IDs (create-or-reuse) or keep fixed IDs and versions\n  - Optional schema normalisation on create\n  - Optional per-subject compatibility propagation when explicitly set on source (global mode is not forced)\n  - Serverless note: schema metadata and rule sets are not copied in serverless mode\n\n- Consumer Groups\n  - Periodic syncing\n  - Group selection via include/exclude regex\n  - Only groups in `Empty` state are migrated (active groups are skipped)\n  - Timestamp-based offset translation (approximate) per partition using previous-record timestamp and `ListOffsetsAfterMilli`\n  - No rewind guarantee: destination offsets are never moved backwards\n  - Commit performed in parallel with per-group metrics\n  - Requires matching partition counts between source and destination topics\n\nHow it runs:\n\n- Topics: synced on demand. The first write triggers discovery and creation; subsequent writes create on first encounter per topic.\n- Schema Registry: one sync at connect, then triggered when topic record has unknown schema; optional background loop controlled by `schema_registry.interval`.\n- Consumer Groups: background loop controlled by `consumer_groups.interval` and filtered by the current topic mappings.\n\nGuarantees:\n\n- Topics are created with the intended partitioning and configured replication factor. Existing topics are respected; partition mismatches are logged and consumer group migration for mismatched topics is skipped.\n- Consumer group offsets are never rewound. Only translated forward positions are committed.\n- ACL replication excludes `ALLOW WRITE` operations and downgrades `ALLOW ALL` to `READ` to avoid unsafe grants.\n\nLimitations and requirements:\n\n- Destination Schema Registry must be in `READWRITE` or `IMPORT` mode.\n- Offset translation is best-effort: if the previous-offset timestamp cannot be read, or no destination offset exists after the timestamp, that partition is skipped.\n- Consumer group migration requires identical partition counts for source and destination topics.\n\nMetrics:\n\nThe component exposes comprehensive metrics for monitoring migration operations:\n\nTopic Migration Metrics:\n- `redpanda_migrator_topics_created_total` (counter): Total number of topics successfully created on the destination cluster\n- `redpanda_migrator_topic_create_errors_total` (counter): Total number of errors encountered when creating topics\n- `redpanda_migrator_topic_create_latency_ns` (timer): Latency in nanoseconds for topic creation operations\n\nSchema Registry Migration Metrics:\n- `redpanda_migrator_sr_schemas_created_total` (counter): Total number of schemas successfully created in the destination schema registry\n- `redpanda_migrator_sr_schema_create_errors_total` (counter): Total number of errors encountered when creating schemas\n- `redpanda_migrator_sr_schema_create_latency_ns` (timer): Latency in nanoseconds for schema creation operations\n- `redpanda_migrator_sr_compatibility_updates_total` (counter): Total number of compatibility level updates applied to subjects\n- `redpanda_migrator_sr_compatibility_update_errors_total` (counter): Total number of errors encountered when updating compatibility levels\n- `redpanda_migrator_sr_compatibility_update_latency_ns` (timer): Latency in nanoseconds for compatibility level update operations\n\nConsumer Group Migration Metrics (with group label):\n- `redpanda_migrator_cg_offsets_translated_total` (counter): Total number of offsets successfully translated per consumer group\n- `redpanda_migrator_cg_offset_translation_errors_total` (counter): Total number of errors encountered when translating offsets per consumer group\n- `redpanda_migrator_cg_offset_translation_latency_ns` (timer): Latency in nanoseconds for offset translation operations per consumer group\n- `redpanda_migrator_cg_offsets_committed_total` (counter): Total number of offsets successfully committed per consumer group\n- `redpanda_migrator_cg_offset_commit_errors_total` (counter): Total number of errors encountered when committing offsets per consumer group\n- `redpanda_migrator_cg_offset_commit_latency_ns` (timer): Latency in nanoseconds for offset commit operations per consumer group\n\nConsumer Lag Metrics (with topic and partition labels):\n- `redpanda_lag` (gauge): Current consumer lag in messages for each topic partition being consumed by the migrator input. This metric shows the difference between the high water mark and the current consumer position, providing visibility into how far behind the consumer is on each partition. The metric includes labels for topic name and partition number to enable per-partition monitoring.\n\nThis component must be paired with the `redpanda_migrator` input in the same pipeline."
      },
      {
        "name": "redpanda_migrator_bundle",
        "type": "outputs",
        "status": "deprecated",
        "version": "",
        "description": "All-in-one output which writes messages and schemas to a Kafka or Redpanda cluster. This output is meant to be used\ntogether with the `redpanda_migrator_bundle` input.\n"
      },
      {
        "name": "reject",
        "type": "outputs",
        "status": "stable",
        "version": "",
        "description": "\nThe routing of messages after this output depends on the type of input it came from. For inputs that support propagating nacks upstream such as AMQP or NATS the message will be nacked. However, for inputs that are sequential such as files or Kafka the messages will simply be reprocessed from scratch.\n\nTo learn when this output could be useful, see [the <<examples>>."
      },
      {
        "name": "reject_errored",
        "type": "outputs",
        "status": "stable",
        "version": "",
        "description": "\nThe routing of messages rejected by this output depends on the type of input it came from. For inputs that support propagating nacks upstream such as AMQP or NATS the message will be nacked. However, for inputs that are sequential such as files or Kafka the messages will simply be reprocessed from scratch."
      },
      {
        "name": "resource",
        "type": "outputs",
        "status": "stable",
        "version": "",
        "description": "Resources allow you to tidy up deeply nested configs. For example, the config:\n\n```yaml\noutput:\n  broker:\n    pattern: fan_out\n    outputs:\n    - kafka:\n        addresses: [ TODO ]\n        topic: foo\n    - gcp_pubsub:\n        project: bar\n        topic: baz\n```\n\nCould also be expressed as:\n\n```yaml\noutput:\n  broker:\n    pattern: fan_out\n    outputs:\n    - resource: foo\n    - resource: bar\n\noutput_resources:\n  - label: foo\n    kafka:\n      addresses: [ TODO ]\n      topic: foo\n\n  - label: bar\n    gcp_pubsub:\n      project: bar\n      topic: baz\n```\n\nYou can find out more about resources in xref:configuration:resources.adoc[]"
      },
      {
        "name": "retry",
        "type": "outputs",
        "status": "stable",
        "version": "",
        "description": "\nAll messages in Redpanda Connect are always retried on an output error, but this would usually involve propagating the error back to the source of the message, whereby it would be reprocessed before reaching the output layer once again.\n\nThis output type is useful whenever we wish to avoid reprocessing a message on the event of a failed send. We might, for example, have a deduplication processor that we want to avoid reapplying to the same message more than once in the pipeline.\n\nRather than retrying the same output you may wish to retry the send using a different output target (a dead letter queue). In which case you should instead use the xref:components:outputs/fallback.adoc[`fallback`] output type."
      },
      {
        "name": "schema_registry",
        "type": "outputs",
        "status": "beta",
        "version": "4.32.2",
        "description": "\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`."
      },
      {
        "name": "sftp",
        "type": "outputs",
        "status": "beta",
        "version": "3.39.0",
        "description": "In order to have a different path for each object you should use function interpolations described xref:configuration:interpolation.adoc#bloblang-queries[here].\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`."
      },
      {
        "name": "slack_post",
        "type": "outputs",
        "status": "experimental",
        "version": "",
        "description": "Post a new message to a Slack channel using https://api.slack.com/methods/chat.postMessage[^chat.postMessage]"
      },
      {
        "name": "slack_reaction",
        "type": "outputs",
        "status": "experimental",
        "version": "",
        "description": "Add or remove an emoji reaction to a Slack message using https://api.slack.com/methods/reactions.add[^reactions.add] and https://api.slack.com/methods/reactions.remove[^reactions.remove]"
      },
      {
        "name": "snowflake_put",
        "type": "outputs",
        "status": "beta",
        "version": "4.0.0",
        "description": "\nIn order to use a different stage and / or Snowpipe for each message, you can use function interpolations as described in\nxref:configuration:interpolation.adoc#bloblang-queries[Bloblang queries]. When using batching, messages are grouped by the calculated\nstage and Snowpipe and are streamed to individual files in their corresponding stage and, optionally, a Snowpipe\n`insertFiles` REST API call will be made for each individual file.\n\n== Credentials\n\nTwo authentication mechanisms are supported:\n\n- User/password\n- Key Pair Authentication\n\n=== User/password\n\nThis is a basic authentication mechanism which allows you to PUT data into a stage. However, it is not compatible with\nSnowpipe.\n\n=== Key pair authentication\n\nThis authentication mechanism allows Snowpipe functionality, but it does require configuring an SSH Private Key\nbeforehand. Please consult the https://docs.snowflake.com/en/user-guide/key-pair-auth.html#configuring-key-pair-authentication[documentation^]\nfor details on how to set it up and assign the Public Key to your user.\n\nNote that the Snowflake documentation https://twitter.com/felipehoffa/status/1560811785606684672[used to suggest^]\nusing this command:\n\n```bash\nopenssl genrsa 2048 | openssl pkcs8 -topk8 -inform PEM -out rsa_key.p8\n```\n\nto generate an encrypted SSH private key. However, in this case, it uses an encryption algorithm called\n`pbeWithMD5AndDES-CBC`, which is part of the PKCS#5 v1.5 and is considered insecure. Due to this, Redpanda Connect does not\nsupport it and, if you wish to use password-protected keys directly, you must use PKCS#5 v2.0 to encrypt them by using\nthe following command (as the current Snowflake docs suggest):\n\n```bash\nopenssl genrsa 2048 | openssl pkcs8 -topk8 -v2 des3 -inform PEM -out rsa_key.p8\n```\n\nIf you have an existing key encrypted with PKCS#5 v1.5, you can re-encrypt it with PKCS#5 v2.0 using this command:\n\n```bash\nopenssl pkcs8 -in rsa_key_original.p8 -topk8 -v2 des3 -out rsa_key.p8\n```\n\nPlease consult the https://linux.die.net/man/1/pkcs8[pkcs8 command documentation^] for details on PKCS#5 algorithms.\n\n== Batching\n\nIt's common to want to upload messages to Snowflake as batched archives. The easiest way to do this is to batch your\nmessages at the output level and join the batch of messages with an\nxref:components:processors/archive.adoc[`archive`] and/or xref:components:processors/compress.adoc[`compress`]\nprocessor.\n\nFor the optimal batch size, please consult the Snowflake https://docs.snowflake.com/en/user-guide/data-load-considerations-prepare.html[documentation^].\n\n== Snowpipe\n\nGiven a table called `BENTHOS_TBL` with one column of type `variant`:\n\n```sql\nCREATE OR REPLACE TABLE BENTHOS_DB.PUBLIC.BENTHOS_TBL(RECORD variant)\n```\n\nand the following `BENTHOS_PIPE` Snowpipe:\n\n```sql\nCREATE OR REPLACE PIPE BENTHOS_DB.PUBLIC.BENTHOS_PIPE AUTO_INGEST = FALSE AS COPY INTO BENTHOS_DB.PUBLIC.BENTHOS_TBL FROM (SELECT * FROM @%BENTHOS_TBL) FILE_FORMAT = (TYPE = JSON COMPRESSION = AUTO)\n```\n\nyou can configure Redpanda Connect to use the implicit table stage `@%BENTHOS_TBL` as the `stage` and\n`BENTHOS_PIPE` as the `snowpipe`. In this case, you must set `compression` to `AUTO` and, if\nusing message batching, you'll need to configure an xref:components:processors/archive.adoc[`archive`] processor\nwith the `concatenate` format. Since the `compression` is set to `AUTO`, the\nhttps://github.com/snowflakedb/gosnowflake[gosnowflake^] client library will compress the messages automatically so you\ndon't need to add a xref:components:processors/compress.adoc[`compress`] processor for message batches.\n\nIf you add `STRIP_OUTER_ARRAY = TRUE` in your Snowpipe `FILE_FORMAT`\ndefinition, then you must use `json_array` instead of `concatenate` as the archive processor format.\n\nNOTE: Only Snowpipes with `FILE_FORMAT` `TYPE` `JSON` are currently supported.\n\n== Snowpipe troubleshooting\n\nSnowpipe https://docs.snowflake.com/en/user-guide/data-load-snowpipe-rest-apis.html[provides^] the `insertReport`\nand `loadHistoryScan` REST API endpoints which can be used to get information about recent Snowpipe calls. In\norder to query them, you'll first need to generate a valid JWT token for your Snowflake account. There are two methods\nfor doing so:\n\n- Using the `snowsql` https://docs.snowflake.com/en/user-guide/snowsql.html[utility^]:\n\n```bash\nsnowsql --private-key-path rsa_key.p8 --generate-jwt -a <account> -u <user>\n```\n\n- Using the Python `sql-api-generate-jwt` https://docs.snowflake.com/en/developer-guide/sql-api/authenticating.html#generating-a-jwt-in-python[utility^]:\n\n```bash\npython3 sql-api-generate-jwt.py --private_key_file_path=rsa_key.p8 --account=<account> --user=<user>\n```\n\nOnce you successfully generate a JWT token and store it into the `JWT_TOKEN` environment variable, then you can,\nfor example, query the `insertReport` endpoint using `curl`:\n\n```bash\ncurl -H \"Authorization: Bearer ${JWT_TOKEN}\" \"https://<account>.snowflakecomputing.com/v1/data/pipes/<database>.<schema>.<snowpipe>/insertReport\"\n```\n\nIf you need to pass in a valid `requestId` to any of these Snowpipe REST API endpoints, you can set a\nxref:guides:bloblang/functions.adoc#uuid_v4[uuid_v4()] string in a metadata field called\n`request_id`, log it via the xref:components:processors/log.adoc[`log`] processor and\nthen configure `request_id: ${ @request_id }` ). Alternatively, you can xref:components:logger/about.adoc[enable debug logging]\n and Redpanda Connect will print the Request IDs that it sends to Snowpipe.\n\n== General troubleshooting\n\nThe underlying https://github.com/snowflakedb/gosnowflake[`gosnowflake` driver^] requires write access to\nthe default directory to use for temporary files. Please consult the https://pkg.go.dev/os#TempDir[`os.TempDir`^]\ndocs for details on how to change this directory via environment variables.\n\nA silent failure can occur due to https://github.com/snowflakedb/gosnowflake/issues/701[this issue^], where the\nunderlying https://github.com/snowflakedb/gosnowflake[`gosnowflake` driver^] doesn't return an error and doesn't\nlog a failure if it can't figure out the current username. One way to trigger this behavior is by running Redpanda Connect in a\nDocker container with a non-existent user ID (such as `--user 1000:1000`).\n\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`.\n\nThis output benefits from sending messages as a batch for improved performance. Batches can be formed at both the input and output level. You can find out more xref:configuration:batching.adoc[in this doc]."
      },
      {
        "name": "snowflake_streaming",
        "type": "outputs",
        "status": "experimental",
        "version": "4.39.0",
        "description": "\nIngest data into Snowflake using Snowpipe Streaming.\n\n[%header,format=dsv]\n|===\nSnowflake column type:Allowed format in Redpanda Connect\nCHAR, VARCHAR:string\nBINARY:[]byte\nNUMBER:any numeric type, string\nFLOAT:any numeric type\nBOOLEAN:bool,any numeric type,string parsable according to `strconv.ParseBool`\nTIME,DATE,TIMESTAMP:unix or RFC 3339 with nanoseconds timestamps\nVARIANT,ARRAY,OBJECT:any data type is converted into JSON\nGEOGRAPHY,GEOMETRY: Not supported\n|===\n\nFor TIMESTAMP, TIME and DATE columns, you can parse different string formats using a bloblang `mapping`.\n\nAuthentication can be configured using a https://docs.snowflake.com/en/user-guide/key-pair-auth[RSA Key Pair^].\n\nThere are https://docs.snowflake.com/en/user-guide/data-load-snowpipe-streaming-overview#limitations[limitations^] of what data types can be loaded into Snowflake using this method.\n\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`.\n\nThis output benefits from sending messages as a batch for improved performance. Batches can be formed at both the input and output level. You can find out more xref:configuration:batching.adoc[in this doc].\n\nIt is recommended that each batches results in at least 16MiB of compressed output being written to Snowflake.\nYou can monitor the output batch size using the `snowflake_compressed_output_size_bytes` metric.\n"
      },
      {
        "name": "socket",
        "type": "outputs",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "splunk_hec",
        "type": "outputs",
        "status": "beta",
        "version": "4.30.0",
        "description": "\n\n== Performance\n\nThis output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`.\n\nThis output benefits from sending messages as a batch for improved performance. Batches can be formed at both the input and output level. You can find out more xref:configuration:batching.adoc[in this doc]."
      },
      {
        "name": "sql",
        "type": "outputs",
        "status": "deprecated",
        "version": "3.65.0",
        "description": "\n== Alternatives\n\nFor basic inserts use the xref:components:outputs/sql.adoc[`sql_insert`] output. For more complex queries use the xref:components:outputs/sql_raw.adoc[`sql_raw`] output."
      },
      {
        "name": "sql_insert",
        "type": "outputs",
        "status": "stable",
        "version": "3.59.0",
        "description": ""
      },
      {
        "name": "sql_raw",
        "type": "outputs",
        "status": "stable",
        "version": "3.65.0",
        "description": ""
      },
      {
        "name": "stdout",
        "type": "outputs",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "subprocess",
        "type": "outputs",
        "status": "beta",
        "version": "",
        "description": "\nMessages are written according to a specified codec. The process is expected to terminate gracefully when stdin is closed.\n\nIf the subprocess exits unexpectedly then Redpanda Connect will log anything printed to stderr and will log the exit code, and will attempt to execute the command again until success.\n\nThe execution environment of the subprocess is the same as the Redpanda Connect instance, including environment variables and the current working directory."
      },
      {
        "name": "switch",
        "type": "outputs",
        "status": "stable",
        "version": "",
        "description": "Messages that do not pass the check of a single output case are effectively dropped. In order to prevent this outcome set the field <<strict_mode, `strict_mode`>> to `true`, in which case messages that do not pass at least one case are considered failed and will be nacked and/or reprocessed depending on your input."
      },
      {
        "name": "sync_response",
        "type": "outputs",
        "status": "stable",
        "version": "",
        "description": "\nFor most inputs this mechanism is ignored entirely, in which case the sync response is dropped without penalty. It is therefore safe to use this output even when combining input types that might not have support for sync responses. An example of an input able to utilize this is the `http_server`.\n\nIt is safe to combine this output with others using broker types. For example, with the `http_server` input we could send the payload to a Kafka topic and also send a modified payload back with:\n\n```yaml\ninput:\n  http_server:\n    path: /post\noutput:\n  broker:\n    pattern: fan_out\n    outputs:\n      - kafka:\n          addresses: [ TODO:9092 ]\n          topic: foo_topic\n      - sync_response: {}\n        processors:\n          - mapping: 'root = content().uppercase()'\n```\n\nUsing the above example and posting the message 'hello world' to the endpoint `/post` Redpanda Connect would send it unchanged to the topic `foo_topic` and also respond with 'HELLO WORLD'.\n\nFor more information please read xref:guides:sync_responses.adoc[synchronous responses]."
      },
      {
        "name": "websocket",
        "type": "outputs",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "archive",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": "\nSome archive formats (such as tar, zip) treat each archive item (message part) as a file with a path. Since message parts only contain raw data a unique path must be generated for each part. This can be done by using function interpolations on the 'path' field as described in xref:configuration:interpolation.adoc#bloblang-queries[Bloblang queries]. For types that aren't file based (such as binary) the file field is ignored.\n\nThe resulting archived message adopts the metadata of the _first_ message part of the batch.\n\nThe functionality of this processor depends on being applied across messages that are batched. You can find out more about batching xref:configuration:batching.adoc[in this doc]."
      },
      {
        "name": "avro",
        "type": "processors",
        "status": "beta",
        "version": "",
        "description": "\nWARNING: If you are consuming or generating messages using a schema registry service then it is likely this processor will fail as those services require messages to be prefixed with the identifier of the schema version being used. Instead, try the xref:components:processors/schema_registry_encode.adoc[`schema_registry_encode`] and xref:components:processors/schema_registry_decode.adoc[`schema_registry_decode`] processors.\n\n== Operators\n\n=== `to_json`\n\nConverts Avro documents into a JSON structure. This makes it easier to\nmanipulate the contents of the document within Benthos. The encoding field\nspecifies how the source documents are encoded.\n\n=== `from_json`\n\nAttempts to convert JSON documents into Avro documents according to the\nspecified encoding."
      },
      {
        "name": "awk",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": "\nWorks by feeding message contents as the program input based on a chosen <<codecs,codec>> and replaces the contents of each message with the result. If the result is empty (nothing is printed by the program) then the original message contents remain unchanged.\n\nComes with a wide range of <<awk-functions,custom functions>> for accessing message metadata, json fields, printing logs, etc. These functions can be overridden by functions within the program.\n\nCheck out the <<examples,examples section>> in order to see how this processor can be used.\n\nThis processor uses https://github.com/benhoyt/goawk[GoAWK^], in order to understand the differences in how the program works you can read more about it in https://github.com/benhoyt/goawk#differences-from-awk[goawk.differences^]."
      },
      {
        "name": "aws_bedrock_chat",
        "type": "processors",
        "status": "experimental",
        "version": "4.34.0",
        "description": "This processor sends prompts to your chosen large language model (LLM) and generates text from the responses, using the AWS Bedrock API.\nFor more information, see the https://docs.aws.amazon.com/bedrock/latest/userguide[AWS Bedrock documentation^]."
      },
      {
        "name": "aws_bedrock_embeddings",
        "type": "processors",
        "status": "experimental",
        "version": "4.37.0",
        "description": "This processor sends text to your chosen large language model (LLM) and computes vector embeddings, using the AWS Bedrock API.\nFor more information, see the https://docs.aws.amazon.com/bedrock/latest/userguide[AWS Bedrock documentation^]."
      },
      {
        "name": "aws_dynamodb_partiql",
        "type": "processors",
        "status": "experimental",
        "version": "3.48.0",
        "description": "Both writes or reads are supported, when the query is a read the contents of the message will be replaced with the result. This processor is more efficient when messages are pre-batched as the whole batch will be executed in a single call."
      },
      {
        "name": "aws_lambda",
        "type": "processors",
        "status": "stable",
        "version": "3.36.0",
        "description": "The `rate_limit` field can be used to specify a rate limit xref:components:rate_limits/about.adoc[resource] to cap the rate of requests across parallel components service wide.\n\nIn order to map or encode the payload to a specific request body, and map the response back into the original payload instead of replacing it entirely, you can use the xref:components:processors/branch.adoc[`branch` processor].\n\n== Error handling\n\nWhen Redpanda Connect is unable to connect to the AWS endpoint or is otherwise unable to invoke the target lambda function it will retry the request according to the configured number of retries. Once these attempts have been exhausted the failed message will continue through the pipeline with it's contents unchanged, but flagged as having failed, allowing you to use xref:configuration:error_handling.adoc[standard processor error handling patterns].\n\nHowever, if the invocation of the function is successful but the function itself throws an error, then the message will have it's contents updated with a JSON payload describing the reason for the failure, and a metadata field `lambda_function_error` will be added to the message allowing you to detect and handle function errors with a xref:components:processors/branch.adoc[`branch`]:\n\n```yaml\npipeline:\n  processors:\n    - branch:\n        processors:\n          - aws_lambda:\n              function: foo\n        result_map: |\n          root = if meta().exists(\"lambda_function_error\") {\n            throw(\"Invocation failed due to %v: %v\".format(this.errorType, this.errorMessage))\n          } else {\n            this\n          }\noutput:\n  switch:\n    retry_until_success: false\n    cases:\n      - check: errored()\n        output:\n          reject: ${! error() }\n      - output:\n          resource: somewhere_else\n```\n\n== Credentials\n\nBy default Redpanda Connect will use a shared credentials file when connecting to AWS services. It's also possible to set them explicitly at the component level, allowing you to transfer data across accounts. You can find out more in xref:guides:cloud/aws.adoc[]."
      },
      {
        "name": "azure_cosmosdb",
        "type": "processors",
        "status": "experimental",
        "version": "v4.25.0",
        "description": "\nWhen creating documents, each message must have the `id` property (case-sensitive) set (or use `auto_id: true`). It is the unique name that identifies the document, that is, no two documents share the same `id` within a logical partition. The `id` field must not exceed 255 characters. https://learn.microsoft.com/en-us/rest/api/cosmos-db/documents[See details^].\n\nThe `partition_keys` field must resolve to the same value(s) across the entire message batch.\n\n\n== Credentials\n\nYou can use one of the following authentication mechanisms:\n\n- Set the `endpoint` field and the `account_key` field\n- Set only the `endpoint` field to use https://pkg.go.dev/github.com/Azure/azure-sdk-for-go/sdk/azidentity#DefaultAzureCredential[DefaultAzureCredential^]\n- Set the `connection_string` field\n\n\n== Metadata\n\nThis component adds the following metadata fields to each message:\n```\n- activity_id\n- request_charge\n```\n\nYou can access these metadata fields using xref:configuration:interpolation.adoc#bloblang-queries[function interpolation].\n\n\n== Batching\n\nCosmosDB limits the maximum batch size to 100 messages and the payload must not exceed 2MB (https://learn.microsoft.com/en-us/azure/cosmos-db/concepts-limits#per-request-limits[details here^]).\n"
      },
      {
        "name": "benchmark",
        "type": "processors",
        "status": "experimental",
        "version": "",
        "description": "Logs messages per second and bytes per second of messages that are processed at a regular interval. A summary of the amount of messages processed over the entire lifetime of the processor will also be printed when the processor shuts down.\n\nThe following metrics are exposed:\n- benchmark_messages_per_second (gauge): The current throughput in messages per second\n- benchmark_messages_total (counter): The total number of messages processed\n- benchmark_bytes_per_second (gauge): The current throughput in bytes per second\n- benchmark_bytes_total (counter): The total number of bytes processed"
      },
      {
        "name": "bloblang",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": "\nBloblang is a powerful language that enables a wide range of mapping, transformation and filtering tasks. For more information see xref:guides:bloblang/about.adoc[].\n\nIf your mapping is large and you'd prefer for it to live in a separate file then you can execute a mapping directly from a file with the expression `from \"<path>\"`, where the path must be absolute, or relative from the location that Redpanda Connect is executed from.\n\n== Component rename\n\nThis processor was recently renamed to the xref:components:processors/mapping.adoc[`mapping` processor] in order to make the purpose of the processor more prominent. It is still valid to use the existing `bloblang` name but eventually it will be deprecated and replaced by the new name in example configs."
      },
      {
        "name": "bounds_check",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "branch",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": "\nThis is useful for preserving the original message contents when using processors that would otherwise replace the entire contents.\n\n== Metadata\n\nMetadata fields that are added to messages during branch processing will not be automatically copied into the resulting message. In order to do this you should explicitly declare in your `result_map` either a wholesale copy with `meta = metadata()`, or selective copies with `meta foo = metadata(\"bar\")` and so on. It is also possible to reference the metadata of the origin message in the `result_map` using the xref:guides:bloblang/about.adoc#metadata[`@` operator].\n\n== Error handling\n\nIf the `request_map` fails the child processors will not be executed. If the child processors themselves result in an (uncaught) error then the `result_map` will not be executed. If the `result_map` fails the message will remain unchanged. Under any of these conditions standard xref:configuration:error_handling.adoc[error handling methods] can be used in order to filter, DLQ or recover the failed messages.\n\n== Conditional branching\n\nIf the root of your request map is set to `deleted()` then the branch processors are skipped for the given message, this allows you to conditionally branch messages."
      },
      {
        "name": "cache",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": "\nFor use cases where you wish to cache the result of processors consider using the xref:components:processors/cached.adoc[`cached` processor] instead.\n\nThis processor will interpolate functions within the `key` and `value` fields individually for each message. This allows you to specify dynamic keys and values based on the contents of the message payloads and metadata. You can find a list of functions in xref:configuration:interpolation.adoc#bloblang-queries[Bloblang queries]."
      },
      {
        "name": "cached",
        "type": "processors",
        "status": "experimental",
        "version": "4.3.0",
        "description": "The format of the data when stored within the cache is a custom and versioned schema chosen to balance performance and storage space. It is therefore not possible to point this processor to a cache that is pre-populated with data that this processor has not created itself."
      },
      {
        "name": "catch",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": "\nBehaves similarly to the xref:components:processors/for_each.adoc[`for_each`] processor, where a list of child processors are applied to individual messages of a batch. However, processors are only applied to messages that failed a processing step prior to the catch.\n\nFor example, with the following config:\n\n```yaml\npipeline:\n  processors:\n    - resource: foo\n    - catch:\n      - resource: bar\n      - resource: baz\n```\n\nIf the processor `foo` fails for a particular message, that message will be fed into the processors `bar` and `baz`. Messages that do not fail for the processor `foo` will skip these processors.\n\nWhen messages leave the catch block their fail flags are cleared. This processor is useful for when it's possible to recover failed messages, or when special actions (such as logging/metrics) are required before dropping them.\n\nMore information about error handling can be found in xref:configuration:error_handling.adoc[]."
      },
      {
        "name": "cohere_chat",
        "type": "processors",
        "status": "experimental",
        "version": "4.37.0",
        "description": "\nThis processor sends the contents of user prompts to the Cohere API, which generates responses. By default, the processor submits the entire payload of each message as a string, unless you use the `prompt` configuration field to customize it.\n\nTo learn more about chat completion, see the https://docs.cohere.com/docs/chat-api[Cohere API documentation^]."
      },
      {
        "name": "cohere_embeddings",
        "type": "processors",
        "status": "experimental",
        "version": "4.37.0",
        "description": "\nThis processor sends text strings to the Cohere API, which generates vector embeddings. By default, the processor submits the entire payload of each message as a string, unless you use the `text_mapping` configuration field to customize it.\n\nTo learn more about vector embeddings, see the https://docs.cohere.com/docs/embeddings[Cohere API documentation^]."
      },
      {
        "name": "cohere_rerank",
        "type": "processors",
        "status": "experimental",
        "version": "4.37.0",
        "description": "\nThis processor sends document strings to the Cohere API, which reranks them based on the relevance to the query.\n\nTo learn more about reranking, see the https://docs.cohere.com/docs/rerank-2[Cohere API documentation^].\n\nThe output of this processor is an array of objects, each containing a \"document\" field with the original document content, a \"relevance_score\" field indicating how relevant it is to the query, and an index field that refers to the document's position within the input documents array. The objects are ordered by their relevance score (highest first).\n\n\t\t"
      },
      {
        "name": "command",
        "type": "processors",
        "status": "experimental",
        "version": "4.21.0",
        "description": "\nThe specified command is executed for each message processed, with the raw bytes of the message being fed into the stdin of the command process, and the resulting message having its contents replaced with the stdout of it.\n\n== Performance\n\nSince this processor executes a new process for each message performance will likely be an issue for high throughput streams. If this is the case then consider using the xref:components:processors/subprocess.adoc[`subprocess` processor] instead as it keeps the underlying process alive long term and uses codecs to insert and extract inputs and outputs to it via stdin/stdout.\n\n== Error handling\n\nIf a non-zero error code is returned by the command then an error containing the entirety of stderr (or a generic message if nothing is written) is set on the message. These failed messages will continue through the pipeline unchanged, but can be dropped or placed in a dead letter queue according to your config, you can read about xref:configuration:error_handling.adoc[these patterns].\n\nIf the command is successful but stderr is written to then a metadata field `command_stderr` is populated with its contents.\n"
      },
      {
        "name": "compress",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": "The 'level' field might not apply to all algorithms."
      },
      {
        "name": "couchbase",
        "type": "processors",
        "status": "experimental",
        "version": "4.11.0",
        "description": "When inserting, replacing or upserting documents, each must have the `content` property set."
      },
      {
        "name": "crash",
        "type": "processors",
        "status": "beta",
        "version": "",
        "description": ""
      },
      {
        "name": "decompress",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "dedupe",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": "\nCaches must be configured as resources, for more information check out the xref:components:caches/about.adoc[cache documentation].\n\nWhen using this processor with an output target that might fail you should always wrap the output within an indefinite xref:components:outputs/retry.adoc[`retry`] block. This ensures that during outages your messages aren't reprocessed after failures, which would result in messages being dropped.\n\n== Batch deduplication\n\nThis processor enacts on individual messages only, in order to perform a deduplication on behalf of a batch (or window) of messages instead use the xref:components:processors/cache.adoc#examples[`cache` processor].\n\n== Delivery guarantees\n\nPerforming deduplication on a stream using a distributed cache voids any at-least-once guarantees that it previously had. This is because the cache will preserve message signatures even if the message fails to leave the Redpanda Connect pipeline, which would cause message loss in the event of an outage at the output sink followed by a restart of the Redpanda Connect instance (or a server crash, etc).\n\nThis problem can be mitigated by using an in-memory cache and distributing messages to horizontally scaled Redpanda Connect pipelines partitioned by the deduplication key. However, in situations where at-least-once delivery guarantees are important it is worth avoiding deduplication in favour of implement idempotent behavior at the edge of your stream pipelines."
      },
      {
        "name": "for_each",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": "\nThis is useful for forcing batch wide processors such as xref:components:processors/dedupe.adoc[`dedupe`] or interpolations such as the `value` field of the `metadata` processor to execute on individual message parts of a batch instead.\n\nPlease note that most processors already process per message of a batch, and this processor is not needed in those cases."
      },
      {
        "name": "gcp_bigquery_select",
        "type": "processors",
        "status": "experimental",
        "version": "3.64.0",
        "description": ""
      },
      {
        "name": "gcp_vertex_ai_chat",
        "type": "processors",
        "status": "experimental",
        "version": "4.34.0",
        "description": "This processor sends prompts to your chosen large language model (LLM) and generates text from the responses, using the Vertex AI API.\n\nFor more information, see the https://cloud.google.com/vertex-ai/docs[Vertex AI documentation^]."
      },
      {
        "name": "gcp_vertex_ai_embeddings",
        "type": "processors",
        "status": "experimental",
        "version": "4.37.0",
        "description": "This processor sends text strings to the Vertex AI API, which generates vector embeddings. By default, the processor submits the entire payload of each message as a string, unless you use the `text` configuration field to customize it.\n\nFor more information, see the https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings[Vertex AI documentation^]."
      },
      {
        "name": "google_drive_download",
        "type": "processors",
        "status": "experimental",
        "version": "",
        "description": "\nCan download a file from Google Drive based on a file ID.\n== Authentication\nBy default, this connector will use Google Application Default Credentials (ADC) to authenticate with Google APIs.\n\nTo use this mechanism locally, the following gcloud commands can be used:\n\n\t# Login for the application default credentials and add scopes for readonly drive access\n\tgcloud auth application-default login --scopes='openid,https://www.googleapis.com/auth/userinfo.email,https://www.googleapis.com/auth/cloud-platform,https://www.googleapis.com/auth/drive.readonly'\n\t# When logging in with a user account, you may need to set the quota project for the application default credentials\n\tgcloud auth application-default set-quota-project <project-id>\n\nOtherwise if using a service account, you can create a JSON key for the service account and set it in the `credentials_json` field.\nIn order for a service account to access files in Google Drive either files need to be explicitly shared with the service account email, otherwise https://support.google.com/a/answer/162106[^domain wide delegation] can be used to share all files within a Google Workspace.\n"
      },
      {
        "name": "google_drive_list_labels",
        "type": "processors",
        "status": "experimental",
        "version": "",
        "description": "\nCan list all labels from Google Drive.\n\t\t== Authentication\nBy default, this connector will use Google Application Default Credentials (ADC) to authenticate with Google APIs.\n\nTo use this mechanism locally, the following gcloud commands can be used:\n\n\t# Login for the application default credentials and add scopes for readonly drive access\n\tgcloud auth application-default login --scopes='openid,https://www.googleapis.com/auth/userinfo.email,https://www.googleapis.com/auth/cloud-platform,https://www.googleapis.com/auth/drive.labels.readonly'\n\t# When logging in with a user account, you may need to set the quota project for the application default credentials\n\tgcloud auth application-default set-quota-project <project-id>\n\nOtherwise if using a service account, you can create a JSON key for the service account and set it in the `credentials_json` field.\nIn order for a service account to access files in Google Drive either files need to be explicitly shared with the service account email, otherwise https://support.google.com/a/answer/162106[^domain wide delegation] can be used to share all files within a Google Workspace.\n"
      },
      {
        "name": "google_drive_search",
        "type": "processors",
        "status": "experimental",
        "version": "",
        "description": "\nThis processor searches for files in Google Drive using the provided query.\n\nSearch results are emitted as message batch, where each message is a https://developers.google.com/workspace/drive/api/reference/rest/v3/files#File[^Google Drive File]\n\n== Authentication\nBy default, this connector will use Google Application Default Credentials (ADC) to authenticate with Google APIs.\n\nTo use this mechanism locally, the following gcloud commands can be used:\n\n\t# Login for the application default credentials and add scopes for readonly drive access\n\tgcloud auth application-default login --scopes='openid,https://www.googleapis.com/auth/userinfo.email,https://www.googleapis.com/auth/cloud-platform,https://www.googleapis.com/auth/drive.readonly'\n\t# When logging in with a user account, you may need to set the quota project for the application default credentials\n\tgcloud auth application-default set-quota-project <project-id>\n\nOtherwise if using a service account, you can create a JSON key for the service account and set it in the `credentials_json` field.\nIn order for a service account to access files in Google Drive either files need to be explicitly shared with the service account email, otherwise https://support.google.com/a/answer/162106[^domain wide delegation] can be used to share all files within a Google Workspace.\n"
      },
      {
        "name": "grok",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": "\nType hints within patterns are respected, therefore with the pattern `%\\{WORD:first},%{INT:second:int}` and a payload of `foo,1` the resulting payload would be `\\{\"first\":\"foo\",\"second\":1}`.\n\n== Performance\n\nThis processor currently uses the https://golang.org/s/re2syntax[Go RE2^] regular expression engine, which is guaranteed to run in time linear to the size of the input. However, this property often makes it less performant than PCRE based implementations of grok. For more information, see https://swtch.com/~rsc/regexp/regexp1.html."
      },
      {
        "name": "group_by",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": "\nOnce the groups are established a list of processors are applied to their respective grouped batch, which can be used to label the batch as per their grouping. Messages that do not pass the check of any specified group are placed in their own group.\n\nThe functionality of this processor depends on being applied across messages that are batched. You can find out more about batching xref:configuration:batching.adoc[in this doc]."
      },
      {
        "name": "group_by_value",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": "\nThis allows you to group messages using arbitrary fields within their content or metadata, process them individually, and send them to unique locations as per their group.\n\nThe functionality of this processor depends on being applied across messages that are batched. You can find out more about batching xref:configuration:batching.adoc[in this doc]."
      },
      {
        "name": "http",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": "\nThe `rate_limit` field can be used to specify a rate limit xref:components:rate_limits/about.adoc[resource] to cap the rate of requests across all parallel components service wide.\n\nThe URL and header values of this type can be dynamically set using function interpolations described xref:configuration:interpolation.adoc#bloblang-queries[here].\n\nIn order to map or encode the payload to a specific request body, and map the response back into the original payload instead of replacing it entirely, you can use the xref:components:processors/branch.adoc[`branch` processor].\n\n== Response codes\n\nRedpanda Connect considers any response code between 200 and 299 inclusive to indicate a successful response, you can add more success status codes with the field `successful_on`.\n\nWhen a request returns a response code within the `backoff_on` field it will be retried after increasing intervals.\n\nWhen a request returns a response code within the `drop_on` field it will not be reattempted and is immediately considered a failed request.\n\n== Add metadata\n\nIf the request returns an error response code this processor sets a metadata field `http_status_code` on the resulting message.\n\nUse the field `extract_headers` to specify rules for which other headers should be copied into the resulting message from the response.\n\n== Error handling\n\nWhen all retry attempts for a message are exhausted the processor cancels the attempt. These failed messages will continue through the pipeline unchanged, but can be dropped or placed in a dead letter queue according to your config, you can read about xref:configuration:error_handling.adoc[these patterns]."
      },
      {
        "name": "insert_part",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": "\nThe index can be negative, and if so the message will be inserted from the end counting backwards starting from -1. E.g. if index = -1 then the new message will become the last of the batch, if index = -2 then the new message will be inserted before the last message, and so on. If the negative index is greater than the length of the existing batch it will be inserted at the beginning.\n\nThe new message will have metadata copied from the first pre-existing message of the batch.\n\nThis processor will interpolate functions within the 'content' field, you can find a list of functions xref:configuration:interpolation.adoc#bloblang-queries[here]."
      },
      {
        "name": "javascript",
        "type": "processors",
        "status": "experimental",
        "version": "4.14.0",
        "description": "\nThe https://github.com/dop251/goja[execution engine^] behind this processor provides full ECMAScript 5.1 support (including regex and strict mode). Most of the ECMAScript 6 spec is implemented but this is a work in progress.\n\nImports via `require` should work similarly to NodeJS, and access to the console is supported which will print via the Redpanda Connect logger. More caveats can be found on https://github.com/dop251/goja#known-incompatibilities-and-caveats[GitHub^].\n\nThis processor is implemented using the https://github.com/dop251/goja[github.com/dop251/goja^] library."
      },
      {
        "name": "jira",
        "type": "processors",
        "status": "experimental",
        "version": "4.68.0",
        "description": "Executes Jira API queries based on input messages and returns structured results. The processor handles pagination, retries, and field expansion automatically.\n\nSupports querying the following Jira resources:\n- Issues (JQL queries)\n- Issue transitions\n- Users\n- Roles\n- Project versions\n- Project categories\n- Project types\n- Projects\n\nThe processor authenticates using basic authentication with username and API token. Input messages should contain valid Jira queries in JSON format."
      },
      {
        "name": "jmespath",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": "\n[TIP]\n.Try out Bloblang\n====\nFor better performance and improved capabilities try native Redpanda Connect mapping with the xref:components:processors/mapping.adoc[`mapping` processor].\n====\n"
      },
      {
        "name": "jq",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": "\n[TIP]\n.Try out Bloblang\n====\nFor better performance and improved capabilities try out native Redpanda Connect mapping with the xref:components:processors/mapping.adoc[`mapping` processor].\n====\n\nThe provided query is executed on each message, targeting either the contents as a structured JSON value or as a raw string using the field `raw`, and the message is replaced with the query result.\n\nMessage metadata is also accessible within the query from the variable `$metadata`.\n\nThis processor uses the https://github.com/itchyny/gojq[gojq library^], and therefore does not require jq to be installed as a dependency. However, this also means there are some https://github.com/itchyny/gojq#difference-to-jq[differences in how these queries are executed^] versus the jq cli.\n\nIf the query does not emit any value then the message is filtered, if the query returns multiple values then the resulting message will be an array containing all values.\n\nThe full query syntax is described in https://stedolan.github.io/jq/manual/[jq's documentation^].\n\n== Error handling\n\nQueries can fail, in which case the message remains unchanged, errors are logged, and the message is flagged as having failed, allowing you to use xref:configuration:error_handling.adoc[standard processor error handling patterns]."
      },
      {
        "name": "json_schema",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": "Please refer to the https://json-schema.org/[JSON Schema website^] for information and tutorials regarding the syntax of the schema."
      },
      {
        "name": "log",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": "\nThe `level` field determines the log level of the printed events and can be any of the following values: TRACE, DEBUG, INFO, WARN, ERROR.\n\n== Structured fields\n\nIt's also possible add custom fields to logs when the format is set to a structured form such as `json` or `logfmt` with the config field <<fields_mapping, `fields_mapping`>>:\n\n```yaml\npipeline:\n  processors:\n    - log:\n        level: DEBUG\n        message: hello world\n        fields_mapping: |\n          root.reason = \"cus I wana\"\n          root.id = this.id\n          root.age = this.user.age\n          root.kafka_topic = meta(\"kafka_topic\")\n```\n"
      },
      {
        "name": "mapping",
        "type": "processors",
        "status": "stable",
        "version": "4.5.0",
        "description": "\nBloblang is a powerful language that enables a wide range of mapping, transformation and filtering tasks. For more information, see xref:guides:bloblang/about.adoc[].\n\nIf your mapping is large and you'd prefer for it to live in a separate file then you can execute a mapping directly from a file with the expression `from \"<path>\"`, where the path must be absolute, or relative from the location that Redpanda Connect is executed from.\n\nNote: This processor is equivalent to the xref:components:processors/bloblang.adoc#component-rename[Bloblang] one. The latter will be deprecated in a future release.\n\n== Input document immutability\n\nMapping operates by creating an entirely new object during assignments, this has the advantage of treating the original referenced document as immutable and therefore queryable at any stage of your mapping. For example, with the following mapping:\n\n```coffeescript\nroot.id = this.id\nroot.invitees = this.invitees.filter(i -> i.mood >= 0.5)\nroot.rejected = this.invitees.filter(i -> i.mood < 0.5)\n```\n\nNotice that we mutate the value of `invitees` in the resulting document by filtering out objects with a lower mood. However, even after doing so we're still able to reference the unchanged original contents of this value from the input document in order to populate a second field. Within this mapping we also have the flexibility to reference the mutable mapped document by using the keyword `root` (i.e. `root.invitees`) on the right-hand side instead.\n\nMapping documents is advantageous in situations where the result is a document with a dramatically different shape to the input document, since we are effectively rebuilding the document in its entirety and might as well keep a reference to the unchanged input document throughout. However, in situations where we are only performing minor alterations to the input document, the rest of which is unchanged, it might be more efficient to use the xref:components:processors/mutation.adoc[`mutation` processor] instead.\n\n== Error handling\n\nBloblang mappings can fail, in which case the message remains unchanged, errors are logged, and the message is flagged as having failed, allowing you to use xref:configuration:error_handling.adoc[standard processor error handling patterns].\n\nHowever, Bloblang itself also provides powerful ways of ensuring your mappings do not fail by specifying desired xref:guides:bloblang/about.adoc#error-handling[fallback behavior].\n\t\t\t"
      },
      {
        "name": "metric",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": "\nThis processor works by evaluating an xref:configuration:interpolation.adoc#bloblang-queries[interpolated field `value`] for each message and updating a emitted metric according to the <<types, type>>.\n\nCustom metrics such as these are emitted along with Redpanda Connect internal metrics, where you can customize where metrics are sent, which metric names are emitted and rename them as/when appropriate. For more information see the xref:components:metrics/about.adoc[metrics docs]."
      },
      {
        "name": "mongodb",
        "type": "processors",
        "status": "experimental",
        "version": "3.43.0",
        "description": ""
      },
      {
        "name": "msgpack",
        "type": "processors",
        "status": "beta",
        "version": "3.59.0",
        "description": ""
      },
      {
        "name": "mutation",
        "type": "processors",
        "status": "stable",
        "version": "4.5.0",
        "description": "\nBloblang is a powerful language that enables a wide range of mapping, transformation and filtering tasks. For more information, see xref:guides:bloblang/about.adoc[].\n\nIf your mapping is large and you'd prefer for it to live in a separate file then you can execute a mapping directly from a file with the expression `from \"<path>\"`, where the path must be absolute, or relative from the location that Redpanda Connect is executed from.\n\n== Input document mutability\n\nA mutation is a mapping that transforms input documents directly, this has the advantage of reducing the need to copy the data fed into the mapping. However, this also means that the referenced document is mutable and therefore changes throughout the mapping. For example, with the following Bloblang:\n\n```coffeescript\nroot.rejected = this.invitees.filter(i -> i.mood < 0.5)\nroot.invitees = this.invitees.filter(i -> i.mood >= 0.5)\n```\n\nNotice that we create a field `rejected` by copying the array field `invitees` and filtering out objects with a high mood. We then overwrite the field `invitees` by filtering out objects with a low mood, resulting in two array fields that are each a subset of the original. If we were to reverse the ordering of these assignments like so:\n\n```coffeescript\nroot.invitees = this.invitees.filter(i -> i.mood >= 0.5)\nroot.rejected = this.invitees.filter(i -> i.mood < 0.5)\n```\n\nThen the new field `rejected` would be empty as we have already mutated `invitees` to exclude the objects that it would be populated by. We can solve this problem either by carefully ordering our assignments or by capturing the original array using a variable (`let invitees = this.invitees`).\n\nMutations are advantageous over a standard mapping in situations where the result is a document with mostly the same shape as the input document, since we can avoid unnecessarily copying data from the referenced input document. However, in situations where we are creating an entirely new document shape it can be more convenient to use the traditional xref:components:processors/mapping.adoc[`mapping` processor] instead.\n\n== Error handling\n\nBloblang mappings can fail, in which case the error is logged and the message is flagged as having failed, allowing you to use xref:configuration:error_handling.adoc[standard processor error handling patterns].\n\nHowever, Bloblang itself also provides powerful ways of ensuring your mappings do not fail by specifying desired xref:guides:bloblang/about.adoc#error-handling[fallback behavior].\n\t\t\t"
      },
      {
        "name": "nats_kv",
        "type": "processors",
        "status": "beta",
        "version": "4.12.0",
        "description": "\n== KV operations\n\nThe NATS KV processor supports a multitude of KV operations via the <<operation>> field. Along with `get`, `put`, and `delete`, this processor supports atomic operations like `update` and `create`, as well as utility operations like `purge`, `history`, and `keys`.\n\n== Metadata\n\nThis processor adds the following metadata fields to each message, depending on the chosen `operation`:\n\n=== get, get_revision\n``` text\n- nats_kv_key\n- nats_kv_bucket\n- nats_kv_revision\n- nats_kv_delta\n- nats_kv_operation\n- nats_kv_created\n```\n\n=== create, update, delete, purge\n``` text\n- nats_kv_key\n- nats_kv_bucket\n- nats_kv_revision\n- nats_kv_operation\n```\n\n=== keys\n``` text\n- nats_kv_bucket\n```\n\n== Connection name\n\nWhen monitoring and managing a production NATS system, it is often useful to\nknow which connection a message was send/received from. This can be achieved by\nsetting the connection name option when creating a NATS connection.\n\nRedpanda Connect will automatically set the connection name based off the label of the given\nNATS component, so that monitoring tools between NATS and Redpanda Connect can stay in sync.\n\n\n== Authentication\n\nThere are several components within Redpanda Connect which uses NATS services. You will find that each of these components\nsupport optional advanced authentication parameters for https://docs.nats.io/nats-server/configuration/securing_nats/auth_intro/nkey_auth[NKeys^]\nand https://docs.nats.io/using-nats/developer/connecting/creds[User Credentials^].\n\nSee an https://docs.nats.io/running-a-nats-service/nats_admin/security/jwt[in-depth tutorial^].\n\n=== NKey file\n\nThe NATS server can use these NKeys in several ways for authentication. The simplest is for the server to be configured\nwith a list of known public keys and for the clients to respond to the challenge by signing it with its private NKey\nconfigured in the `nkey_file` or `nkey` field.\n\nhttps://docs.nats.io/running-a-nats-service/configuration/securing_nats/auth_intro/nkey_auth[More details^].\n\n=== User credentials\n\nNATS server supports decentralized authentication based on JSON Web Tokens (JWT). Clients need an https://docs.nats.io/nats-server/configuration/securing_nats/jwt#json-web-tokens[user JWT^]\nand a corresponding https://docs.nats.io/running-a-nats-service/configuration/securing_nats/auth_intro/nkey_auth[NKey secret^] when connecting to a server\nwhich is configured to use this authentication scheme.\n\nThe `user_credentials_file` field should point to a file containing both the private key and the JWT and can be\ngenerated with the https://docs.nats.io/nats-tools/nsc[nsc tool^].\n\nAlternatively, the `user_jwt` field can contain a plain text JWT and the `user_nkey_seed`can contain\nthe plain text NKey Seed.\n\nhttps://docs.nats.io/using-nats/developer/connecting/creds[More details^]."
      },
      {
        "name": "nats_request_reply",
        "type": "processors",
        "status": "experimental",
        "version": "4.27.0",
        "description": "\n== Metadata\n\nThis input adds the following metadata fields to each message:\n\n```text\n- nats_subject\n- nats_sequence_stream\n- nats_sequence_consumer\n- nats_num_delivered\n- nats_num_pending\n- nats_domain\n- nats_timestamp_unix_nano\n```\n\nYou can access these metadata fields using xref:configuration:interpolation.adoc#bloblang-queries[function interpolation].\n\n== Connection name\n\nWhen monitoring and managing a production NATS system, it is often useful to\nknow which connection a message was send/received from. This can be achieved by\nsetting the connection name option when creating a NATS connection.\n\nRedpanda Connect will automatically set the connection name based off the label of the given\nNATS component, so that monitoring tools between NATS and Redpanda Connect can stay in sync.\n\n\n== Authentication\n\nThere are several components within Redpanda Connect which uses NATS services. You will find that each of these components\nsupport optional advanced authentication parameters for https://docs.nats.io/nats-server/configuration/securing_nats/auth_intro/nkey_auth[NKeys^]\nand https://docs.nats.io/using-nats/developer/connecting/creds[User Credentials^].\n\nSee an https://docs.nats.io/running-a-nats-service/nats_admin/security/jwt[in-depth tutorial^].\n\n=== NKey file\n\nThe NATS server can use these NKeys in several ways for authentication. The simplest is for the server to be configured\nwith a list of known public keys and for the clients to respond to the challenge by signing it with its private NKey\nconfigured in the `nkey_file` or `nkey` field.\n\nhttps://docs.nats.io/running-a-nats-service/configuration/securing_nats/auth_intro/nkey_auth[More details^].\n\n=== User credentials\n\nNATS server supports decentralized authentication based on JSON Web Tokens (JWT). Clients need an https://docs.nats.io/nats-server/configuration/securing_nats/jwt#json-web-tokens[user JWT^]\nand a corresponding https://docs.nats.io/running-a-nats-service/configuration/securing_nats/auth_intro/nkey_auth[NKey secret^] when connecting to a server\nwhich is configured to use this authentication scheme.\n\nThe `user_credentials_file` field should point to a file containing both the private key and the JWT and can be\ngenerated with the https://docs.nats.io/nats-tools/nsc[nsc tool^].\n\nAlternatively, the `user_jwt` field can contain a plain text JWT and the `user_nkey_seed`can contain\nthe plain text NKey Seed.\n\nhttps://docs.nats.io/using-nats/developer/connecting/creds[More details^]."
      },
      {
        "name": "noop",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "ollama_chat",
        "type": "processors",
        "status": "experimental",
        "version": "4.32.0",
        "description": "This processor sends prompts to your chosen Ollama large language model (LLM) and generates text from the responses, using the Ollama API.\n\nBy default, the processor starts and runs a locally installed Ollama server. Alternatively, to use an already running Ollama server, add your server details to the `server_address` field. You can https://ollama.com/download[download and install Ollama from the Ollama website^].\n\nFor more information, see the https://github.com/ollama/ollama/tree/main/docs[Ollama documentation^]."
      },
      {
        "name": "ollama_embeddings",
        "type": "processors",
        "status": "experimental",
        "version": "4.32.0",
        "description": "This processor sends text to your chosen Ollama large language model (LLM) and creates vector embeddings, using the Ollama API. Vector embeddings are long arrays of numbers that represent values or objects, in this case text. \n\nBy default, the processor starts and runs a locally installed Ollama server. Alternatively, to use an already running Ollama server, add your server details to the `server_address` field. You can https://ollama.com/download[download and install Ollama from the Ollama website^].\n\nFor more information, see the https://github.com/ollama/ollama/tree/main/docs[Ollama documentation^]."
      },
      {
        "name": "ollama_moderation",
        "type": "processors",
        "status": "experimental",
        "version": "4.42.0",
        "description": "This processor checks LLM response safety using either `llama-guard3` or `shieldgemma`. If you want to check if a given prompt is safe, then that can be done with the `ollama_chat` processor - this processor is for response classification only.\n\nBy default, the processor starts and runs a locally installed Ollama server. Alternatively, to use an already running Ollama server, add your server details to the `server_address` field. You can https://ollama.com/download[download and install Ollama from the Ollama website^].\n\nFor more information, see the https://github.com/ollama/ollama/tree/main/docs[Ollama documentation^]."
      },
      {
        "name": "openai_chat_completion",
        "type": "processors",
        "status": "experimental",
        "version": "4.32.0",
        "description": "\nThis processor sends the contents of user prompts to the OpenAI API, which generates responses. By default, the processor submits the entire payload of each message as a string, unless you use the `prompt` configuration field to customize it.\n\nTo learn more about chat completion, see the https://platform.openai.com/docs/guides/chat-completions[OpenAI API documentation^]."
      },
      {
        "name": "openai_embeddings",
        "type": "processors",
        "status": "experimental",
        "version": "4.32.0",
        "description": "\nThis processor sends text strings to the OpenAI API, which generates vector embeddings. By default, the processor submits the entire payload of each message as a string, unless you use the `text_mapping` configuration field to customize it.\n\nTo learn more about vector embeddings, see the https://platform.openai.com/docs/guides/embeddings[OpenAI API documentation^]."
      },
      {
        "name": "openai_image_generation",
        "type": "processors",
        "status": "experimental",
        "version": "4.32.0",
        "description": "\nThis processor sends an image description and other attributes, such as image size and quality to the OpenAI API, which generates an image. By default, the processor submits the entire payload of each message as a string, unless you use the `prompt` configuration field to customize it.\n\nTo learn more about image generation, see the https://platform.openai.com/docs/guides/images[OpenAI API documentation^]."
      },
      {
        "name": "openai_speech",
        "type": "processors",
        "status": "experimental",
        "version": "4.32.0",
        "description": "\nThis processor sends a text description and other attributes, such as a voice type and format to the OpenAI API, which generates audio. By default, the processor submits the entire payload of each message as a string, unless you use the `input` configuration field to customize it.\n\nTo learn more about turning text into spoken audio, see the https://platform.openai.com/docs/guides/text-to-speech[OpenAI API documentation^]."
      },
      {
        "name": "openai_transcription",
        "type": "processors",
        "status": "experimental",
        "version": "4.32.0",
        "description": "\nThis processor sends an audio file object along with the input language to OpenAI API to generate a transcription. By default, the processor submits the entire payload of each message as a string, unless you use the `file` configuration field to customize it.\n\nTo learn more about audio transcription, see the: https://platform.openai.com/docs/guides/speech-to-text[OpenAI API documentation^]."
      },
      {
        "name": "openai_translation",
        "type": "processors",
        "status": "experimental",
        "version": "4.32.0",
        "description": "\nThis processor sends an audio file object to OpenAI API to generate a translation. By default, the processor submits the entire payload of each message as a string, unless you use the `file` configuration field to customize it.\n\nTo learn more about translation, see the https://platform.openai.com/docs/guides/speech-to-text[OpenAI API documentation^]."
      },
      {
        "name": "parallel",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": "\nThe field `cap`, if greater than zero, caps the maximum number of parallel processing threads.\n\nThe functionality of this processor depends on being applied across messages that are batched. You can find out more about batching in xref:configuration:batching.adoc[]."
      },
      {
        "name": "parquet",
        "type": "processors",
        "status": "deprecated",
        "version": "3.62.0",
        "description": "\n== Alternatives\n\nThis processor is now deprecated, it's recommended that you use the new xref:components:processors/parquet_decode.adoc[`parquet_decode`] and xref:components:processors/parquet_encode.adoc[`parquet_encode`] processors as they provide a number of advantages, the most important of which is better error messages for when schemas are mismatched or files could not be consumed.\n\n== Troubleshooting\n\nThis processor is experimental and the error messages that it provides are often vague and unhelpful. An error message of the form `interface \\{} is nil, not <value type>` implies that a field of the given type was expected but not found in the processed message when writing parquet files.\n\nUnfortunately the name of the field will sometimes be missing from the error, in which case it's worth double checking the schema you provided to make sure that there are no typos in the field names, and if that doesn't reveal the issue it can help to mark fields as OPTIONAL in the schema and gradually change them back to REQUIRED until the error returns.\n\n== Define the schema\n\nThe schema must be specified as a JSON string, containing an object that describes the fields expected at the root of each document. Each field can itself have more fields defined, allowing for nested structures:\n\n```json\n{\n  \"Tag\": \"name=root, repetitiontype=REQUIRED\",\n  \"Fields\": [\n    {\"Tag\": \"name=name, inname=NameIn, type=BYTE_ARRAY, convertedtype=UTF8, repetitiontype=REQUIRED\"},\n    {\"Tag\": \"name=age, inname=Age, type=INT32, repetitiontype=REQUIRED\"},\n    {\"Tag\": \"name=id, inname=Id, type=INT64, repetitiontype=REQUIRED\"},\n    {\"Tag\": \"name=weight, inname=Weight, type=FLOAT, repetitiontype=REQUIRED\"},\n    {\n      \"Tag\": \"name=favPokemon, inname=FavPokemon, type=LIST, repetitiontype=OPTIONAL\",\n      \"Fields\": [\n        {\"Tag\": \"name=name, inname=PokeName, type=BYTE_ARRAY, convertedtype=UTF8, repetitiontype=REQUIRED\"},\n        {\"Tag\": \"name=coolness, inname=Coolness, type=FLOAT, repetitiontype=REQUIRED\"}\n      ]\n    }\n  ]\n}\n```\n\nA schema can be derived from a source file using https://github.com/xitongsys/parquet-go/tree/master/tool/parquet-tools:\n\n```sh\n./parquet-tools -cmd schema -file foo.parquet\n```"
      },
      {
        "name": "parquet_decode",
        "type": "processors",
        "status": "experimental",
        "version": "4.4.0",
        "description": "\nThis processor uses https://github.com/parquet-go/parquet-go[https://github.com/parquet-go/parquet-go^], which is itself experimental. Therefore changes could be made into how this processor functions outside of major version releases."
      },
      {
        "name": "parquet_encode",
        "type": "processors",
        "status": "experimental",
        "version": "4.4.0",
        "description": "\nThis processor uses https://github.com/parquet-go/parquet-go[https://github.com/parquet-go/parquet-go^], which is itself experimental. Therefore changes could be made into how this processor functions outside of major version releases.\n"
      },
      {
        "name": "parse_log",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "processors",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": "This processor is useful in situations where you want to collect several processors under a single resource identifier, whether it is for making your configuration easier to read and navigate, or for improving the testability of your configuration. The behavior of child processors will match exactly the behavior they would have under any other processors block."
      },
      {
        "name": "protobuf",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": "\nThe main functionality of this processor is to map to and from JSON documents, you can read more about JSON mapping of protobuf messages here: [https://developers.google.com/protocol-buffers/docs/proto3#json](https://developers.google.com/protocol-buffers/docs/proto3#json)\n\nUsing reflection for processing protobuf messages in this way is less performant than generating and using native code. Therefore when performance is critical it is recommended that you use Redpanda Connect plugins instead for processing protobuf messages natively, you can find an example of Redpanda Connect plugins at [https://github.com/redpanda-data/redpanda-connect-plugin-example](https://github.com/redpanda-data/redpanda-connect-plugin-example)\n\nThe processor will ignore any files that begin with a dot (\".\"g), a convention for hidden files, when loading protocol buffer definitions.\n== Operators\n\n=== `to_json`\n\nConverts protobuf messages into serialized proto3 JSON.\n\n=== `from_json`\n\nAttempts to create a target protobuf message from a serialized proto3 JSON.\n\n=== `decode`\n\nConverts protobuf messages into a generic structured message. This makes it easier to manipulate the contents of the document within Redpanda Connect.\nThis differs from `to_json` in the following ways:\n\n- 64 bit numbers are *not* converted into strings\n- Bytes and google.protobuf.Timestamp types are preserved (not encoded as strings unless serialized)\n\nThis operator is also considerably faster in scenario where you manipulate the data as the data does not need to be serialized then deserialized like with the `to_json` operator.\n"
      },
      {
        "name": "qdrant",
        "type": "processors",
        "status": "experimental",
        "version": "",
        "description": ""
      },
      {
        "name": "rate_limit",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "redis",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "redis_script",
        "type": "processors",
        "status": "beta",
        "version": "4.11.0",
        "description": "Actions are performed for each message and the message contents are replaced with the result.\n\nIn order to merge the result into the original message compose this processor within a xref:components:processors/branch.adoc[`branch` processor]."
      },
      {
        "name": "redpanda_data_transform",
        "type": "processors",
        "status": "experimental",
        "version": "4.31.0",
        "description": "\nThis processor executes a Redpanda Data Transform WebAssembly module, calling OnRecordWritten for each message being processed.\n\nYou can find out about how transforms work here: https://docs.redpanda.com/current/develop/data-transforms/how-transforms-work/[https://docs.redpanda.com/current/develop/data-transforms/how-transforms-work/^]\n"
      },
      {
        "name": "resource",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": "\nThis processor allows you to reference the same configured processor resource in multiple places, and can also tidy up large nested configs. For example, the config:\n\n```yaml\npipeline:\n  processors:\n    - mapping: |\n        root.message = this\n        root.meta.link_count = this.links.length()\n        root.user.age = this.user.age.number()\n```\n\nIs equivalent to:\n\n```yaml\npipeline:\n  processors:\n    - resource: foo_proc\n\nprocessor_resources:\n  - label: foo_proc\n    mapping: |\n      root.message = this\n      root.meta.link_count = this.links.length()\n      root.user.age = this.user.age.number()\n```\n\nYou can find out more about resources in xref:configuration:resources.adoc[]"
      },
      {
        "name": "retry",
        "type": "processors",
        "status": "beta",
        "version": "4.27.0",
        "description": "\nExecutes child processors and if a resulting message is errored then, after a specified backoff period, the same original message will be attempted again through those same processors. If the child processors result in more than one message then the retry mechanism will kick in if _any_ of the resulting messages are errored.\n\nIt is important to note that any mutations performed on the message during these child processors will be discarded for the next retry, and therefore it is safe to assume that each execution of the child processors will always be performed on the data as it was when it first reached the retry processor.\n\nBy default the retry backoff has a specified <<backoffmax_elapsed_time,`max_elapsed_time`>>, if this time period is reached during retries and an error still occurs these errored messages will proceed through to the next processor after the retry (or your outputs). Normal xref:configuration:error_handling.adoc[error handling patterns] can be used on these messages.\n\nIn order to avoid permanent loops any error associated with messages as they first enter a retry processor will be cleared.\n\n== Metadata\n\nThis processor adds the following metadata fields to each message:\n\n```text\n- retry_count - The number of retry attempts.\n- backoff_duration - The total time (in nanoseconds) elapsed while performing retries.\n```\n\n[CAUTION]\n.Batching\n====\nIf you wish to wrap a batch-aware series of processors then take a look at the <<batching, batching section>>.\n====\n"
      },
      {
        "name": "schema_registry_decode",
        "type": "processors",
        "status": "beta",
        "version": "",
        "description": "\nDecodes messages automatically from a schema stored within a https://docs.confluent.io/platform/current/schema-registry/index.html[Confluent Schema Registry service^] by extracting a schema ID from the message and obtaining the associated schema from the registry. If a message fails to match against the schema then it will remain unchanged and the error can be caught using xref:configuration:error_handling.adoc[error handling methods].\n\nAvro, Protobuf and Json schemas are supported, all are capable of expanding from schema references as of v4.22.0.\n\n== Avro JSON format\n\nThis processor creates documents formatted as https://avro.apache.org/docs/current/specification/_print/#json-encoding[Avro JSON^] when decoding with Avro schemas. In this format the value of a union is encoded in JSON as follows:\n\n- if its type is `null`, then it is encoded as a JSON `null`;\n- otherwise it is encoded as a JSON object with one name/value pair whose name is the type's name and whose value is the recursively encoded value. For Avro's named types (record, fixed or enum) the user-specified name is used, for other types the type name is used.\n\nFor example, the union schema `[\"null\",\"string\",\"Foo\"]`, where `Foo` is a record name, would encode:\n\n- `null` as `null`;\n- the string `\"a\"` as `{\"string\": \"a\"}`; and\n- a `Foo` instance as `{\"Foo\": {...}}`, where `{...}` indicates the JSON encoding of a `Foo` instance.\n\nHowever, it is possible to instead create documents in https://pkg.go.dev/github.com/linkedin/goavro/v2#NewCodecForStandardJSONFull[standard/raw JSON format^] by setting the field <<avro_raw_json, `avro_raw_json`>> to `true`.\n\n== Protobuf format\n\nThis processor decodes protobuf messages to JSON documents, you can read more about JSON mapping of protobuf messages here: https://developers.google.com/protocol-buffers/docs/proto3#json\n\n== Metadata\n\nThis processor also adds the following metadata to each outgoing message:\n\nschema_id: the ID of the schema in the schema registry that was associated with the message.\n"
      },
      {
        "name": "schema_registry_encode",
        "type": "processors",
        "status": "beta",
        "version": "3.58.0",
        "description": "\nEncodes messages automatically from schemas obtains from a https://docs.confluent.io/platform/current/schema-registry/index.html[Confluent Schema Registry service^] by polling the service for the latest schema version for target subjects.\n\nIf a message fails to encode under the schema then it will remain unchanged and the error can be caught using xref:configuration:error_handling.adoc[error handling methods].\n\nAvro, Protobuf and Json schemas are supported, all are capable of expanding from schema references as of v4.22.0.\n\n== Avro JSON format\n\nBy default this processor expects documents formatted as https://avro.apache.org/docs/current/specification/_print/#json-encoding[Avro JSON^] when encoding with Avro schemas. In this format the value of a union is encoded in JSON as follows:\n\n- if its type is `null`, then it is encoded as a JSON `null`;\n- otherwise it is encoded as a JSON object with one name/value pair whose name is the type's name and whose value is the recursively encoded value. For Avro's named types (record, fixed or enum) the user-specified name is used, for other types the type name is used.\n\nFor example, the union schema `[\"null\",\"string\",\"Foo\"]`, where `Foo` is a record name, would encode:\n\n- `null` as `null`;\n- the string `\"a\"` as `\\{\"string\": \"a\"}`; and\n- a `Foo` instance as `\\{\"Foo\": {...}}`, where `{...}` indicates the JSON encoding of a `Foo` instance.\n\nHowever, it is possible to instead consume documents in https://pkg.go.dev/github.com/linkedin/goavro/v2#NewCodecForStandardJSONFull[standard/raw JSON format^] by setting the field <<avro_raw_json, `avro_raw_json`>> to `true`.\n\n=== Known issues\n\nImportant! There is an outstanding issue in the https://github.com/linkedin/goavro[avro serializing library^] that Redpanda Connect uses which means it https://github.com/linkedin/goavro/issues/252[doesn't encode logical types correctly^]. It's still possible to encode logical types that are in-line with the spec if `avro_raw_json` is set to true, though now of course non-logical types will not be in-line with the spec.\n\n== Protobuf format\n\nThis processor encodes protobuf messages either from any format parsed within Redpanda Connect (encoded as JSON by default), or from raw JSON documents, you can read more about JSON mapping of protobuf messages here: https://developers.google.com/protocol-buffers/docs/proto3#json\n\n=== Multiple message support\n\nWhen a target subject presents a protobuf schema that contains multiple messages it becomes ambiguous which message definition a given input data should be encoded against. In such scenarios Redpanda Connect will attempt to encode the data against each of them and select the first to successfully match against the data, this process currently *ignores all nested message definitions*. In order to speed up this exhaustive search the last known successful message will be attempted first for each subsequent input.\n\nWe will be considering alternative approaches in future so please https://redpanda.com/slack[get in touch^] with thoughts and feedback.\n"
      },
      {
        "name": "select_parts",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": "\nThe selected parts are added to the new message batch in the same order as the selection array. E.g. with 'parts' set to [ 2, 0, 1 ] and the message parts [ '0', '1', '2', '3' ], the output will be [ '2', '0', '1' ].\n\nIf none of the selected parts exist in the input batch (resulting in an empty output message) the batch is dropped entirely.\n\nMessage indexes can be negative, and if so the part will be selected from the end counting backwards starting from -1. E.g. if index = -1 then the selected part will be the last part of the message, if index = -2 then the part before the last element with be selected, and so on.\n\nThis processor is only applicable to xref:configuration:batching.adoc[batched messages]."
      },
      {
        "name": "sentry_capture",
        "type": "processors",
        "status": "experimental",
        "version": "4.16.0",
        "description": ""
      },
      {
        "name": "slack_thread",
        "type": "processors",
        "status": "experimental",
        "version": "",
        "description": "Read a thread using the https://api.slack.com/methods/conversations.replies[^Slack API]"
      },
      {
        "name": "sleep",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "split",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": "\nThis processor is for breaking batches down into smaller ones. In order to break a single message out into multiple messages use the xref:components:processors/unarchive.adoc[`unarchive` processor].\n\nIf there is a remainder of messages after splitting a batch the remainder is also sent as a single batch. For example, if your target size was 10, and the processor received a batch of 95 message parts, the result would be 9 batches of 10 messages followed by a batch of 5 messages."
      },
      {
        "name": "sql",
        "type": "processors",
        "status": "deprecated",
        "version": "3.65.0",
        "description": "\nIf the query fails to execute then the message will remain unchanged and the error can be caught using xref:configuration:error_handling.adoc[error handling methods].\n\n== Alternatives\n\nFor basic inserts or select queries use either the xref:components:processors/sql_insert.adoc[`sql_insert`] or the xref:components:processors/sql_select.adoc[`sql_select`] processor. For more complex queries use the xref:components:processors/sql_raw.adoc[`sql_raw`] processor."
      },
      {
        "name": "sql_insert",
        "type": "processors",
        "status": "stable",
        "version": "3.59.0",
        "description": "\nIf the insert fails to execute then the message will still remain unchanged and the error can be caught using xref:configuration:error_handling.adoc[error handling methods]."
      },
      {
        "name": "sql_raw",
        "type": "processors",
        "status": "stable",
        "version": "3.65.0",
        "description": "\nIf the query fails to execute then the message will remain unchanged and the error can be caught using xref:configuration:error_handling.adoc[error handling methods]."
      },
      {
        "name": "sql_select",
        "type": "processors",
        "status": "stable",
        "version": "3.59.0",
        "description": "\nIf the query fails to execute then the message will remain unchanged and the error can be caught using xref:configuration:error_handling.adoc[error handling methods]."
      },
      {
        "name": "subprocess",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": "\n[NOTE]\n====\nThis processor keeps the subprocess alive and requires very specific behavior from the command executed. If you wish to simply execute a command for each message take a look at the xref:components:processors/command.adoc[`command` processor] instead.\n====\n\nThe subprocess must then either return a line over stdout or stderr. If a response is returned over stdout then its contents will replace the message. If a response is instead returned from stderr it will be logged and the message will continue unchanged and will be xref:configuration:error_handling.adoc[marked as failed].\n\nRather than separating data by a newline it's possible to specify alternative <<codec_send,`codec_send`>> and <<codec_recv,`codec_recv`>> values, which allow binary messages to be encoded for logical separation.\n\nThe execution environment of the subprocess is the same as the Redpanda Connect instance, including environment variables and the current working directory.\n\nThe field `max_buffer` defines the maximum response size able to be read from the subprocess. This value should be set significantly above the real expected maximum response size.\n\n== Subprocess requirements\n\nIt is required that subprocesses flush their stdout and stderr pipes for each line. Redpanda Connect will attempt to keep the process alive for as long as the pipeline is running. If the process exits early it will be restarted.\n\n== Messages containing line breaks\n\nIf a message contains line breaks each line of the message is piped to the subprocess and flushed, and a response is expected from the subprocess before another line is fed in."
      },
      {
        "name": "switch",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": "For each switch case a xref:guides:bloblang/about.adoc[Bloblang query] is checked and, if the result is true (or the check is empty) the child processors are executed on the message."
      },
      {
        "name": "sync_response",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": "\nFor most inputs this mechanism is ignored entirely, in which case the sync response is dropped without penalty. It is therefore safe to use this processor even when combining input types that might not have support for sync responses. An example of an input able to utilize this is the `http_server`.\n\nFor more information please read xref:guides:sync_responses.adoc[synchronous responses]."
      },
      {
        "name": "text_chunker",
        "type": "processors",
        "status": "experimental",
        "version": "",
        "description": "A processor allowing splitting text into chunks based on several different strategies."
      },
      {
        "name": "try",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": "\nThis processor behaves similarly to the xref:components:processors/for_each.adoc[`for_each`] processor, where a list of child processors are applied to individual messages of a batch. However, if a message has failed any prior processor (before or during the try block) then that message will skip all following processors.\n\nFor example, with the following config:\n\n```yaml\npipeline:\n  processors:\n    - resource: foo\n    - try:\n      - resource: bar\n      - resource: baz\n      - resource: buz\n```\n\nIf the processor `bar` fails for a particular message, that message will skip the processors `baz` and `buz`. Similarly, if `bar` succeeds but `baz` does not then `buz` will be skipped. If the processor `foo` fails for a message then none of `bar`, `baz` or `buz` are executed on that message.\n\nThis processor is useful for when child processors depend on the successful output of previous processors. This processor can be followed with a xref:components:processors/catch.adoc[catch] processor for defining child processors to be applied only to failed messages.\n\nMore information about error handing can be found in xref:configuration:error_handling.adoc[].\n\n== Nest within a catch block\n\nIn some cases it might be useful to nest a try block within a catch block, since the xref:components:processors/catch.adoc[`catch` processor] only clears errors _after_ executing its child processors this means a nested try processor will not execute unless the errors are explicitly cleared beforehand.\n\nThis can be done by inserting an empty catch block before the try block like as follows:\n\n```yaml\npipeline:\n  processors:\n    - resource: foo\n    - catch:\n      - log:\n          level: ERROR\n          message: \"Foo failed due to: ${! error() }\"\n      - catch: [] # Clear prior error\n      - try:\n        - resource: bar\n        - resource: baz\n```"
      },
      {
        "name": "unarchive",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": "\nWhen a message is unarchived the new messages replace the original message in the batch. Messages that are selected but fail to unarchive (invalid format) will remain unchanged in the message batch but will be flagged as having failed, allowing you to xref:configuration:error_handling.adoc[error handle them].\n\n== Metadata\n\nThe metadata found on the messages handled by this processor will be copied into the resulting messages. For the unarchive formats that contain file information (tar, zip), a metadata field is also added to each message called `archive_filename` with the extracted filename.\n"
      },
      {
        "name": "wasm",
        "type": "processors",
        "status": "experimental",
        "version": "4.11.0",
        "description": "\nThis processor uses https://github.com/tetratelabs/wazero[Wazero^] to execute a WASM module (with support for WASI), calling a specific function for each message being processed. From within the WASM module it is possible to query and mutate the message being processed via a suite of functions exported to the module.\n\nThis ecosystem is delicate as WASM doesn't have a single clearly defined way to pass strings back and forth between the host and the module. In order to remedy this we're gradually working on introducing libraries and examples for multiple languages which can be found in https://github.com/redpanda-data/benthos/tree/main/public/wasm/README.md[the codebase^].\n\nThese examples, as well as the processor itself, is a work in progress.\n\n== Parallelism\n\nIt's not currently possible to execute a single WASM runtime across parallel threads with this processor. Therefore, in order to support parallel processing this processor implements pooling of module runtimes. Ideally your WASM module shouldn't depend on any global state, but if it does then you need to ensure the processor xref:configuration:processing_pipelines.adoc[is only run on a single thread].\n"
      },
      {
        "name": "while",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": "\nThe field `at_least_once`, if true, ensures that the child processors are always executed at least one time (like a do .. while loop.)\n\nThe field `max_loops`, if greater than zero, caps the number of loops for a message batch to this value.\n\nIf following a loop execution the number of messages in a batch is reduced to zero the loop is exited regardless of the condition result. If following a loop execution there are more than 1 message batches the query is checked against the first batch only.\n\nThe conditions of this processor are applied across entire message batches. You can find out more about batching xref:configuration:batching.adoc[in this doc]."
      },
      {
        "name": "workflow",
        "type": "processors",
        "status": "stable",
        "version": "",
        "description": "\n== Why use a workflow\n\n=== Performance\n\nMost of the time the best way to compose processors is also the simplest, just configure them in series. This is because processors are often CPU bound, low-latency, and you can gain vertical scaling by increasing the number of processor pipeline threads, allowing Redpanda Connect to process xref:configuration:processing_pipelines.adoc[multiple messages in parallel].\n\nHowever, some processors such as xref:components:processors/http.adoc[`http`], xref:components:processors/aws_lambda.adoc[`aws_lambda`] or xref:components:processors/cache.adoc[`cache`] interact with external services and therefore spend most of their time waiting for a response. These processors tend to be high-latency and low CPU activity, which causes messages to process slowly.\n\nWhen a processing pipeline contains multiple network processors that aren't dependent on each other we can benefit from performing these processors in parallel for each individual message, reducing the overall message processing latency.\n\n=== Simplifying processor topology\n\nA workflow is often expressed as a https://en.wikipedia.org/wiki/Directed_acyclic_graph[DAG^] of processing stages, where each stage can result in N possible next stages, until finally the flow ends at an exit node.\n\nFor example, if we had processing stages A, B, C and D, where stage A could result in either stage B or C being next, always followed by D, it might look something like this:\n\n```text\n     /--> B --\\\nA --|          |--> D\n     \\--> C --/\n```\n\nThis flow would be easy to express in a standard Redpanda Connect config, we could simply use a xref:components:processors/switch.adoc[`switch` processor] to route to either B or C depending on a condition on the result of A. However, this method of flow control quickly becomes unfeasible as the DAG gets more complicated, imagine expressing this flow using switch processors:\n\n```text\n      /--> B -------------|--> D\n     /                   /\nA --|          /--> E --|\n     \\--> C --|          \\\n               \\----------|--> F\n```\n\nAnd imagine doing so knowing that the diagram is subject to change over time. Yikes! Instead, with a workflow we can either trust it to automatically resolve the DAG or express it manually as simply as `order: [ [ A ], [ B, C ], [ E ], [ D, F ] ]`, and the conditional logic for determining if a stage is executed is defined as part of the branch itself."
      },
      {
        "name": "xml",
        "type": "processors",
        "status": "beta",
        "version": "",
        "description": "\n== Operators\n\n=== `to_json`\n\nConverts an XML document into a JSON structure, where elements appear as keys of an object according to the following rules:\n\n- If an element contains attributes they are parsed by prefixing a hyphen, `-`, to the attribute label.\n- If the element is a simple element and has attributes, the element value is given the key `#text`.\n- XML comments, directives, and process instructions are ignored.\n- When elements are repeated the resulting JSON value is an array.\n\nFor example, given the following XML:\n\n```xml\n<root>\n  <title>This is a title</title>\n  <description tone=\"boring\">This is a description</description>\n  <elements id=\"1\">foo1</elements>\n  <elements id=\"2\">foo2</elements>\n  <elements>foo3</elements>\n</root>\n```\n\nThe resulting JSON structure would look like this:\n\n```json\n{\n  \"root\":{\n    \"title\":\"This is a title\",\n    \"description\":{\n      \"#text\":\"This is a description\",\n      \"-tone\":\"boring\"\n    },\n    \"elements\":[\n      {\"#text\":\"foo1\",\"-id\":\"1\"},\n      {\"#text\":\"foo2\",\"-id\":\"2\"},\n      \"foo3\"\n    ]\n  }\n}\n```\n\nWith cast set to true, the resulting JSON structure would look like this:\n\n```json\n{\n  \"root\":{\n    \"title\":\"This is a title\",\n    \"description\":{\n      \"#text\":\"This is a description\",\n      \"-tone\":\"boring\"\n    },\n    \"elements\":[\n      {\"#text\":\"foo1\",\"-id\":1},\n      {\"#text\":\"foo2\",\"-id\":2},\n      \"foo3\"\n    ]\n  }\n}\n```"
      },
      {
        "name": "local",
        "type": "rate-limits",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "redis",
        "type": "rate-limits",
        "status": "experimental",
        "version": "4.12.0",
        "description": ""
      },
      {
        "name": "aws_cloudwatch",
        "type": "metrics",
        "status": "stable",
        "version": "3.36.0",
        "description": "\n== Timing metrics\n\nThe smallest timing unit that CloudWatch supports is microseconds, therefore timing metrics are automatically downgraded to microseconds (by dividing delta values by 1000). This conversion will also apply to custom timing metrics produced with a `metric` processor.\n\n== Billing\n\nAWS bills per metric series exported, it is therefore STRONGLY recommended that you reduce the metrics that are exposed with a `mapping` like this:\n\n```yaml\nmetrics:\n  mapping: |\n    if ![\n      \"input_received\",\n      \"input_latency\",\n      \"output_sent\",\n    ].contains(this) { deleted() }\n  aws_cloudwatch:\n    namespace: Foo\n```"
      },
      {
        "name": "influxdb",
        "type": "metrics",
        "status": "beta",
        "version": "3.36.0",
        "description": "See https://docs.influxdata.com/influxdb/v1.8/tools/api/#write-http-endpoint for further details on the write API."
      },
      {
        "name": "json_api",
        "type": "metrics",
        "status": "stable",
        "version": "",
        "description": "This metrics type is useful for debugging as it provides a human readable format that you can parse with tools such as `jq`"
      },
      {
        "name": "logger",
        "type": "metrics",
        "status": "beta",
        "version": "",
        "description": "\nPrints each metric produced by Redpanda Connect as a log event (level `info` by default) during shutdown, and optionally on an interval.\n\nThis metrics type is useful for debugging pipelines when you only have access to the logger output and not the service-wide server. Otherwise it's recommended that you use either the `prometheus` or `json_api`types."
      },
      {
        "name": "none",
        "type": "metrics",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "prometheus",
        "type": "metrics",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "statsd",
        "type": "metrics",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "gcp_cloudtrace",
        "type": "tracers",
        "status": "experimental",
        "version": "4.2.0",
        "description": ""
      },
      {
        "name": "jaeger",
        "type": "tracers",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "none",
        "type": "tracers",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "open_telemetry_collector",
        "type": "tracers",
        "status": "experimental",
        "version": "",
        "description": ""
      },
      {
        "name": "avro",
        "type": "scanners",
        "status": "stable",
        "version": "",
        "description": "\n== Avro JSON format\n\nThis scanner yields documents formatted as https://avro.apache.org/docs/current/specification/_print/#json-encoding[Avro JSON^] when decoding with Avro schemas. In this format the value of a union is encoded in JSON as follows:\n\n- if its type is `null`, then it is encoded as a JSON `null`;\n- otherwise it is encoded as a JSON object with one name/value pair whose name is the type's name and whose value is the recursively encoded value. For Avro's named types (record, fixed or enum) the user-specified name is used, for other types the type name is used.\n\nFor example, the union schema `[\"null\",\"string\",\"Foo\"]`, where `Foo` is a record name, would encode:\n\n- `null` as `null`;\n- the string `\"a\"` as `{\"string\": \"a\"}`; and\n- a `Foo` instance as `{\"Foo\": {...}}`, where `{...}` indicates the JSON encoding of a `Foo` instance.\n\nHowever, it is possible to instead create documents in https://pkg.go.dev/github.com/linkedin/goavro/v2#NewCodecForStandardJSONFull[standard/raw JSON format^] by setting the field <<avro_raw_json,`avro_raw_json`>> to `true`.\n\nThis scanner also emits the canonical Avro schema as `@avro_schema` metadata, along with the schema's fingerprint available via `@avro_schema_fingerprint`.\n"
      },
      {
        "name": "chunker",
        "type": "scanners",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "csv",
        "type": "scanners",
        "status": "stable",
        "version": "",
        "description": "\n== Metadata\n\nThis scanner adds the following metadata to each message:\n\n- `csv_row` The index of each row, beginning at 0.\n\n"
      },
      {
        "name": "decompress",
        "type": "scanners",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "json_array",
        "type": "scanners",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "json_documents",
        "type": "scanners",
        "status": "stable",
        "version": "4.27.0",
        "description": ""
      },
      {
        "name": "lines",
        "type": "scanners",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "re_match",
        "type": "scanners",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "skip_bom",
        "type": "scanners",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "switch",
        "type": "scanners",
        "status": "stable",
        "version": "",
        "description": "This scanner outlines a list of potential child scanner candidates to be chosen, and for each source of data the first candidate to pass will be selected. A candidate without any conditions acts as a catch-all and will pass for every source, it is recommended to always have a catch-all scanner at the end of your list. If a given source of data does not pass a candidate an error is returned and the data is rejected."
      },
      {
        "name": "tar",
        "type": "scanners",
        "status": "stable",
        "version": "",
        "description": "\n== Metadata\n\nThis scanner adds the following metadata to each message:\n\n- `tar_name`\n\n"
      },
      {
        "name": "to_the_end",
        "type": "scanners",
        "status": "stable",
        "version": "",
        "description": "\n[CAUTION]\n====\nSome sources of data may not have a logical end, therefore caution should be made to exclusively use this scanner when the end of an input stream is clearly defined (and well within memory).\n====\n"
      },
      {
        "name": "batch_index",
        "type": "bloblang-functions",
        "status": "stable",
        "version": "",
        "description": "Returns the index of the mapped message within a batch. This is useful for applying maps only on certain messages of a batch."
      },
      {
        "name": "batch_size",
        "type": "bloblang-functions",
        "status": "stable",
        "version": "",
        "description": "Returns the size of the message batch."
      },
      {
        "name": "bytes",
        "type": "bloblang-functions",
        "status": "stable",
        "version": "",
        "description": "Create a new byte array that is zero initialized"
      },
      {
        "name": "content",
        "type": "bloblang-functions",
        "status": "stable",
        "version": "",
        "description": "Returns the full raw contents of the mapping target message as a byte array. When mapping to a JSON field the value should be encoded using the method xref:guides:bloblang/methods.adoc#encode[`encode`], or cast to a string directly using the method xref:guides:bloblang/methods.adoc#string[`string`], otherwise it will be base64 encoded by default."
      },
      {
        "name": "count",
        "type": "bloblang-functions",
        "status": "deprecated",
        "version": "",
        "description": "The `count` function is a counter starting at 1 which increments after each time it is called. Count takes an argument which is an identifier for the counter, allowing you to specify multiple unique counters in your configuration."
      },
      {
        "name": "counter",
        "type": "bloblang-functions",
        "status": "experimental",
        "version": "",
        "description": "Returns a non-negative integer that increments each time it is resolved, yielding the minimum (`1` by default) as the first value. Each instantiation of `counter` has its own independent count. Once the maximum integer (or `max` argument) is reached the counter resets back to the minimum."
      },
      {
        "name": "deleted",
        "type": "bloblang-functions",
        "status": "stable",
        "version": "",
        "description": "A function that returns a result indicating that the mapping target should be deleted. Deleting, also known as dropping, messages will result in them being acknowledged as successfully processed to inputs in a Redpanda Connect pipeline. For more information about error handling patterns read xref:configuration:error_handling.adoc[]."
      },
      {
        "name": "env",
        "type": "bloblang-functions",
        "status": "stable",
        "version": "",
        "description": "Returns the value of an environment variable, or `null` if the environment variable does not exist."
      },
      {
        "name": "error",
        "type": "bloblang-functions",
        "status": "stable",
        "version": "",
        "description": "If an error has occurred during the processing of a message this function returns the reported cause of the error as a string, otherwise `null`. For more information about error handling patterns read xref:configuration:error_handling.adoc[]."
      },
      {
        "name": "error_source_label",
        "type": "bloblang-functions",
        "status": "stable",
        "version": "",
        "description": "Returns the label of the source component which raised the error during the processing of a message or an empty string if not set. `null` is returned when the error is null or no source component is associated with it. For more information about error handling patterns read xref:configuration:error_handling.adoc[]."
      },
      {
        "name": "error_source_name",
        "type": "bloblang-functions",
        "status": "stable",
        "version": "",
        "description": "Returns the name of the source component which raised the error during the processing of a message. `null` is returned when the error is null or no source component is associated with it. For more information about error handling patterns read xref:configuration:error_handling.adoc[]."
      },
      {
        "name": "error_source_path",
        "type": "bloblang-functions",
        "status": "stable",
        "version": "",
        "description": "Returns the path of the source component which raised the error during the processing of a message. `null` is returned when the error is null or no source component is associated with it. For more information about error handling patterns read xref:configuration:error_handling.adoc[]."
      },
      {
        "name": "errored",
        "type": "bloblang-functions",
        "status": "stable",
        "version": "",
        "description": "Returns a boolean value indicating whether an error has occurred during the processing of a message. For more information about error handling patterns read xref:configuration:error_handling.adoc[]."
      },
      {
        "name": "fake",
        "type": "bloblang-functions",
        "status": "beta",
        "version": "",
        "description": "Takes in a string that maps to a https://github.com/go-faker/faker[faker^] function and returns the result from that faker function. Returns an error if the given string doesn't match a supported faker function. Supported functions: `latitude`, `longitude`, `unix_time`, `date`, `time_string`, `month_name`, `year_string`, `day_of_week`, `day_of_month`, `timestamp`, `century`, `timezone`, `time_period`, `email`, `mac_address`, `domain_name`, `url`, `username`, `ipv4`, `ipv6`, `password`, `jwt`, `word`, `sentence`, `paragraph`, `cc_type`, `cc_number`, `currency`, `amount_with_currency`, `title_male`, `title_female`, `first_name`, `first_name_male`, `first_name_female`, `last_name`, `name`, `gender`, `chinese_first_name`, `chinese_last_name`, `chinese_name`, `phone_number`, `toll_free_phone_number`, `e164_phone_number`, `uuid_hyphenated`, `uuid_digit`. Refer to the https://github.com/go-faker/faker[faker^] docs for details on these functions."
      },
      {
        "name": "file",
        "type": "bloblang-functions",
        "status": "stable",
        "version": "",
        "description": "Reads a file and returns its contents. Relative paths are resolved from the directory of the process executing the mapping. In order to read files relative to the mapping file use the newer <<file_rel, `file_rel` function>>"
      },
      {
        "name": "file_rel",
        "type": "bloblang-functions",
        "status": "stable",
        "version": "",
        "description": "Reads a file and returns its contents. Relative paths are resolved from the directory of the mapping."
      },
      {
        "name": "hostname",
        "type": "bloblang-functions",
        "status": "stable",
        "version": "",
        "description": "Returns a string matching the hostname of the machine running Benthos."
      },
      {
        "name": "json",
        "type": "bloblang-functions",
        "status": "stable",
        "version": "",
        "description": "Returns the value of a field within a JSON message located by a [dot path][field_paths] argument. This function always targets the entire source JSON document regardless of the mapping context."
      },
      {
        "name": "ksuid",
        "type": "bloblang-functions",
        "status": "stable",
        "version": "",
        "description": "Generates a new ksuid each time it is invoked and prints a string representation."
      },
      {
        "name": "meta",
        "type": "bloblang-functions",
        "status": "deprecated",
        "version": "",
        "description": "Returns the value of a metadata key from the input message as a string, or `null` if the key does not exist. Since values are extracted from the read-only input message they do NOT reflect changes made from within the map. In order to query metadata mutations made within a mapping use the <<root_meta, `root_meta` function>>. This function supports extracting metadata from other messages of a batch with the `from` method."
      },
      {
        "name": "metadata",
        "type": "bloblang-functions",
        "status": "stable",
        "version": "",
        "description": "Returns the value of a metadata key from the input message, or `null` if the key does not exist. Since values are extracted from the read-only input message they do NOT reflect changes made from within the map, in order to query metadata mutations made within a mapping use the xref:guides:bloblang/about.adoc#metadata[`@` operator]. This function supports extracting metadata from other messages of a batch with the `from` method."
      },
      {
        "name": "nanoid",
        "type": "bloblang-functions",
        "status": "stable",
        "version": "",
        "description": "Generates a new nanoid each time it is invoked and prints a string representation."
      },
      {
        "name": "nothing",
        "type": "bloblang-functions",
        "status": "hidden",
        "version": "",
        "description": ""
      },
      {
        "name": "now",
        "type": "bloblang-functions",
        "status": "stable",
        "version": "",
        "description": "Returns the current timestamp as a string in RFC 3339 format with the local timezone. Use the method `ts_format` in order to change the format and timezone."
      },
      {
        "name": "pi",
        "type": "bloblang-functions",
        "status": "stable",
        "version": "",
        "description": "Returns the value of the mathematical constant Pi."
      },
      {
        "name": "random_int",
        "type": "bloblang-functions",
        "status": "stable",
        "version": "",
        "description": "\nGenerates a non-negative pseudo-random 64-bit integer. An optional integer argument can be provided in order to seed the random number generator.\n\nOptional `min` and `max` arguments can be provided in order to only generate numbers within a range. Neither of these parameters can be set via a dynamic expression (i.e. from values taken from mapped data). Instead, for dynamic ranges extract a min and max manually using a modulo operator (`random_int() % a + b`)."
      },
      {
        "name": "range",
        "type": "bloblang-functions",
        "status": "stable",
        "version": "",
        "description": "The `range` function creates an array of integers following a range between a start, stop and optional step integer argument. If the step argument is omitted then it defaults to 1. A negative step can be provided as long as stop < start."
      },
      {
        "name": "root_meta",
        "type": "bloblang-functions",
        "status": "deprecated",
        "version": "",
        "description": "Returns the value of a metadata key from the new message being created as a string, or `null` if the key does not exist. Changes made to metadata during a mapping will be reflected by this function."
      },
      {
        "name": "snowflake_id",
        "type": "bloblang-functions",
        "status": "stable",
        "version": "",
        "description": "Generate a new snowflake ID each time it is invoked and prints a string representation. I.e.: 1559229974454472704"
      },
      {
        "name": "throw",
        "type": "bloblang-functions",
        "status": "stable",
        "version": "",
        "description": "Throws an error similar to a regular mapping error. This is useful for abandoning a mapping entirely given certain conditions."
      },
      {
        "name": "timestamp_unix",
        "type": "bloblang-functions",
        "status": "stable",
        "version": "",
        "description": "Returns the current unix timestamp in seconds."
      },
      {
        "name": "timestamp_unix_micro",
        "type": "bloblang-functions",
        "status": "stable",
        "version": "",
        "description": "Returns the current unix timestamp in microseconds."
      },
      {
        "name": "timestamp_unix_milli",
        "type": "bloblang-functions",
        "status": "stable",
        "version": "",
        "description": "Returns the current unix timestamp in milliseconds."
      },
      {
        "name": "timestamp_unix_nano",
        "type": "bloblang-functions",
        "status": "stable",
        "version": "",
        "description": "Returns the current unix timestamp in nanoseconds."
      },
      {
        "name": "tracing_id",
        "type": "bloblang-functions",
        "status": "experimental",
        "version": "",
        "description": "Provides the message trace id. The returned value will be zeroed if the message does not contain a span."
      },
      {
        "name": "tracing_span",
        "type": "bloblang-functions",
        "status": "experimental",
        "version": "",
        "description": "Provides the message tracing span xref:components:tracers/about.adoc[(created via Open Telemetry APIs)] as an object serialized via text map formatting. The returned value will be `null` if the message does not have a span."
      },
      {
        "name": "ulid",
        "type": "bloblang-functions",
        "status": "experimental",
        "version": "",
        "description": "Generate a random ULID."
      },
      {
        "name": "uuid_v4",
        "type": "bloblang-functions",
        "status": "stable",
        "version": "",
        "description": "Generates a new RFC-4122 UUID each time it is invoked and prints a string representation."
      },
      {
        "name": "uuid_v7",
        "type": "bloblang-functions",
        "status": "stable",
        "version": "",
        "description": "Generates a new time ordered UUID each time it is invoked and prints a string representation."
      },
      {
        "name": "var",
        "type": "bloblang-functions",
        "status": "hidden",
        "version": "",
        "description": ""
      },
      {
        "name": "with_schema_registry_header",
        "type": "bloblang-functions",
        "status": "beta",
        "version": "",
        "description": "Prepends a 5-byte Schema Registry header to a message. The header consists of a magic byte (0x00) followed by a 4-byte big-endian schema ID."
      },
      {
        "name": "abs",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Returns the absolute value of an int64 or float64 number. As a special case, when an integer is provided that is the minimum value it is converted to the maximum value."
      },
      {
        "name": "all",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Checks each element of an array against a query and returns true if all elements passed. An error occurs if the target is not an array, or if any element results in the provided query returning a non-boolean result. Returns false if the target array is empty."
      },
      {
        "name": "any",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Checks the elements of an array against a query and returns true if any element passes. An error occurs if the target is not an array, or if an element results in the provided query returning a non-boolean result. Returns false if the target array is empty."
      },
      {
        "name": "append",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Returns an array with new elements appended to the end."
      },
      {
        "name": "apply",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Apply a declared mapping to a target value."
      },
      {
        "name": "array",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "assign",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Merge a source object into an existing destination object. When a collision is found within the merged structures (both a source and destination object contain the same non-object keys) the value in the destination object will be overwritten by that of source object. In order to preserve both values on collision use the <<merge, `merge`>> method."
      },
      {
        "name": "bitwise_and",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Returns the number bitwise AND-ed with the specified value."
      },
      {
        "name": "bitwise_or",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Returns the number bitwise OR-ed with the specified value."
      },
      {
        "name": "bitwise_xor",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Returns the number bitwise eXclusive-OR-ed with the specified value."
      },
      {
        "name": "bloblang",
        "type": "bloblang-methods",
        "status": "beta",
        "version": "",
        "description": "Executes an argument Bloblang mapping on the target. This method can be used in order to execute dynamic mappings. Imports and functions that interact with the environment, such as `file` and `env`, or that access message information directly, such as `content` or `json`, are not enabled for dynamic Bloblang mappings."
      },
      {
        "name": "bool",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "bytes",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "capitalize",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "catch",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "If the result of a target query fails (due to incorrect types, failed parsing, etc) the argument is returned instead."
      },
      {
        "name": "ceil",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Returns the least integer value greater than or equal to a number. If the resulting value fits within a 64-bit integer then that is returned, otherwise a new floating point number is returned."
      },
      {
        "name": "collapse",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "compare_argon2",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Checks whether a string matches a hashed secret using Argon2."
      },
      {
        "name": "compare_bcrypt",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Checks whether a string matches a hashed secret using bcrypt."
      },
      {
        "name": "compress",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Compresses a string or byte array value according to a specified algorithm."
      },
      {
        "name": "concat",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Concatenates an array value with one or more argument arrays."
      },
      {
        "name": "contains",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "cos",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Calculates the cosine of a given angle specified in radians."
      },
      {
        "name": "decode",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "decompress",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Decompresses a string or byte array value according to a specified algorithm. The result of decompression "
      },
      {
        "name": "decrypt_aes",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "diff",
        "type": "bloblang-methods",
        "status": "beta",
        "version": "4.25.0",
        "description": "Create a diff by comparing the current value with the given one. Wraps the github.com/r3labs/diff/v3 package. See its https://pkg.go.dev/github.com/r3labs/diff/v3[docs^] for more information."
      },
      {
        "name": "encode",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "encrypt_aes",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "enumerated",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Converts an array into a new array of objects, where each object has a field index containing the `index` of the element and a field `value` containing the original value of the element."
      },
      {
        "name": "escape_html",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "escape_url_query",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "exists",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Checks that a field, identified via a xref:configuration:field_paths.adoc[dot path], exists in an object."
      },
      {
        "name": "explode",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "filepath_join",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "filepath_split",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "filter",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "find",
        "type": "bloblang-methods",
        "status": "beta",
        "version": "",
        "description": "Returns the index of the first occurrence of a value in an array. `-1` is returned if there are no matches. Numerical comparisons are made irrespective of the representation type (float versus integer)."
      },
      {
        "name": "find_all",
        "type": "bloblang-methods",
        "status": "beta",
        "version": "",
        "description": "Returns an array containing the indexes of all occurrences of a value in an array. An empty array is returned if there are no matches. Numerical comparisons are made irrespective of the representation type (float versus integer)."
      },
      {
        "name": "find_all_by",
        "type": "bloblang-methods",
        "status": "beta",
        "version": "",
        "description": "Returns an array containing the indexes of all occurrences of an array where the provided query resolves to a boolean `true`. An empty array is returned if there are no matches. Numerical comparisons are made irrespective of the representation type (float versus integer)."
      },
      {
        "name": "find_by",
        "type": "bloblang-methods",
        "status": "beta",
        "version": "",
        "description": "Returns the index of the first occurrence of an array where the provided query resolves to a boolean `true`. `-1` is returned if there are no matches."
      },
      {
        "name": "flatten",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Iterates an array and any element that is itself an array is removed and has its elements inserted directly in the resulting array."
      },
      {
        "name": "float32",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "\nConverts a numerical type into a 32-bit floating point number, this is for advanced use cases where a specific data type is needed for a given component (such as the ClickHouse SQL driver).\n\nIf the value is a string then an attempt will be made to parse it as a 32-bit floating point number. Please refer to the https://pkg.go.dev/strconv#ParseFloat[`strconv.ParseFloat` documentation] for details regarding the supported formats."
      },
      {
        "name": "float64",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "\nConverts a numerical type into a 64-bit floating point number, this is for advanced use cases where a specific data type is needed for a given component (such as the ClickHouse SQL driver).\n\nIf the value is a string then an attempt will be made to parse it as a 64-bit floating point number. Please refer to the https://pkg.go.dev/strconv#ParseFloat[`strconv.ParseFloat` documentation] for details regarding the supported formats."
      },
      {
        "name": "floor",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Returns the greatest integer value less than or equal to the target number. If the resulting value fits within a 64-bit integer then that is returned, otherwise a new floating point number is returned."
      },
      {
        "name": "fold",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Takes two arguments: an initial value, and a mapping query. For each element of an array the mapping context is an object with two fields `tally` and `value`, where `tally` contains the current accumulated value and `value` is the value of the current element. The mapping must return the result of adding the value to the tally.\n\nThe first argument is the value that `tally` will have on the first call."
      },
      {
        "name": "format",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "format_json",
        "type": "bloblang-methods",
        "status": "beta",
        "version": "",
        "description": ""
      },
      {
        "name": "format_msgpack",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Formats data as a https://msgpack.org/[MessagePack^] message in bytes format."
      },
      {
        "name": "format_timestamp",
        "type": "bloblang-methods",
        "status": "deprecated",
        "version": "",
        "description": "Attempts to format a timestamp value as a string according to a specified format, or RFC 3339 by default. Timestamp values can either be a numerical unix time in seconds (with up to nanosecond precision via decimals), or a string in RFC 3339 format.\n\nThe output format is defined by showing how the reference time, defined to be Mon Jan 2 15:04:05 -0700 MST 2006, would be displayed if it were the value. For an alternative way to specify formats check out the <<ts_strftime, `ts_strftime`>> method."
      },
      {
        "name": "format_timestamp_strftime",
        "type": "bloblang-methods",
        "status": "deprecated",
        "version": "",
        "description": "Attempts to format a timestamp value as a string according to a specified strftime-compatible format. Timestamp values can either be a numerical unix time in seconds (with up to nanosecond precision via decimals), or a string in RFC 3339 format."
      },
      {
        "name": "format_timestamp_unix",
        "type": "bloblang-methods",
        "status": "deprecated",
        "version": "",
        "description": "Attempts to format a timestamp value as a unix timestamp. Timestamp values can either be a numerical unix time in seconds (with up to nanosecond precision via decimals), or a string in RFC 3339 format. The <<ts_parse, `ts_parse`>> method can be used in order to parse different timestamp formats."
      },
      {
        "name": "format_timestamp_unix_micro",
        "type": "bloblang-methods",
        "status": "deprecated",
        "version": "",
        "description": "Attempts to format a timestamp value as a unix timestamp with microsecond precision. Timestamp values can either be a numerical unix time in seconds (with up to nanosecond precision via decimals), or a string in RFC 3339 format. The <<ts_parse, `ts_parse`>> method can be used in order to parse different timestamp formats."
      },
      {
        "name": "format_timestamp_unix_milli",
        "type": "bloblang-methods",
        "status": "deprecated",
        "version": "",
        "description": "Attempts to format a timestamp value as a unix timestamp with millisecond precision. Timestamp values can either be a numerical unix time in seconds (with up to nanosecond precision via decimals), or a string in RFC 3339 format. The <<ts_parse, `ts_parse`>> method can be used in order to parse different timestamp formats."
      },
      {
        "name": "format_timestamp_unix_nano",
        "type": "bloblang-methods",
        "status": "deprecated",
        "version": "",
        "description": "Attempts to format a timestamp value as a unix timestamp with nanosecond precision. Timestamp values can either be a numerical unix time in seconds (with up to nanosecond precision via decimals), or a string in RFC 3339 format. The <<ts_parse, `ts_parse`>> method can be used in order to parse different timestamp formats."
      },
      {
        "name": "format_xml",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "\nSerializes a target value into an XML byte array.\n"
      },
      {
        "name": "format_yaml",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "from",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Modifies a target query such that certain functions are executed from the perspective of another message in the batch. This allows you to mutate events based on the contents of other messages. Functions that support this behavior are `content`, `json` and `meta`."
      },
      {
        "name": "from_all",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Modifies a target query such that certain functions are executed from the perspective of each message in the batch, and returns the set of results as an array. Functions that support this behavior are `content`, `json` and `meta`."
      },
      {
        "name": "geoip_anonymous_ip",
        "type": "bloblang-methods",
        "status": "experimental",
        "version": "",
        "description": "Looks up an IP address against a https://www.maxmind.com/en/home[MaxMind database file^] and, if found, returns an object describing the anonymous IP associated with it."
      },
      {
        "name": "geoip_asn",
        "type": "bloblang-methods",
        "status": "experimental",
        "version": "",
        "description": "Looks up an IP address against a https://www.maxmind.com/en/home[MaxMind database file^] and, if found, returns an object describing the ASN associated with it."
      },
      {
        "name": "geoip_city",
        "type": "bloblang-methods",
        "status": "experimental",
        "version": "",
        "description": "Looks up an IP address against a https://www.maxmind.com/en/home[MaxMind database file^] and, if found, returns an object describing the city associated with it."
      },
      {
        "name": "geoip_connection_type",
        "type": "bloblang-methods",
        "status": "experimental",
        "version": "",
        "description": "Looks up an IP address against a https://www.maxmind.com/en/home[MaxMind database file^] and, if found, returns an object describing the connection type associated with it."
      },
      {
        "name": "geoip_country",
        "type": "bloblang-methods",
        "status": "experimental",
        "version": "",
        "description": "Looks up an IP address against a https://www.maxmind.com/en/home[MaxMind database file^] and, if found, returns an object describing the country associated with it."
      },
      {
        "name": "geoip_domain",
        "type": "bloblang-methods",
        "status": "experimental",
        "version": "",
        "description": "Looks up an IP address against a https://www.maxmind.com/en/home[MaxMind database file^] and, if found, returns an object describing the domain associated with it."
      },
      {
        "name": "geoip_enterprise",
        "type": "bloblang-methods",
        "status": "experimental",
        "version": "",
        "description": "Looks up an IP address against a https://www.maxmind.com/en/home[MaxMind database file^] and, if found, returns an object describing the enterprise associated with it."
      },
      {
        "name": "geoip_isp",
        "type": "bloblang-methods",
        "status": "experimental",
        "version": "",
        "description": "Looks up an IP address against a https://www.maxmind.com/en/home[MaxMind database file^] and, if found, returns an object describing the ISP associated with it."
      },
      {
        "name": "get",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Extract a field value, identified via a xref:configuration:field_paths.adoc[dot path], from an object."
      },
      {
        "name": "has_prefix",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "has_suffix",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "hash",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "index",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Extract an element from an array by an index. The index can be negative, and if so the element will be selected from the end counting backwards starting from -1. E.g. an index of -1 returns the last element, an index of -2 returns the element before the last, and so on."
      },
      {
        "name": "index_of",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "infer_schema",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Attempt to infer the schema of a given value. The resulting schema can then be used as an input to schema conversion and enforcement methods."
      },
      {
        "name": "int16",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "\nConverts a numerical type into a 16-bit signed integer, this is for advanced use cases where a specific data type is needed for a given component (such as the ClickHouse SQL driver).\n\nIf the value is a string then an attempt will be made to parse it as a 16-bit signed integer. If the target value exceeds the capacity of an integer or contains decimal values then this method will throw an error. In order to convert a floating point number containing decimals first use <<round, `.round()`>> on the value. Please refer to the https://pkg.go.dev/strconv#ParseInt[`strconv.ParseInt` documentation] for details regarding the supported formats."
      },
      {
        "name": "int32",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "\nConverts a numerical type into a 32-bit signed integer, this is for advanced use cases where a specific data type is needed for a given component (such as the ClickHouse SQL driver).\n\nIf the value is a string then an attempt will be made to parse it as a 32-bit signed integer. If the target value exceeds the capacity of an integer or contains decimal values then this method will throw an error. In order to convert a floating point number containing decimals first use <<round, `.round()`>> on the value. Please refer to the https://pkg.go.dev/strconv#ParseInt[`strconv.ParseInt` documentation] for details regarding the supported formats."
      },
      {
        "name": "int64",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "\nConverts a numerical type into a 64-bit signed integer, this is for advanced use cases where a specific data type is needed for a given component (such as the ClickHouse SQL driver).\n\nIf the value is a string then an attempt will be made to parse it as a 64-bit signed integer. If the target value exceeds the capacity of an integer or contains decimal values then this method will throw an error. In order to convert a floating point number containing decimals first use <<round, `.round()`>> on the value. Please refer to the https://pkg.go.dev/strconv#ParseInt[`strconv.ParseInt` documentation] for details regarding the supported formats."
      },
      {
        "name": "int8",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "\nConverts a numerical type into a 8-bit signed integer, this is for advanced use cases where a specific data type is needed for a given component (such as the ClickHouse SQL driver).\n\nIf the value is a string then an attempt will be made to parse it as a 8-bit signed integer. If the target value exceeds the capacity of an integer or contains decimal values then this method will throw an error. In order to convert a floating point number containing decimals first use <<round, `.round()`>> on the value. Please refer to the https://pkg.go.dev/strconv#ParseInt[`strconv.ParseInt` documentation] for details regarding the supported formats."
      },
      {
        "name": "join",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "json_path",
        "type": "bloblang-methods",
        "status": "experimental",
        "version": "",
        "description": "Executes the given JSONPath expression on an object or array and returns the result. The JSONPath expression syntax can be found at https://goessner.net/articles/JsonPath/. For more complex logic, you can use Gval expressions (https://github.com/PaesslerAG/gval)."
      },
      {
        "name": "json_schema",
        "type": "bloblang-methods",
        "status": "beta",
        "version": "",
        "description": "Checks a https://json-schema.org/[JSON schema^] against a value and returns the value if it matches or throws and error if it does not."
      },
      {
        "name": "key_values",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Returns the key/value pairs of an object as an array, where each element is an object with a `key` field and a `value` field. The order of the resulting array will be random."
      },
      {
        "name": "keys",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Returns the keys of an object as an array."
      },
      {
        "name": "length",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "log",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Returns the natural logarithm of a number."
      },
      {
        "name": "log10",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Returns the decimal logarithm of a number."
      },
      {
        "name": "lowercase",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "map",
        "type": "bloblang-methods",
        "status": "hidden",
        "version": "",
        "description": ""
      },
      {
        "name": "map_each",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "map_each_key",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "max",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Returns the largest numerical value found within an array. All values must be numerical and the array must not be empty, otherwise an error is returned."
      },
      {
        "name": "merge",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Merge a source object into an existing destination object. When a collision is found within the merged structures (both a source and destination object contain the same non-object keys) the result will be an array containing both values, where values that are already arrays will be expanded into the resulting array. In order to simply override destination fields on collision use the <<assign, `assign`>> method."
      },
      {
        "name": "min",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Returns the smallest numerical value found within an array. All values must be numerical and the array must not be empty, otherwise an error is returned."
      },
      {
        "name": "not",
        "type": "bloblang-methods",
        "status": "hidden",
        "version": "",
        "description": ""
      },
      {
        "name": "not_empty",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "not_null",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "number",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "or",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "If the result of the target query fails or resolves to `null`, returns the argument instead. This is an explicit method alternative to the coalesce pipe operator `|`."
      },
      {
        "name": "parse_csv",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "parse_duration",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Attempts to parse a string as a duration and returns an integer of nanoseconds. A duration string is a possibly signed sequence of decimal numbers, each with an optional fraction and a unit suffix, such as \"300ms\", \"-1.5h\" or \"2h45m\". Valid time units are \"ns\", \"us\" (or \"s\"), \"ms\", \"s\", \"m\", \"h\"."
      },
      {
        "name": "parse_duration_iso8601",
        "type": "bloblang-methods",
        "status": "beta",
        "version": "",
        "description": "Attempts to parse a string using ISO-8601 rules as a duration and returns an integer of nanoseconds. A duration string is represented by the format \"P[n]Y[n]M[n]DT[n]H[n]M[n]S\" or \"P[n]W\". In these representations, the \"[n]\" is replaced by the value for each of the date and time elements that follow the \"[n]\". For example, \"P3Y6M4DT12H30M5S\" represents a duration of \"three years, six months, four days, twelve hours, thirty minutes, and five seconds\". The last field of the format allows fractions with one decimal place, so \"P3.5S\" will return 3500000000ns. Any additional decimals will be truncated."
      },
      {
        "name": "parse_form_url_encoded",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Attempts to parse a url-encoded query string (from an x-www-form-urlencoded request body) and returns a structured result."
      },
      {
        "name": "parse_json",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "parse_jwt_es256",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "v4.20.0",
        "description": "Parses a claims object from a JWT string encoded with ES256. This method does not validate JWT claims."
      },
      {
        "name": "parse_jwt_es384",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "v4.20.0",
        "description": "Parses a claims object from a JWT string encoded with ES384. This method does not validate JWT claims."
      },
      {
        "name": "parse_jwt_es512",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "v4.20.0",
        "description": "Parses a claims object from a JWT string encoded with ES512. This method does not validate JWT claims."
      },
      {
        "name": "parse_jwt_hs256",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "v4.12.0",
        "description": "Parses a claims object from a JWT string encoded with HS256. This method does not validate JWT claims."
      },
      {
        "name": "parse_jwt_hs384",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "v4.12.0",
        "description": "Parses a claims object from a JWT string encoded with HS384. This method does not validate JWT claims."
      },
      {
        "name": "parse_jwt_hs512",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "v4.12.0",
        "description": "Parses a claims object from a JWT string encoded with HS512. This method does not validate JWT claims."
      },
      {
        "name": "parse_jwt_rs256",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "v4.20.0",
        "description": "Parses a claims object from a JWT string encoded with RS256. This method does not validate JWT claims."
      },
      {
        "name": "parse_jwt_rs384",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "v4.20.0",
        "description": "Parses a claims object from a JWT string encoded with RS384. This method does not validate JWT claims."
      },
      {
        "name": "parse_jwt_rs512",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "v4.20.0",
        "description": "Parses a claims object from a JWT string encoded with RS512. This method does not validate JWT claims."
      },
      {
        "name": "parse_msgpack",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Parses a https://msgpack.org/[MessagePack^] message into a structured document."
      },
      {
        "name": "parse_parquet",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Decodes a https://parquet.apache.org/docs/[Parquet file^] into an array of objects, one for each row within the file."
      },
      {
        "name": "parse_timestamp",
        "type": "bloblang-methods",
        "status": "deprecated",
        "version": "",
        "description": "Attempts to parse a string as a timestamp following a specified format and outputs a timestamp, which can then be fed into methods such as <<ts_format, `ts_format`>>.\n\nThe input format is defined by showing how the reference time, defined to be Mon Jan 2 15:04:05 -0700 MST 2006, would be displayed if it were the value. For an alternative way to specify formats check out the <<ts_strptime, `ts_strptime`>> method."
      },
      {
        "name": "parse_timestamp_strptime",
        "type": "bloblang-methods",
        "status": "deprecated",
        "version": "",
        "description": "Attempts to parse a string as a timestamp following a specified strptime-compatible format and outputs a timestamp, which can then be fed into <<ts_format, `ts_format`>>."
      },
      {
        "name": "parse_url",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Attempts to parse a URL from a string value, returning a structured result that describes the various facets of the URL. The fields returned within the structured result roughly follow https://pkg.go.dev/net/url#URL, and may be expanded in future in order to present more information."
      },
      {
        "name": "parse_xml",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "\nAttempts to parse a string as an XML document and returns a structured result, where elements appear as keys of an object according to the following rules:\n\n- If an element contains attributes they are parsed by prefixing a hyphen, `-`, to the attribute label.\n- If the element is a simple element and has attributes, the element value is given the key `#text`.\n- XML comments, directives, and process instructions are ignored.\n- When elements are repeated the resulting JSON value is an array.\n- If cast is true, try to cast values to numbers and booleans instead of returning strings.\n"
      },
      {
        "name": "parse_yaml",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "patch",
        "type": "bloblang-methods",
        "status": "beta",
        "version": "4.25.0",
        "description": "Create a diff by comparing the current value with the given one. Wraps the github.com/r3labs/diff/v3 package. See its https://pkg.go.dev/github.com/r3labs/diff/v3[docs^] for more information."
      },
      {
        "name": "pow",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Returns the number raised to the specified exponent."
      },
      {
        "name": "quote",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "re_find_all",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "re_find_all_object",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "re_find_all_submatch",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "re_find_object",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "re_match",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "re_replace",
        "type": "bloblang-methods",
        "status": "hidden",
        "version": "",
        "description": ""
      },
      {
        "name": "re_replace_all",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "repeat",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "replace",
        "type": "bloblang-methods",
        "status": "hidden",
        "version": "",
        "description": ""
      },
      {
        "name": "replace_all",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "replace_all_many",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "replace_many",
        "type": "bloblang-methods",
        "status": "hidden",
        "version": "",
        "description": ""
      },
      {
        "name": "reverse",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "round",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Rounds numbers to the nearest integer, rounding half away from zero. If the resulting value fits within a 64-bit integer then that is returned, otherwise a new floating point number is returned."
      },
      {
        "name": "sign_jwt_es256",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "v4.20.0",
        "description": "Hash and sign an object representing JSON Web Token (JWT) claims using ES256."
      },
      {
        "name": "sign_jwt_es384",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "v4.20.0",
        "description": "Hash and sign an object representing JSON Web Token (JWT) claims using ES384."
      },
      {
        "name": "sign_jwt_es512",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "v4.20.0",
        "description": "Hash and sign an object representing JSON Web Token (JWT) claims using ES512."
      },
      {
        "name": "sign_jwt_hs256",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "v4.12.0",
        "description": "Hash and sign an object representing JSON Web Token (JWT) claims using HS256."
      },
      {
        "name": "sign_jwt_hs384",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "v4.12.0",
        "description": "Hash and sign an object representing JSON Web Token (JWT) claims using HS384."
      },
      {
        "name": "sign_jwt_hs512",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "v4.12.0",
        "description": "Hash and sign an object representing JSON Web Token (JWT) claims using HS512."
      },
      {
        "name": "sign_jwt_rs256",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "v4.18.0",
        "description": "Hash and sign an object representing JSON Web Token (JWT) claims using RS256."
      },
      {
        "name": "sign_jwt_rs384",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "v4.18.0",
        "description": "Hash and sign an object representing JSON Web Token (JWT) claims using RS384."
      },
      {
        "name": "sign_jwt_rs512",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "v4.18.0",
        "description": "Hash and sign an object representing JSON Web Token (JWT) claims using RS512."
      },
      {
        "name": "sin",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Calculates the sine of a given angle specified in radians."
      },
      {
        "name": "slice",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "slug",
        "type": "bloblang-methods",
        "status": "beta",
        "version": "4.2.0",
        "description": "Creates a \"slug\" from a given string. Wraps the github.com/gosimple/slug package. See its https://pkg.go.dev/github.com/gosimple/slug[docs^] for more information."
      },
      {
        "name": "sort",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "sort_by",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "split",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "squash",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Squashes an array of objects into a single object, where key collisions result in the values being merged (following similar rules as the `.merge()` method)"
      },
      {
        "name": "string",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "strip_html",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Attempts to remove all HTML tags from a target string."
      },
      {
        "name": "sum",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "tan",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Calculates the tangent of a given angle specified in radians."
      },
      {
        "name": "timestamp",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "trim",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "trim_prefix",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "4.12.0",
        "description": ""
      },
      {
        "name": "trim_suffix",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "4.12.0",
        "description": ""
      },
      {
        "name": "ts_add_iso8601",
        "type": "bloblang-methods",
        "status": "beta",
        "version": "",
        "description": "Parse parameter string as ISO 8601 period and add it to value with high precision for units larger than an hour."
      },
      {
        "name": "ts_format",
        "type": "bloblang-methods",
        "status": "beta",
        "version": "",
        "description": "Attempts to format a timestamp value as a string according to a specified format, or RFC 3339 by default. Timestamp values can either be a numerical unix time in seconds (with up to nanosecond precision via decimals), or a string in RFC 3339 format.\n\nThe output format is defined by showing how the reference time, defined to be Mon Jan 2 15:04:05 -0700 MST 2006, would be displayed if it were the value. For an alternative way to specify formats check out the <<ts_strftime, `ts_strftime`>> method."
      },
      {
        "name": "ts_parse",
        "type": "bloblang-methods",
        "status": "beta",
        "version": "",
        "description": "Attempts to parse a string as a timestamp following a specified format and outputs a timestamp, which can then be fed into methods such as <<ts_format, `ts_format`>>.\n\nThe input format is defined by showing how the reference time, defined to be Mon Jan 2 15:04:05 -0700 MST 2006, would be displayed if it were the value. For an alternative way to specify formats check out the <<ts_strptime, `ts_strptime`>> method."
      },
      {
        "name": "ts_round",
        "type": "bloblang-methods",
        "status": "beta",
        "version": "4.2.0",
        "description": "Returns the result of rounding a timestamp to the nearest multiple of the argument duration (nanoseconds). The rounding behavior for halfway values is to round up. Timestamp values can either be a numerical unix time in seconds (with up to nanosecond precision via decimals), or a string in RFC 3339 format. The <<ts_parse, `ts_parse`>> method can be used in order to parse different timestamp formats."
      },
      {
        "name": "ts_strftime",
        "type": "bloblang-methods",
        "status": "beta",
        "version": "",
        "description": "Attempts to format a timestamp value as a string according to a specified strftime-compatible format. Timestamp values can either be a numerical unix time in seconds (with up to nanosecond precision via decimals), or a string in RFC 3339 format."
      },
      {
        "name": "ts_strptime",
        "type": "bloblang-methods",
        "status": "beta",
        "version": "",
        "description": "Attempts to parse a string as a timestamp following a specified strptime-compatible format and outputs a timestamp, which can then be fed into <<ts_format, `ts_format`>>."
      },
      {
        "name": "ts_sub",
        "type": "bloblang-methods",
        "status": "beta",
        "version": "4.23.0",
        "description": "Returns the difference in nanoseconds between the target timestamp (t1) and the timestamp provided as a parameter (t2). The <<ts_parse, `ts_parse`>> method can be used in order to parse different timestamp formats."
      },
      {
        "name": "ts_sub_iso8601",
        "type": "bloblang-methods",
        "status": "beta",
        "version": "",
        "description": "Parse parameter string as ISO 8601 period and subtract it from value with high precision for units larger than an hour."
      },
      {
        "name": "ts_tz",
        "type": "bloblang-methods",
        "status": "beta",
        "version": "4.3.0",
        "description": "Returns the result of converting a timestamp to a specified timezone. Timestamp values can either be a numerical unix time in seconds (with up to nanosecond precision via decimals), or a string in RFC 3339 format. The <<ts_parse, `ts_parse`>> method can be used in order to parse different timestamp formats."
      },
      {
        "name": "ts_unix",
        "type": "bloblang-methods",
        "status": "beta",
        "version": "",
        "description": "Attempts to format a timestamp value as a unix timestamp. Timestamp values can either be a numerical unix time in seconds (with up to nanosecond precision via decimals), or a string in RFC 3339 format. The <<ts_parse, `ts_parse`>> method can be used in order to parse different timestamp formats."
      },
      {
        "name": "ts_unix_micro",
        "type": "bloblang-methods",
        "status": "beta",
        "version": "",
        "description": "Attempts to format a timestamp value as a unix timestamp with microsecond precision. Timestamp values can either be a numerical unix time in seconds (with up to nanosecond precision via decimals), or a string in RFC 3339 format. The <<ts_parse, `ts_parse`>> method can be used in order to parse different timestamp formats."
      },
      {
        "name": "ts_unix_milli",
        "type": "bloblang-methods",
        "status": "beta",
        "version": "",
        "description": "Attempts to format a timestamp value as a unix timestamp with millisecond precision. Timestamp values can either be a numerical unix time in seconds (with up to nanosecond precision via decimals), or a string in RFC 3339 format. The <<ts_parse, `ts_parse`>> method can be used in order to parse different timestamp formats."
      },
      {
        "name": "ts_unix_nano",
        "type": "bloblang-methods",
        "status": "beta",
        "version": "",
        "description": "Attempts to format a timestamp value as a unix timestamp with nanosecond precision. Timestamp values can either be a numerical unix time in seconds (with up to nanosecond precision via decimals), or a string in RFC 3339 format. The <<ts_parse, `ts_parse`>> method can be used in order to parse different timestamp formats."
      },
      {
        "name": "type",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "uint16",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "\nConverts a numerical type into a 16-bit unsigned integer, this is for advanced use cases where a specific data type is needed for a given component (such as the ClickHouse SQL driver).\n\nIf the value is a string then an attempt will be made to parse it as a 16-bit unsigned integer. If the target value exceeds the capacity of an integer or contains decimal values then this method will throw an error. In order to convert a floating point number containing decimals first use <<round, `.round()`>> on the value. Please refer to the https://pkg.go.dev/strconv#ParseInt[`strconv.ParseInt` documentation] for details regarding the supported formats."
      },
      {
        "name": "uint32",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "\nConverts a numerical type into a 32-bit unsigned integer, this is for advanced use cases where a specific data type is needed for a given component (such as the ClickHouse SQL driver).\n\nIf the value is a string then an attempt will be made to parse it as a 32-bit unsigned integer. If the target value exceeds the capacity of an integer or contains decimal values then this method will throw an error. In order to convert a floating point number containing decimals first use <<round, `.round()`>> on the value. Please refer to the https://pkg.go.dev/strconv#ParseInt[`strconv.ParseInt` documentation] for details regarding the supported formats."
      },
      {
        "name": "uint64",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "\nConverts a numerical type into a 64-bit unsigned integer, this is for advanced use cases where a specific data type is needed for a given component (such as the ClickHouse SQL driver).\n\nIf the value is a string then an attempt will be made to parse it as a 64-bit unsigned integer. If the target value exceeds the capacity of an integer or contains decimal values then this method will throw an error. In order to convert a floating point number containing decimals first use <<round, `.round()`>> on the value. Please refer to the https://pkg.go.dev/strconv#ParseInt[`strconv.ParseInt` documentation] for details regarding the supported formats."
      },
      {
        "name": "uint8",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "\nConverts a numerical type into a 8-bit unsigned integer, this is for advanced use cases where a specific data type is needed for a given component (such as the ClickHouse SQL driver).\n\nIf the value is a string then an attempt will be made to parse it as a 8-bit unsigned integer. If the target value exceeds the capacity of an integer or contains decimal values then this method will throw an error. In order to convert a floating point number containing decimals first use <<round, `.round()`>> on the value. Please refer to the https://pkg.go.dev/strconv#ParseInt[`strconv.ParseInt` documentation] for details regarding the supported formats."
      },
      {
        "name": "unescape_html",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "unescape_url_query",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "unicode_segments",
        "type": "bloblang-methods",
        "status": "beta",
        "version": "",
        "description": "Splits text into segments from a given string based on the unicode text segmentation rules."
      },
      {
        "name": "unique",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "unquote",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "uppercase",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "uuid_v5",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "values",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "vector",
        "type": "bloblang-methods",
        "status": "beta",
        "version": "4.33.0",
        "description": "Creates a vector from a given array of floating point numbers.\n\nThis vector can be inserted into various SQL databases if they have support for embeddings vectors (for example `pgvector`)."
      },
      {
        "name": "with",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Returns an object where all but one or more xref:configuration:field_paths.adoc[field path] arguments are removed. Each path specifies a specific field to be retained from the input object, allowing for nested fields.\n\nIf a key within a nested path does not exist then it is ignored."
      },
      {
        "name": "without",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": ""
      },
      {
        "name": "zip",
        "type": "bloblang-methods",
        "status": "stable",
        "version": "",
        "description": "Zip an array value with one or more argument arrays. Each array must match in length."
      }
    ],
    "removedComponents": [],
    "newFields": [],
    "removedFields": [],
    "deprecatedComponents": [],
    "deprecatedFields": [],
    "changedDefaults": []
  }
}