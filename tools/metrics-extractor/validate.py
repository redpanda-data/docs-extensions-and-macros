#!/usr/bin/env python3
"""
Integration script to validate the complete metrics extraction pipeline
"""
import os
import sys
import json
import subprocess
from pathlib import Path


def check_dependencies():
    """Check if all required dependencies are available"""
    print("ğŸ”§ Checking dependencies...")
    
    try:
        import tree_sitter
        print("  âœ“ tree-sitter is available")
    except ImportError:
        print("  âŒ tree-sitter not found. Install with: pip install tree-sitter")
        return False
    
    # Check if we can import our modules
    try:
        from metrics_parser import build_treesitter_cpp_library, extract_metrics_from_files
        from metrics_bag import MetricsBag
        print("  âœ“ All custom modules are available")
    except ImportError as e:
        print(f"  âŒ Import error: {e}")
        return False
    
    return True


def test_end_to_end():
    """Test the complete extraction pipeline"""
    print("\nğŸ§ª Running end-to-end test...")
    
    try:
        # Run the example script
        result = subprocess.run([
            sys.executable, "example.py", "--verbose"
        ], capture_output=True, text=True)
        
        if result.returncode == 0:
            print("  âœ“ Example script completed successfully")
            return True
        else:
            print(f"  âŒ Example script failed:")
            print(f"     stdout: {result.stdout}")
            print(f"     stderr: {result.stderr}")
            return False
    except Exception as e:
        print(f"  âŒ Error running example: {e}")
        return False


def validate_cli_integration():
    """Validate that the CLI integration works"""
    print("\nğŸ”— Validating CLI integration...")
    
    # Check if the doc-tools.js file has our new command
    doc_tools_path = Path("../../bin/doc-tools.js")
    if not doc_tools_path.exists():
        print("  âŒ doc-tools.js not found")
        return False
    
    with open(doc_tools_path, 'r') as f:
        content = f.read()
    
    if 'source-metrics-docs' in content:
        print("  âœ“ source-metrics-docs command found in doc-tools.js")
    else:
        print("  âŒ source-metrics-docs command not found in doc-tools.js")
        return False
    
    if 'verifyMetricsExtractorDependencies' in content:
        print("  âœ“ verifyMetricsExtractorDependencies function found")
    else:
        print("  âŒ verifyMetricsExtractorDependencies function not found")
        return False
    
    return True


def generate_usage_summary():
    """Generate a summary of how to use the new automation"""
    print("\nğŸ“‹ Usage Summary")
    print("================")
    print()
    print("The new Redpanda metrics automation has been successfully created!")
    print()
    print("ğŸ”§ Setup:")
    print("  1. cd tools/metrics-extractor")
    print("  2. make setup-venv")
    print("  3. make install-deps")
    print()
    print("ğŸš€ Usage:")
    print("  â€¢ Extract from dev branch:")
    print("    make build TAG=dev")
    print()
    print("  â€¢ Extract from specific version:")
    print("    make build TAG=v23.3.1")
    print()
    print("  â€¢ Extract from local Redpanda repo:")
    print("    make extract-local REDPANDA_PATH=/path/to/redpanda")
    print()
    print("  â€¢ CLI integration:")
    print("    npx doc-tools generate source-metrics-docs --tag=dev")
    print()
    print("ğŸ“Š Output files:")
    print("  â€¢ autogenerated/{TAG}/source-metrics/metrics.json")
    print("  â€¢ autogenerated/{TAG}/source-metrics/metrics.adoc")
    print()
    print("ğŸ†š Comparison with existing metrics:")
    print("  python compare_metrics.py autogenerated/dev/source-metrics/metrics.json")
    print()
    print("ğŸ“ Key differences from the current metrics automation:")
    print("  â€¢ Extracts metrics directly from C++ source code")
    print("  â€¢ Uses tree-sitter for robust parsing")
    print("  â€¢ Captures ALL metrics defined in source, not just exposed ones")
    print("  â€¢ Provides file locations and constructor information")
    print("  â€¢ Works offline without requiring a running cluster")
    print()
    print("ğŸ” Supported metric constructors:")
    print("  â€¢ sm::make_gauge")
    print("  â€¢ sm::make_counter") 
    print("  â€¢ sm::make_histogram")
    print("  â€¢ sm::make_total_bytes")
    print("  â€¢ sm::make_derive")
    print("  â€¢ ss::metrics::make_total_operations")
    print("  â€¢ ss::metrics::make_current_bytes")


def main():
    """Main integration validation"""
    print("ğŸš€ Redpanda Metrics Extractor Integration Test")
    print("===============================================")
    
    # Change to the metrics-extractor directory
    os.chdir(Path(__file__).parent)
    
    success = True
    
    # Check dependencies
    if not check_dependencies():
        success = False
    
    # Test end-to-end functionality
    if success and not test_end_to_end():
        success = False
    
    # Validate CLI integration
    if not validate_cli_integration():
        print("  âš ï¸  CLI integration validation failed, but automation should still work")
    
    if success:
        print("\nğŸ‰ All tests passed!")
        generate_usage_summary()
        return 0
    else:
        print("\nâŒ Some tests failed. Please check the errors above.")
        print("\nFor manual testing:")
        print("  python example.py")
        return 1


if __name__ == "__main__":
    exit(main())
