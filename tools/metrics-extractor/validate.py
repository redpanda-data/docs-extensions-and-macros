#!/usr/bin/env python3
"""
Integration script to validate the complete metrics extraction pipeline
"""
import os
import sys
import json
import subprocess
from pathlib import Path


def check_dependencies():
    """Check if all required dependencies are available"""
    print("🔧 Checking dependencies...")
    
    try:
        import tree_sitter
        print("  ✓ tree-sitter is available")
    except ImportError:
        print("  ❌ tree-sitter not found. Install with: pip install tree-sitter")
        return False
    
    # Check if we can import our modules
    try:
        from metrics_parser import build_treesitter_cpp_library, extract_metrics_from_files
        from metrics_bag import MetricsBag
        print("  ✓ All custom modules are available")
    except ImportError as e:
        print(f"  ❌ Import error: {e}")
        return False
    
    return True


def test_end_to_end():
    """Test the complete extraction pipeline"""
    print("\n🧪 Running end-to-end test...")
    
    try:
        # Run the example script
        result = subprocess.run([
            sys.executable, "example.py", "--verbose"
        ], capture_output=True, text=True)
        
        if result.returncode == 0:
            print("  ✓ Example script completed successfully")
            return True
        else:
            print(f"  ❌ Example script failed:")
            print(f"     stdout: {result.stdout}")
            print(f"     stderr: {result.stderr}")
            return False
    except Exception as e:
        print(f"  ❌ Error running example: {e}")
        return False


def validate_cli_integration():
    """Validate that the CLI integration works"""
    print("\n🔗 Validating CLI integration...")
    
    # Check if the doc-tools.js file has our new command
    doc_tools_path = Path("../../bin/doc-tools.js")
    if not doc_tools_path.exists():
        print("  ❌ doc-tools.js not found")
        return False
    
    with open(doc_tools_path, 'r') as f:
        content = f.read()
    
    if 'source-metrics-docs' in content:
        print("  ✓ source-metrics-docs command found in doc-tools.js")
    else:
        print("  ❌ source-metrics-docs command not found in doc-tools.js")
        return False
    
    if 'verifyMetricsExtractorDependencies' in content:
        print("  ✓ verifyMetricsExtractorDependencies function found")
    else:
        print("  ❌ verifyMetricsExtractorDependencies function not found")
        return False
    
    return True


def generate_usage_summary():
    """Generate a summary of how to use the new automation"""
    print("\n📋 Usage Summary")
    print("================")
    print()
    print("The new Redpanda metrics automation has been successfully created!")
    print()
    print("🔧 Setup:")
    print("  1. cd tools/metrics-extractor")
    print("  2. make setup-venv")
    print("  3. make install-deps")
    print()
    print("🚀 Usage:")
    print("  • Extract from dev branch:")
    print("    make build TAG=dev")
    print()
    print("  • Extract from specific version:")
    print("    make build TAG=v23.3.1")
    print()
    print("  • Extract from local Redpanda repo:")
    print("    make extract-local REDPANDA_PATH=/path/to/redpanda")
    print()
    print("  • CLI integration:")
    print("    npx doc-tools generate source-metrics-docs --tag=dev")
    print()
    print("📊 Output files:")
    print("  • autogenerated/{TAG}/source-metrics/metrics.json")
    print("  • autogenerated/{TAG}/source-metrics/metrics.adoc")
    print()
    print("🆚 Comparison with existing metrics:")
    print("  python compare_metrics.py autogenerated/dev/source-metrics/metrics.json")
    print()
    print("📁 Key differences from the current metrics automation:")
    print("  • Extracts metrics directly from C++ source code")
    print("  • Uses tree-sitter for robust parsing")
    print("  • Captures ALL metrics defined in source, not just exposed ones")
    print("  • Provides file locations and constructor information")
    print("  • Works offline without requiring a running cluster")
    print()
    print("🔍 Supported metric constructors:")
    print("  • sm::make_gauge")
    print("  • sm::make_counter") 
    print("  • sm::make_histogram")
    print("  • sm::make_total_bytes")
    print("  • sm::make_derive")
    print("  • ss::metrics::make_total_operations")
    print("  • ss::metrics::make_current_bytes")


def main():
    """Main integration validation"""
    print("🚀 Redpanda Metrics Extractor Integration Test")
    print("===============================================")
    
    # Change to the metrics-extractor directory
    os.chdir(Path(__file__).parent)
    
    success = True
    
    # Check dependencies
    if not check_dependencies():
        success = False
    
    # Test end-to-end functionality
    if success and not test_end_to_end():
        success = False
    
    # Validate CLI integration
    if not validate_cli_integration():
        print("  ⚠️  CLI integration validation failed, but automation should still work")
    
    if success:
        print("\n🎉 All tests passed!")
        generate_usage_summary()
        return 0
    else:
        print("\n❌ Some tests failed. Please check the errors above.")
        print("\nFor manual testing:")
        print("  python example.py")
        return 1


if __name__ == "__main__":
    exit(main())
